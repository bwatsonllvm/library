{
  "source": {
    "slug": "openalex-discovery",
    "name": "OpenAlex Discovery (seeded by LLVM speakers/authors)",
    "url": "https://api.openalex.org"
  },
  "papers": [
    {
      "id": "openalex-w7125374165",
      "source": "openalex-discovery",
      "title": "ThreadMonitor : Low‐Overhead Data Race Detection Using Intel Processor Trace",
      "authors": [
        {
          "name": "Farzam Dorostkar",
          "affiliation": "Polytechnique Montréal"
        },
        {
          "name": "Michel Dagenais",
          "affiliation": "Polytechnique Montréal"
        },
        {
          "name": "A.K. Tyagi",
          "affiliation": "Ericsson (United States)"
        },
        {
          "name": "Vince Bridgers",
          "affiliation": "Ericsson (United States)"
        }
      ],
      "year": "2026",
      "venue": "Concurrency and Computation Practice and Experience | Vol. 38 (Issue 2)",
      "type": "research-paper",
      "abstract": "ABSTRACT Data races are among the most difficult multithreading bugs to find, due to their non‐deterministic nature. This and the increasing popularity of multithreaded programming have led to the need for practical automated data race detection. In this context, dynamic data race detectors have received more attention, compared to static tools, owing to their higher accuracy and scalability. Yet, state‐of‐the‐art dynamic data race detectors cannot be used in many real‐world testing scenarios, since they cause significant slowdown and memory overhead. Notably, ThreadSanitizer (TSan), the default dynamic data race detector in both clang and gcc compilers, is reported to typically impose a – slowdown and a – memory overhead, which is not tolerable in many industrial use cases. To address this issue, this paper introduces ThreadMonitor (TMon), a low‐overhead postmortem data race detector for multithreaded C/C++ programs that use the Pthread library. At runtime, TMon traces the information required for detecting occurrences of data races (i.e., shared memory accesses and timing constraints among threads) using Intel Processor Trace (Intel PT), a non‐intrusive hardware feature dedicated to tracing software execution. Thereafter, its postmortem analyzer examines the collected trace data to determine whether the traced program execution exhibited data races, performing a verification similar to that carried out by TSan at runtime. Introducing algorithmic improvements in its postmortem analyzer, TMon can further achieve a higher data race detection coverage compared to TSan. TMon has no direct data memory overhead, incurs minimal instruction memory overhead, and causes a very small slowdown, making it an ideal choice in test environments with limited resources.",
      "paperUrl": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/cpe.70517",
      "sourceUrl": "https://doi.org/10.1002/cpe.70517",
      "tags": [
        "C++",
        "Clang",
        "Testing"
      ],
      "matchedAuthors": [
        "Vince Bridgers"
      ]
    },
    {
      "id": "openalex-w4417409138",
      "source": "openalex-discovery",
      "title": "The Munich Quantum Software Stack: Connecting End Users, Integrating Diverse Quantum Technologies, Accelerating HPC",
      "authors": [
        {
          "name": "Lukas Burgholzer",
          "affiliation": "Munich Center for Quantum Science and Technology"
        },
        {
          "name": "Jorge Echavarria",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Patrick Hopf",
          "affiliation": "Munich Center for Quantum Science and Technology"
        },
        {
          "name": "Yannick Stade",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Damian Rovara",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "L. Schmid",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Ercüment Kaya",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Burak Mete",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Muhammad Nufail Farooqi",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Minh Thanh Chung",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Marco De Pascale",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Laura Schulz",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Martin Schulz",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Robert Wille",
          "affiliation": "Munich Center for Quantum Science and Technology"
        }
      ],
      "year": "2026",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Quantum computing is advancing rapidly in hardware and algorithms, but broad accessibility demands a comprehensive, efficient, unified software stack. Such a stack must flexibly span diverse hardware and evolving algorithms, expose usable programming models for experts and non-experts, manage resources dynamically, and integrate seamlessly with classical High-Performance Computing (HPC). As quantum systems increasingly act as accelerators in hybrid workflows -- ranging from loosely to tightly coupled -- few full-featured implementations exist despite many proposals. We introduce the Munich Quantum Software Stack (MQSS), a modular, open-source, community-driven ecosystem for hybrid quantum-classical applications. MQSS's multi-layer architecture executes high-level applications on heterogeneous quantum back ends and coordinates their coupling with classical workloads. Core elements include front-end adapters for popular frameworks and new programming approaches, an HPC-integrated scheduler, a powerful MLIR-based compiler, and a standardized hardware abstraction layer, the Quantum Device Management Interface (QDMI). While under active development, MQSS already provides mature concepts and open-source components that form the basis of a robust quantum computing software stack, with a forward-looking design that anticipates fault-tolerant quantum computing, including varied qubit encodings and mid-circuit measurements.",
      "paperUrl": "https://doi.org/10.1145/3773656.3773669",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Performance",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Martin Schulz"
      ]
    },
    {
      "id": "openalex-w7127342222",
      "source": "openalex-discovery",
      "title": "SplittingSecrets: A Compiler-Based Defense for Preventing Data Memory-Dependent Prefetcher Side-Channels",
      "authors": [
        {
          "name": "Reshabh K Sharma",
          "affiliation": "University of Washington"
        },
        {
          "name": "Dan Grossman",
          "affiliation": "University of Washington"
        },
        {
          "name": "David Kohlbrenner",
          "affiliation": "University of Washington"
        }
      ],
      "year": "2026",
      "venue": "Proceedings of the Microarchitecture Security Conference | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Traditional side-channels take advantage of secrets being used as inputs to unsafe instructions, used for memory accesses, or used in control flow decisions. Constant-time programming, which restricts such code patterns, has been widely adopted as a defense against these vulnerabilities. However, new hardware optimizations in the form of Data Memory-dependent Prefetchers (DMP) present in Apple, Intel, and ARM CPUs have shown such defenses are not sufficient. These prefetchers, unlike classical prefetchers, use the content of memory as well as the trace of prior accesses to determine prefetch targets. An adversary abusing such a prefetcher has been shown to be able to mount attacks leaking data-at-rest; data that is never used by the program, even speculatively, in an unsafe manner. In response, this paper introduces SplittingSecrets, a compiler-based tool that can harden software libraries against side-channels arising from DMPs. SplittingSecrets's approach avoids reasoning about the complex internals of different DMPs and instead relies on one key aspect of all DMPs: activation requires data to resemble addresses. To prevent secret data from leaking, SplittingSecrets transforms memory operations to ensure that secrets are never stored in memory in a manner resembling an address, thereby avoiding DMP activation on those secrets. Rather than disable a DMP entirely, SplittingSecrets can provide targeted hardening for only specific secrets entirely in software. We have implemented SplittingSecrets using LLVM, supporting both source-level memory operations and those generated by the compiler backend for the AArch64 architecture, We have analyzed the performance overhead involved in safeguarding secrets from DMP-induced attacks using common primitives in libsodium, a popular cryptographic library when built for Apple M-series CPUs.",
      "paperUrl": "https://tosc.iacr.org/index.php/uASC/article/download/12709/12397",
      "sourceUrl": "https://doi.org/10.46586/uasc.2026.005",
      "tags": [
        "Backend",
        "Libraries",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Dan Grossman",
        "Reshabh K Sharma"
      ]
    },
    {
      "id": "openalex-w7127540761",
      "source": "openalex-discovery",
      "title": "RVDebloater: Mode-based Adaptive Firmware Debloating for Robotic Vehicles",
      "authors": [
        {
          "name": "Mohsen Amini Salehi",
          "affiliation": ""
        },
        {
          "name": "Karthik Pattabiraman",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "ArXiv.org | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As the number of embedded devices grows and their functional requirements increase, embedded firmware is becoming increasingly larger, thereby expanding its attack surface. Despite the increase in firmware size, many embedded devices, such as robotic vehicles (RVs), operate in distinct modes, each requiring only a small subset of the firmware code at runtime. We refer to such devices as mode-based embedded devices. Debloating is an approach to reduce attack surfaces by removing or restricting unneeded code, but existing techniques suffer from significant limitations, such as coarse granularity and irreversible code removal, limiting their applicability. To address these limitations, we propose RVDebloater, a novel adaptive debloating technique for mode-based embedded devices that automatically identifies unneeded firmware code for each mode using either static or dynamic analysis, and dynamically debloats the firmware for each mode at the function level at runtime. RVDebloater introduces a new software-based enforcement approach that supports diverse mode-based embedded devices. We implemented RVDebloater using the LLVM compiler and evaluated its efficiency and effectiveness on six different RVs, including both simulated and real ones, with different real-world missions. We find that device requirements change throughout its lifetime for each mode, and that many critical firmware functions can be restricted in other modes, with an average of 85% of functions not being required. The results showed that none of the missions failed after debloating with RVDebloater, indicating that it neither incurred false positives nor false negatives. Further, RVDebloater prunes the firmware call graph by an average of 45% across different firmware. Finally, RVDebloater incurred an average performance overhead of 3.9% and memory overhead of 4% (approximately 0.25 MB) on real RVs.",
      "paperUrl": "https://arxiv.org/pdf/2602.00270",
      "sourceUrl": "http://arxiv.org/abs/2602.00270",
      "tags": [
        "Dynamic Analysis",
        "Embedded",
        "Performance"
      ],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w7128408966",
      "source": "openalex-discovery",
      "title": "Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering",
      "authors": [
        {
          "name": "Amir H. Ashouri",
          "affiliation": ""
        },
        {
          "name": "Shayan Shirahmad Gale Bagi",
          "affiliation": ""
        },
        {
          "name": "Kavin Satheeskumar",
          "affiliation": ""
        },
        {
          "name": "Tejas Srikanth",
          "affiliation": ""
        },
        {
          "name": "Jonathan Zhao",
          "affiliation": ""
        },
        {
          "name": "Ibrahim Saidoun",
          "affiliation": ""
        },
        {
          "name": "Ziwen Wang",
          "affiliation": ""
        },
        {
          "name": "Bryan Chan",
          "affiliation": ""
        },
        {
          "name": "Tomasz S. Czajkowski",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "ArXiv.org | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.",
      "paperUrl": "https://arxiv.org/pdf/2602.06142",
      "sourceUrl": "http://arxiv.org/abs/2602.06142",
      "tags": [
        "ML",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Bryan Chan"
      ]
    },
    {
      "id": "openalex-w7128648222",
      "source": "openalex-discovery",
      "title": "PICASSO: Scaling CHERI Use-After-Free Protection to Millions of Allocations using Colored Capabilities",
      "authors": [
        {
          "name": "Merve Gülmez",
          "affiliation": ""
        },
        {
          "name": "Ruben Sturm",
          "affiliation": ""
        },
        {
          "name": "Hossam ElAtali",
          "affiliation": ""
        },
        {
          "name": "Håkan Englund",
          "affiliation": ""
        },
        {
          "name": "Jonathan Woodruff",
          "affiliation": ""
        },
        {
          "name": "N. Asokan",
          "affiliation": ""
        },
        {
          "name": "Thomas Nyman",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "While the CHERI instruction-set architecture extensions for capabilities enable strong spatial memory safety, CHERI lacks built-in temporal safety, particularly for heap allocations. Prior attempts to augment CHERI with temporal safety fall short in terms of scalability, memory overhead, and incomplete security guarantees due to periodical sweeps of the system's memory to individually revoke stale capabilities. We address these limitations by introducing colored capabilities that add a controlled form of indirection to CHERI's capability model. This enables provenance tracking of capabilities to their respective allocations via a hardware-managed provenance-validity table, allowing bulk retraction of dangling pointers without needing to quarantine freed memory. Colored capabilities significantly reduce the frequency of capability revocation sweeps while improving security. We realize colored capabilities in PICASSO, an extension of the CHERI-RISC-V architecture on a speculative out-of-order FPGA softcore (CHERI-Toooba). We also integrate colored-capability support into the CheriBSD OS and CHERI-enabled Clang/LLVM toolchain. Our evaluation shows effective mitigation of use-after-free and double-free bugs across all heap-based temporal memory-safety vulnerabilities in NIST Juliet test cases, with only a small performance overhead on SPEC CPU benchmarks (5% g.m.), less latency, and more consistent performance in long-running SQLite, PostgreSQL, and gRPC workloads compared to prior work.",
      "paperUrl": "https://arxiv.org/pdf/2602.09131",
      "sourceUrl": "http://arxiv.org/abs/2602.09131",
      "tags": [
        "Clang",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Jonathan Woodruff"
      ]
    },
    {
      "id": "openalex-w7119532866",
      "source": "openalex-discovery",
      "title": "Nice to Meet You: Synthesizing Practical MLIR Abstract Transformers",
      "authors": [
        {
          "name": "Xuanyu Peng",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Dominic Kennedy",
          "affiliation": "University of Utah"
        },
        {
          "name": "Yuyou Fan",
          "affiliation": "University of Utah"
        },
        {
          "name": "Ben Greenman",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        },
        {
          "name": "Loris D’Antoni",
          "affiliation": "University of California, San Diego"
        }
      ],
      "year": "2026",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 10 (Issue POPL)",
      "type": "research-paper",
      "abstract": "Static analyses play a fundamental role during compilation: they discover facts that are true in all executions of the code being compiled, and then these facts are used to justify optimizations and diagnostics. Each static analysis is based on a collection of abstract transformers that provide abstract semantics for the concrete instructions that make up a program. It can be challenging to implement abstract transformers that are sound, precise, and efficient—and in fact both LLVM and GCC have suffered from miscompilations caused by unsound abstract transformers. Moreover, even after more than 20 years of development, LLVM lacks abstract transformers for hundreds of instructions in its intermediate representation (IR). We developed NiceToMeetYou: a program synthesis framework for abstract transformers that are aimed at the kinds of non-relational integer abstract domains that are heavily used by today’s production compilers. It exploits a simple but novel technique for breaking the synthesis problem into parts: each of our transformers is the meet of a collection of simpler, sound transformers that are synthesized such that each new piece fills a gap in the precision of the final transformer. Our design point is bulk automation: no sketches are required. Transformers are verified by lowering to a previously-created SMT dialect of MLIR. Each of our synthesized transformers is provably sound and some (17 %) are more precise than those provided by LLVM.",
      "paperUrl": "https://doi.org/10.1145/3776722",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR",
        "Optimizations",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Yuyou Fan"
      ]
    },
    {
      "id": "openalex-w7126246459",
      "source": "openalex-discovery",
      "title": "Magellan: Autonomous Discovery of Novel Compiler Optimization Heuristics with AlphaEvolve",
      "authors": [
        {
          "name": "Hongzheng Chen",
          "affiliation": ""
        },
        {
          "name": "Alexander Novikov",
          "affiliation": ""
        },
        {
          "name": "Ngân Vũ",
          "affiliation": ""
        },
        {
          "name": "Hanna Alam",
          "affiliation": ""
        },
        {
          "name": "Zhiru Zhang",
          "affiliation": ""
        },
        {
          "name": "Aiden Grossman",
          "affiliation": ""
        },
        {
          "name": "Mircea Trofin",
          "affiliation": ""
        },
        {
          "name": "Hadi Esmaeilzadeh",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern compilers rely on hand-crafted heuristics to guide optimization passes. These human-designed rules often struggle to adapt to the complexity of modern software and hardware and lead to high maintenance burden. To address this challenge, we present Magellan, an agentic framework that evolves the compiler pass itself by synthesizing executable C++ decision logic. Magellan couples an LLM coding agent with evolutionary search and autotuning in a closed loop of generation, evaluation on user-provided macro-benchmarks, and refinement, producing compact heuristics that integrate directly into existing compilers. Across several production optimization tasks, Magellan discovers policies that match or surpass expert baselines. In LLVM function inlining, Magellan synthesizes new heuristics that outperform decades of manual engineering for both binary-size reduction and end-to-end performance. In register allocation, it learns a concise priority rule for live-range processing that matches intricate human-designed policies on a large-scale workload. We also report preliminary results on XLA problems, demonstrating portability beyond LLVM with reduced engineering effort.",
      "paperUrl": "https://doi.org/10.48550/arxiv.2601.21096",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Performance"
      ],
      "matchedAuthors": [
        "Aiden Grossman",
        "Hongzheng Chen",
        "Mircea Trofin",
        "Zhiru Zhang"
      ]
    },
    {
      "id": "openalex-w7119233747",
      "source": "openalex-discovery",
      "title": "MLIR-Smith: A Novel Random Program Generator for Evaluating Compiler Pipelines",
      "authors": [
        {
          "name": "Berke Ates",
          "affiliation": ""
        },
        {
          "name": "Filip Dobrosavljević",
          "affiliation": ""
        },
        {
          "name": "Theodoros Theodoridis",
          "affiliation": ""
        },
        {
          "name": "Zhendong Su",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "ArXiv.org | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compilers are essential for the performance and correct execution of software and hold universal relevance across various scientific disciplines. Despite this, there is a notable lack of tools for testing and evaluating them, especially within the adaptable Multi-Level Intermediate Representation (MLIR) context. This paper addresses the need for a tool that can accommodate MLIR's extensibility, a feature not provided by previous methods such as Csmith. Here we introduce MLIR-Smith, a novel random program generator specifically designed to test and evaluate MLIR-based compiler optimizations. We demonstrate the utility of MLIR-Smith by conducting differential testing on MLIR, LLVM, DaCe, and DCIR, which led to the discovery of multiple bugs in these compiler pipelines. The introduction of MLIR-Smith not only fills a void in the realm of compiler testing but also emphasizes the importance of comprehensive testing within these systems. By providing a tool that can generate random MLIR programs, this paper enhances our ability to evaluate and improve compilers and paves the way for future tools, potentially shaping the wider landscape of software testing and quality assurance.",
      "paperUrl": "https://arxiv.org/pdf/2601.02218",
      "sourceUrl": "http://arxiv.org/abs/2601.02218",
      "tags": [
        "MLIR",
        "Optimizations",
        "Performance",
        "Testing"
      ],
      "matchedAuthors": [
        "Theodoros Theodoridis"
      ]
    },
    {
      "id": "openalex-w7124680514",
      "source": "openalex-discovery",
      "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR",
      "authors": [
        {
          "name": "Erwei Wang",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Samuel Bayliss",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Andra Bisca",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Zachary Taylor Blair",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Sangeeta Chowdhary",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "K. Denolf",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Jeff Fifield",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Brandon Freiberger",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Erika Hunhoff",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Phil James-Roxby",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Jack Lo",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Joseph Melber",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Eddie Richter",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "A. Rosti",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Javier Setoain",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Gagandeep Singh",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Endri Taka",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Pranathi Vasireddy",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Zhewen Yu",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Niansong Zhang",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Jinming Zhuang",
          "affiliation": "Advanced Micro Devices (United States)"
        }
      ],
      "year": "2026",
      "venue": "ACM Transactions on Reconfigurable Technology and Systems | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "General-purpose compilers abstract away parallelism, locality, and synchronization, limiting their effectiveness on modern spatial architectures. As modern computing architectures increasingly rely on fine-grained control over data movement, execution order, and compute placement for performance, compiler infrastructure must provide explicit mechanisms for orchestrating compute and data to fully exploit such architectures. We introduce MLIR-AIR, a novel, open-source compiler stack built on MLIR that bridges the semantic gap between high-level workloads and fine-grained spatial architectures such as AMD’s NPUs. MLIR-AIR defines the AIR dialect, which provides structured representations for asynchronous and hierarchical operations across compute and memory resources. AIR primitives allow the compiler to orchestrate spatial scheduling, distribute computation across hardware regions, and overlap communication with computation without relying on ad hoc runtime coordination or manual scheduling. We demonstrate MLIR-AIR’s capabilities through two case studies: matrix multiplication and the multi-head attention block from the LLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute efficiency and generates implementations with performance almost identical to state-of-the-art, hand-optimized matrix multiplication written using the lower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we demonstrate that the AIR interface supports fused implementations using approximately 150 lines of code, enabling tractable expression of complex workloads with efficient mapping to spatial hardware. MLIR-AIR transforms high-level structured control flow into spatial programs that efficiently utilize the compute fabric and memory hierarchy of an NPU, leveraging asynchronous execution, tiling, and communication overlap through compiler-managed scheduling.",
      "paperUrl": "https://doi.org/10.1145/3785670",
      "sourceUrl": "",
      "tags": [
        "AI",
        "Infrastructure",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeff Fifield",
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w7130529454",
      "source": "openalex-discovery",
      "title": "E-Graphs as a Persistent Compiler Abstraction",
      "authors": [
        {
          "name": "Jules Merckx",
          "affiliation": ""
        },
        {
          "name": "Alexandre Lopoukhine",
          "affiliation": ""
        },
        {
          "name": "Samuel Coward",
          "affiliation": ""
        },
        {
          "name": "Jianyi Cheng",
          "affiliation": ""
        },
        {
          "name": "Bjorn De Sutter",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Recent algorithmic advances have made equality saturation an appealing approach to program optimization because it avoids the phase-ordering problem. Existing work uses external equality saturation libraries, or custom implementations that are deeply tied to the specific application. However, these works only apply equality saturation at a single level of abstraction, or discard the discovered equalities when code is transformed by other compiler passes. We propose an alternative approach that represents an e-graph natively in the compiler's intermediate representation, facilitating the application of constructive compiler passes that maintain the e-graph state throughout the compilation flow. We build on a Python-based MLIR framework, xDSL, and introduce a new MLIR dialect, eqsat, that represents e-graphs in MLIR code. We show that this representation expands the scope of equality saturation in the compiler, allowing us to interleave pattern rewriting with other compiler transformations. The eqsat dialect provides a unified abstraction for compilers to utilize equality saturation across various levels of intermediate representations concurrently within the same MLIR flow.",
      "paperUrl": "https://doi.org/10.48550/arxiv.2602.16707",
      "sourceUrl": "",
      "tags": [
        "Libraries",
        "MLIR"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w7125775521",
      "source": "openalex-discovery",
      "title": "An MLIR Lowering Pipeline for Stencils at Wafer-Scale",
      "authors": [
        {
          "name": "Nicolai Stawinoga",
          "affiliation": ""
        },
        {
          "name": "David Katz",
          "affiliation": ""
        },
        {
          "name": "Anton Lydike",
          "affiliation": ""
        },
        {
          "name": "Justs Zarins",
          "affiliation": ""
        },
        {
          "name": "Nick Brown",
          "affiliation": ""
        },
        {
          "name": "George Bisbas",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2026",
      "venue": "ArXiv.org | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Cerebras Wafer-Scale Engine (WSE) delivers performance at an unprecedented scale of over 900,000 compute units, all connected via a single-wafer on-chip interconnect. Initially designed for AI, the WSE architecture is also well-suited for High Performance Computing (HPC). However, its distributed asynchronous programming model diverges significantly from the simple sequential or bulk-synchronous programs that one would typically derive for a given mathematical program description. Targeting the WSE requires a bespoke re-implementation when porting existing code. The absence of WSE support in compilers such as MLIR, meant that there was little hope for automating this process. Stencils are ubiquitous in HPC, and in this paper we explore the hypothesis that domain specific information about stencils can be leveraged by the compiler to automatically target the WSE without requiring application-level code changes. We present a compiler pipeline that transforms stencil-based kernels into highly optimized CSL code for the WSE, bridging the semantic gap between the mathematical representation of the problem and the WSE's asynchronous execution model. Based upon five benchmarks across three HPC programming technologies, running on both the Cerebras WSE2 and WSE3, our approach delivers comparable, if not slightly better, performance than manually optimized code. Furthermore, without requiring any application level code changes, performance on the WSE3 is around 14 times faster than 128 Nvidia A100 GPUs and 20 times faster than 128 nodes of a CPU-based Cray-EX supercomputer when using our approach.",
      "paperUrl": "https://arxiv.org/pdf/2601.17754",
      "sourceUrl": "https://doi.org/10.1145/3779212.3790124",
      "tags": [
        "AI",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Lydike",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4407863036",
      "source": "openalex-discovery",
      "title": "xDSL: Sidekick Compilation for SSA-Based Compilers",
      "authors": [
        {
          "name": "Mathieu Fehr",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Michel Weber",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Christian Ulmann",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Alexandre Lopoukhine",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Martin Paul Lücke",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Théo Degioanni",
          "affiliation": "École Normale Supérieure de Rennes"
        },
        {
          "name": "Christos Vasiladiotis",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Michel Steuwer",
          "affiliation": "Technische Universität Berlin"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Traditionally, compiler researchers either conduct experiments within an existing production compiler or develop their own prototype compiler; both options come with trade-offs. On one hand, prototyping in a production compiler can be cumbersome, as they are often optimized for program compilation speed at the expense of software simplicity and development speed. On the other hand, the transition from a prototype compiler to production requires significant engineering work. To bridge this gap, we introduce the concept of sidekick compiler frameworks, an approach that uses multiple frameworks that interoperate with each other by leveraging textual interchange formats and declarative descriptions of abstractions. Each such compiler framework is specialized for specific use cases, such as performance or prototyping. Abstractions are by design shared across frameworks, simplifying the transition from prototyping to production. We demonstrate this idea with xDSL, a sidekick for MLIR focused on prototyping and teaching. xDSL interoperates with MLIR through a shared textual IR and the exchange of IRs through an IR Definition Language. The benefits of sidekick compiler frameworks are evaluated by showing on three use cases how xDSL impacts their development: teaching, DSL compilation, and rewrite system prototyping. We also investigate the trade-offs that xDSL offers, and demonstrate how we simplify the transition between frameworks using the IRDL dialect. With sidekick compilation, we envision a future in which engineers minimize the cost of development by choosing a framework built for their immediate needs, and later transitioning to production with minimal overhead.",
      "paperUrl": "https://doi.org/10.1145/3696443.3708945",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Mathieu Fehr",
        "Michel Steuwer",
        "Théo Degioanni",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4415034317",
      "source": "openalex-discovery",
      "title": "llvm-dimeta: A library for extracting source-level type information in LLVM IR using debug metadata",
      "authors": [
        {
          "name": "Alexander Hück",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Sebastian Kreutzer",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Christian Bischof",
          "affiliation": "Technical University of Darmstadt"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "LLVM frontends like Clang preserve source-level type information in the intermediate representation (IR) primarily through debug metadata. Although intended for debuggers, this metadata benefits compiler tools like sanitizers, which need type information of stack, heap and global allocations for tasks such as verifying memory safety. We present llvm-dimeta, a library that extracts type information for memory allocations directly from LLVM IR via debug metadata. It handles the necessary IR analysis internally, enabling frontend-independent type queries. As a case study, we integrate llvm-dimeta into a type correctness checker for MPI-parallelized HPC applications. Using llvm-dimeta, the checker achieves 98% accuracy on an MPI correctness benchmark, up from 76% when relying solely on LLVM IR’s more limited type system. Code is available at https://github.com/ahueck/llvm-dimeta",
      "paperUrl": "https://doi.org/10.1109/scam67354.2025.00019",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Frontend",
        "IR"
      ],
      "matchedAuthors": [
        "Alexander Hück",
        "Sebastian Kreutzer"
      ]
    },
    {
      "id": "openalex-w4415198039",
      "source": "openalex-discovery",
      "title": "eqsat: An Equality Saturation Dialect for Non-destructive Rewriting",
      "authors": [
        {
          "name": "Jules Merckx",
          "affiliation": ""
        },
        {
          "name": "Alexandre Lopoukhine",
          "affiliation": ""
        },
        {
          "name": "Samuel Coward",
          "affiliation": ""
        },
        {
          "name": "Jianyi Cheng",
          "affiliation": ""
        },
        {
          "name": "Bjorn De Sutter",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "With recent algorithmic improvements and easy-to-use libraries, equality saturation is being picked up for hardware design, program synthesis, theorem proving, program optimization, and more. Existing work on using equality saturation for program optimization makes use of external equality saturation libraries such as egg, typically generating a single optimized expression. In the context of a compiler, such an approach uses equality saturation to replace a small number of passes. In this work, we propose an alternative approach that represents equality saturation natively in the compiler's intermediate representation, facilitating the application of constructive compiler passes that maintain the e-graph state throughout the compilation flow. We take LLVM's MLIR framework and propose a new MLIR dialect named eqsat that represents e-graphs in MLIR code. This not only provides opportunities to rethink e-matching and extraction techniques by orchestrating existing MLIR passes, such as common subexpression elimination, but also avoids translation overhead between the chosen e-graph library and MLIR. Our eqsat intermediate representation (IR) allows programmers to apply equality saturation on arbitrary domain-specific IRs using the same flow as other compiler transformations in MLIR.",
      "paperUrl": "https://arxiv.org/pdf/2505.09363",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2505.09363",
      "tags": [
        "IR",
        "Libraries",
        "MLIR"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4411115416",
      "source": "openalex-discovery",
      "title": "Vellvm: Formalizing the Informal LLVM",
      "authors": [
        {
          "name": "Calvin Beck",
          "affiliation": "University of Pennsylvania"
        },
        {
          "name": "Hanxi Chen",
          "affiliation": "University of Pennsylvania"
        },
        {
          "name": "Steve Zdancewic",
          "affiliation": "University of Pennsylvania"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-031-93706-4_6",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Steve Zdancewic"
      ]
    },
    {
      "id": "openalex-w7117992936",
      "source": "openalex-discovery",
      "title": "Vector Extensions in Clang-Based Compilers",
      "authors": [
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Maker Innovations Series | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Clang provides support for special #pragma clang loop directive, which allows developers to use special high-level hints for the compiler, potentially improving performance without the need for low-level code modifications. This directive allows to specify several key categories of loop hints:",
      "paperUrl": "https://doi.org/10.1007/979-8-8688-2169-1_3",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexey Bataev"
      ]
    },
    {
      "id": "openalex-w4416070486",
      "source": "openalex-discovery",
      "title": "VecTrans: Enhancing Compiler Auto-Vectorization through LLM-Assisted Code Transformations",
      "authors": [
        {
          "name": "Zaosong Zheng",
          "affiliation": ""
        },
        {
          "name": "Kan Wu",
          "affiliation": ""
        },
        {
          "name": "Long Cheng",
          "affiliation": ""
        },
        {
          "name": "Li Lü",
          "affiliation": ""
        },
        {
          "name": "Rodrigo C. O. Rocha",
          "affiliation": ""
        },
        {
          "name": "Tianyi Liu",
          "affiliation": ""
        },
        {
          "name": "Wei Wei",
          "affiliation": ""
        },
        {
          "name": "Jianjiang Zeng",
          "affiliation": ""
        },
        {
          "name": "Xianwei Zhang",
          "affiliation": ""
        },
        {
          "name": "Yaoqing Gao",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Auto-vectorization is a fundamental optimization for modern compilers to exploit SIMD parallelism. However, state-of-the-art approaches still struggle to handle intricate code patterns, often requiring manual hints or domain-specific expertise. Large language models (LLMs), with their ability to capture intricate patterns, provide a promising solution, yet their effective application in compiler optimizations remains an open challenge due to issues such as hallucinations and a lack of domain-specific reasoning. In this paper, we present VecTrans, a novel framework that leverages LLMs to enhance compiler-based code vectorization. VecTrans first employs compiler analysis to identify potentially vectorizable code regions. It then utilizes an LLM to refactor these regions into patterns that are more amenable to the compilers auto-vectorization. To ensure semantic correctness, VecTrans further integrates a hybrid validation mechanism at the intermediate representation (IR) level. With the above efforts, VecTrans combines the adaptability of LLMs with the precision of compiler vectorization, thereby effectively opening up the vectorization opportunities. experimental results show that among all TSVC functions unvectorizable by GCC, ICC, Clang, and BiSheng Compiler, VecTrans achieves an geomean speedup of 1.77x and successfully vectorizes 24 of 51 test cases. This marks a significant advancement over state-of-the-art approaches while maintaining a cost efficiency of $0.012 per function optimization for LLM API usage.",
      "paperUrl": "https://arxiv.org/pdf/2503.19449",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2503.19449",
      "tags": [
        "Clang",
        "IR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Long Cheng",
        "Rodrigo C. O. Rocha",
        "Wei Wei"
      ]
    },
    {
      "id": "openalex-w4415008065",
      "source": "openalex-discovery",
      "title": "Translation Validation for LLVM’s AArch64 Backend",
      "authors": [
        {
          "name": "Richard A. Berger",
          "affiliation": "Nvidia (United States)"
        },
        {
          "name": "Mitch Briles",
          "affiliation": "University of Utah"
        },
        {
          "name": "Nader Boushehrinejad Moradi",
          "affiliation": "University of Utah"
        },
        {
          "name": "Nicholas Coughlin",
          "affiliation": "University of Queensland"
        },
        {
          "name": "K.P. Lam",
          "affiliation": "Defence Science and Technology Group"
        },
        {
          "name": "Nuno P. Lopes",
          "affiliation": "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
        },
        {
          "name": "Stefan Mada",
          "affiliation": "University of Utah"
        },
        {
          "name": "Tanmay Tirpankar",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "LLVM's backends translate its intermediate representation (IR) to assembly or object code. Alongside register allocation and instruction selection, these backends contain many analogues of components traditionally associated with compiler middle ends: dataflow analyses, common subexpression elimination, loop invariant code motion, and a first-class IR -- MIR, the \"machine IR.\" In effect, this kind of compiler backend is a highly optimizing compiler in its own right, with all of the correctness hazards entailed by a million lines of intricate C++. As a step towards gaining confidence in the correctness of work done by LLVM backends, we have created arm-tv, which formally verifies translations between LLVM IR and AArch64 (64-bit ARM) code. Ours is not the first translation validation work for LLVM, but we have advanced the state of the art along multiple fronts: arm-tv is a checking validator that enforces numerous ABI rules; we have extended Alive2 (which we reuse as a verification backend) to deal with unstructured mixes of pointers and integers that are typical of assembly code; we investigate the tradeoffs between hand-written AArch64 semantics and those derived mechanically from ARM's published formal semantics; and, we have used arm-tv to discover 45 previously unknown miscompilation bugs in this LLVM backend, most of which are now fixed in upstream LLVM.",
      "paperUrl": "https://doi.org/10.1145/3763147",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "C++",
        "IR"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Mitch Briles",
        "Nader Boushehrinejad Moradi",
        "Nuno P. Lopes"
      ]
    },
    {
      "id": "openalex-w4416875127",
      "source": "openalex-discovery",
      "title": "Towards a Unified Multi-Target Mlir-Based Compiler: A Heterogeneous Compilation Framework for High-Performance and Quantum Computing Integration",
      "authors": [
        {
          "name": "Martín Letras",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Jorge Echavarria",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Muhammad Nufail Farooqi",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Marco De Pascale",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Mario Hernández Vera",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Nathaniel Tornow",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Laura Schulz",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Martin Schulz",
          "affiliation": "Leibniz Supercomputing Centre"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Munich Quantum Software Stack (MQSS) is a compilation and runtime infrastructure designed to bridge the gap between high-performance computing (HPC) and quantum computing (QC). A unified, extensible, and efficient compilation framework becomes important as quantum devices and applications scale. This paper proposes the integration of Multi-Level Intermediate Representation (MLIR) as intermediate representation into the MQSS to address the challenges of optimizing and compiling hybrid classical-quantum applications across diverse quantum devices. Accordingly, the MQSS can separate abstraction layers, enabling high-level optimizations, seamless integration of existing quantum dialects, and targetspecific optimizations for diverse quantum devices.",
      "paperUrl": "https://doi.org/10.1109/qce65121.2025.10288",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Performance",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Martin Schulz"
      ]
    },
    {
      "id": "openalex-w4411088314",
      "source": "openalex-discovery",
      "title": "Towards Source Mapping for Zero-Knowledge Smart Contracts: Design and Preliminary Evaluation",
      "authors": [
        {
          "name": "Pei Xu",
          "affiliation": "University of Technology Sydney"
        },
        {
          "name": "Yulei Sui",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Mark Staples",
          "affiliation": "RoZetta Institute"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Debugging and auditing zero-knowledge-compatible smart contracts remains a significant challenge due to the lack of source mapping in compilers such as zkSolc. In this work, we present a preliminary source mapping framework that establishes traceability between Solidity source code, LLVM IR, and zkEVM bytecode within the zkSolc compilation pipeline. Our approach addresses the traceability challenges introduced by non-linear transformations and proof-friendly optimizations in zero-knowledge compilation.",
      "paperUrl": "https://doi.org/10.1145/3713081.3732932",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Yulei Sui"
      ]
    },
    {
      "id": "openalex-w4413927182",
      "source": "openalex-discovery",
      "title": "Towards Optimized Arithmetic Circuits with MLIR",
      "authors": [
        {
          "name": "Louis V. Ledoux",
          "affiliation": "Institut National des Sciences Appliquées de Lyon"
        },
        {
          "name": "Pierre Cochard",
          "affiliation": "Institut National des Sciences Appliquées de Lyon"
        },
        {
          "name": "Florent de Dinechin",
          "affiliation": "Institut National des Sciences Appliquées de Lyon"
        }
      ],
      "year": "2025",
      "venue": "WiPiEC Journal - Works in Progress in Embedded Computing Journal | Vol. 11 (Issue 1)",
      "type": "research-paper",
      "abstract": "\"Numerical programs are typically conceived using real numbers.However, programming languages and compiler infrastructures operate at a lower abstraction level, constrained by fixed-width machine arithmetic.This abstraction gap limits the scope of legal arithmetic optimizations and complicates the generation of efficient circuits for application-specific hardware. This work introduces a set of MLIR dialects that explicitly separate concerns between real-valued computation and low-level arithmetic representation.The RealArith dialect captures mathematical intent, enabling algebraic rewrites and approximation-aware transformations, while the FixedPointArith dialect expresses quantized arithmetic with fine-grained control over bit widths.We implement an end-to-end lowering flow that performs polynomial approximation, generating fixed-point Horner-form architectures tailored for hardware synthesis.This separation enables arithmetic optimizations beyond those supported by conventional compilers. We present early hardware results on signal processing benchmarks, demonstrating the potential of our approach for arithmetic-aware circuit generation.Our evaluation includes multiple polynomial approximation schemes, including single and uniform piecewise forms, which highlight trade-offs between arithmetic complexity and memory usage, and compares against Vitis HLS and standard MLIR lowering baselines.\"",
      "paperUrl": "https://wipiec.digitalheritage.me/index.php/wipiecjournal/article/download/90/75",
      "sourceUrl": "https://doi.org/10.64552/wipiec.v11i1.90",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Florent de Dinechin"
      ]
    },
    {
      "id": "openalex-w4407848518",
      "source": "openalex-discovery",
      "title": "The MLIR Transform Dialect",
      "authors": [
        {
          "name": "Martin Paul Lücke",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        },
        {
          "name": "William S. Moses",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Michel Steuwer",
          "affiliation": "Technische Universität Berlin"
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "To take full advantage of a specific hardware target, performance engineers need to gain control on compilers in order to leverage their domain knowledge about the program and hardware. Yet, modern compilers are poorly controlled, usually by configuring a sequence of coarse-grained monolithic black-box passes, or by means of predefined compiler annotations/pragmas. These can be effective, but often do not let users precisely optimize their varying compute loads. As a consequence, performance engineers have to resort to implementing custom passes for a specific optimization heuristic, requiring compiler engineering expert knowledge. In this paper, we present a technique that provides fine-grained control of general-purpose compilers by introducing the Transform dialect, a controllable IR-based transformation system implemented in MLIR. The Transform dialect empowers performance engineers to optimize their various compute loads by composing and reusing existing---but currently hidden---compiler features without the need to implement new passes or even rebuilding the compiler. We demonstrate in five case studies that the Transform dialect enables precise, safe composition of compiler transformations and allows for straightforward integration with state-of-the-art search methods.",
      "paperUrl": "https://doi.org/10.1145/3696443.3708922",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Michel Steuwer",
        "Oleksandr Zinenko",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4416239860",
      "source": "openalex-discovery",
      "title": "The Configuration Wall: Characterization and Elimination of Accelerator Configuration Overhead",
      "authors": [
        {
          "name": "Josse Van Delm",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Anton Lydike",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Joren Dumoulin",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Jonas Crols",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Xun Yi",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Ryan Antonio",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Jackson Woodruff",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Marian Verhelst",
          "affiliation": "KU Leuven"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Contemporary compute platforms increasingly offload compute kernels from CPU to integrated hardware accelerators to reach maximum performance per Watt. Unfortunately, the time the CPU spends on setup control and synchronization has increased with growing accelerator complexity. For systems with complex accelerators, this means that performance can be configuration-bound. Faster accelerators are more severely impacted by this overlooked performance drop, which we call the configuration wall. Prior work evidences this wall and proposes ad-hoc solutions to reduce configuration overhead. However, these solutions are not universally applicable, nor do they offer comprehensive insights into the underlying causes of performance degradation. In this work, we first introduce a widely-applicable variant of the well-known roofline model to quantify when system performance is configuration-bound. To move systems out of the performance-bound region, we subsequently propose a domain-specific compiler abstraction and associated optimization passes. We implement the abstraction and passes in the MLIR compiler framework to run optimized binaries on open-source architectures to prove its effectiveness and generality. Experiments demonstrate a geomean performance boost of 2x on the open-source OpenGeMM system, by eliminating redundant configuration cycles and by automatically hiding the remaining configuration cycles. Our work provides key insights in how accelerator performance is affected by setup mechanisms, thereby facilitating automatic code generation for circumventing the configuration wall.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3760250.3762225",
      "sourceUrl": "https://doi.org/10.1145/3760250.3762225",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Lydike",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4407857939",
      "source": "openalex-discovery",
      "title": "Tensorize: Fast Synthesis of Tensor Programs from Legacy Code using Symbolic Tracing, Sketching and Solving",
      "authors": [
        {
          "name": "Alexander Brauckmann",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Luc Jaulmes",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "José Wesley de Souza Magalhães",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Elizabeth Polgreen",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Michael O’Boyle",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Tensor domain specific languages (DSLs) achieve substantial performance due to high-level compiler optimization and hardware acceleration. However, to achieve such performance for existing applications requires the programmer to manual rewrite their legacy code in evolving Tensor DSLs. Prior efforts to automate this translation face significant scalability issues which greatly reduces their applicability to real-world code. <br/><br/>This paper presents TENSORIZE, a novel MLIR-based compiler approach to automatically lift legacy code to high level Tensor DSLs using program synthesis. TENSORIZE uses a symbolic trace of the legacy program as a specification and automatically selects sketches from the target Tensor DSLs to drive the program synthesis. It uses an algebraic solver to rapidly simplify the specification, resulting in a fast, automatic approach that is correct by design. We evaluate TENSORIZE on several legacy code benchmarks and compare against state-of-the-art techniques. Tensorize is able to lift more code than prior schemes, is an order of magnitude faster in synthesis time, and guarantees correctness by construction.",
      "paperUrl": "https://www.research.ed.ac.uk/en/publications/57c9f206-5528-4a06-9855-9c8ebfc66a5d",
      "sourceUrl": "https://doi.org/10.1145/3696443.3708956",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexander Brauckmann"
      ]
    },
    {
      "id": "openalex-w4407231834",
      "source": "openalex-discovery",
      "title": "Tensor Evolution: A Framework for Fast Evaluation of Tensor Computations using Recurrences",
      "authors": [
        {
          "name": "Javed Absar",
          "affiliation": ""
        },
        {
          "name": "Samarth Narang",
          "affiliation": ""
        },
        {
          "name": "Muthu Manikandan Baskaran",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper introduces a new mathematical framework for analysis and optimization of tensor expressions within an enclosing loop. Tensors are multi-dimensional arrays of values. They are common in high performance computing (HPC) and machine learning domains. Our framework extends Scalar Evolution - an important optimization pass implemented in both LLVM and GCC - to tensors. Scalar Evolution (SCEV) relies on the theory of `Chain of Recurrences' for its mathematical underpinnings. We use the same theory for Tensor Evolution (TeV). While some concepts from SCEV map easily to TeV -- e.g. element-wise operations; tensors introduce new operations such as concatenation, slicing, broadcast, reduction, and reshape which have no equivalent in scalars and SCEV. Not all computations are amenable to TeV analysis but it can play a part in the optimization and analysis parts of ML and HPC compilers. Also, for many mathematical/compiler ideas, applications may go beyond what was initially envisioned, once others build on it and take it further. We hope for a similar trajectory for the tensor-evolution concept.",
      "paperUrl": "https://arxiv.org/pdf/2502.03402",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2502.03402",
      "tags": [
        "ML",
        "Performance"
      ],
      "matchedAuthors": [
        "Javed Absar"
      ]
    },
    {
      "id": "openalex-w4416048708",
      "source": "openalex-discovery",
      "title": "TPDE: A Fast Adaptable Compiler Back-End Framework",
      "authors": [
        {
          "name": "Tobias Schwarz",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Tobias Kamm",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Alexis Engelke",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Fast machine code generation is especially important for fast start-up just-in-time compilation, where the compilation time is part of the end-to-end latency. However, widely used compiler frameworks like LLVM do not prioritize fast compilation and require an extra IR translation step increasing latency even further; and rolling a custom code generator is a substantial engineering effort, especially when targeting multiple architectures. Therefore, in this paper, we present TPDE, a compiler back-end framework that adapts to existing code representations in SSA form. Using an IR-specific adapter providing canonical access to IR data structures and a specification of the IR semantics, the framework performs one analysis pass and then performs the compilation in just a single pass, combining instruction selection, register allocation, and instruction encoding. The generated target instructions are primarily derived code written in high-level language through LLVM's Machine IR, easing portability to different architectures while enabling optimizations during code generation. To show the generality of our framework, we build a new back-end for LLVM from scratch targeting x86-64 and AArch64. Performance results on SPECint 2017 show that we can compile LLVM-IR 8--24x faster than LLVM -O0 while being on-par in terms of run-time performance. We also demonstrate the benefits of adapting to domain-specific IRs in JIT contexts, particularly WebAssembly and database query compilation, where avoiding the extra IR translation further reduces compilation latency.",
      "paperUrl": "https://arxiv.org/pdf/2505.22610",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2505.22610",
      "tags": [
        "IR",
        "JIT",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexis Engelke",
        "Tobias Schwarz"
      ]
    },
    {
      "id": "openalex-w4414978573",
      "source": "openalex-discovery",
      "title": "Sound and Modular Activity Analysis for Automatic Differentiation in MLIR",
      "authors": [
        {
          "name": "Mai Jacob Peng",
          "affiliation": "McGill University"
        },
        {
          "name": "William S. Moses",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": "Constellium (France)"
        },
        {
          "name": "Christophe Dubach",
          "affiliation": "McGill University"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "Computing derivatives is paramount for multiple domains ranging from training neural networks to precise climate simulations. While derivatives can be generated by Automatic Differentiation (AD) tools, they often require aggressive optimization to avoid compromising program performance. One of the central optimizations consists of identifying inactive operations that do not contribute to the partial derivatives of interest. Multiple tools provide activity analyses for a variety of input languages, though often with only informal correctness guarantees. This paper formally defines activity analysis for AD as an abstract interpretation, proves its soundness, and implements it within the MLIR compiler infrastructure. To account for MLIR’s genericity, a subset of MLIR’s internal representation amenable to AD is formalized for the first time. Furthermore, the paper proposes a sound intraprocedural approximation of the whole-program activity analysis via function summaries along with a mechanism to automatically derive these summaries from function definitions. The implementation is evaluated on a differentiation-specific benchmark suite. It achieves a 1.24 geometric mean speedup on CPU and a 1.7 geometric mean speedup on GPU in the runtime of generated programs, when compared to a baseline that does not use activity analysis. The evaluation also demonstrates that the intraprocedural analysis with function summaries proves inactive 100% of instructions proven inactive by the whole-program analysis.",
      "paperUrl": "https://doi.org/10.1145/3763125",
      "sourceUrl": "",
      "tags": [
        "GPU",
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Mai Jacob Peng",
        "Oleksandr Zinenko",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4416004133",
      "source": "openalex-discovery",
      "title": "Scabbard: LLVM Instrumentation-aided Race Checking in CPU/GPU Unified Memory for AMD GPUs",
      "authors": [
        {
          "name": "Andrew Osterhout",
          "affiliation": "University of Utah"
        },
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Ganesh Gopalakrishnan",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Data Races are serious bugs whose occurrence in shared memory parallel and concurrent systems can cause unexpected non-deterministic outcomes and undermine the integrity of computational results. CPU/GPU systems that use Unified Heterogeneous Memory—for simplified memory sharing between GPUs and CPUs—are highly prone to data races. Unfortunately, none of the available data-race checkers are capable of detecting such data races. In this paper, we present Scabbard [18], the first tool that detects such races. Scabbard’s design is based on instrumenting the CPU and GPU codes by linking into LLVM’s compilation tool-chain as a pass plugin, to instrument the user’s code to to record writes, reads and synchronizations into trace files during execution, then subsequently performing offline trace analysis to report races. The main contribution of this paper is in detailing our algorithms and implementation challenges faced by us—newcomers to the LLVM’s and AMD’s ROCm/Hip ecosystems—that can help better the learning experience for future tool-builders who aim to use, improve and contribute to LLVM for the betterment of the GPU/CPU space.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767481",
      "sourceUrl": "",
      "tags": [
        "GPU"
      ],
      "matchedAuthors": [
        "Ganesh Gopalakrishnan",
        "Ignacio Laguna"
      ]
    },
    {
      "id": "openalex-w4409967258",
      "source": "openalex-discovery",
      "title": "SVF-SVC: Software Verification Using SVF (Competition Contribution)",
      "authors": [
        {
          "name": "Clement L. McGowan",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Michael Richards",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Yulei Sui",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract The Static Value-Flow Analysis Framework (SVF) is a tool that enables interprocedural static value-flow analysis for LLVM-based languages by leveraging sparse and on-demand analysis. This work, SVF-SVC, presents an adaptation of SVF for its debut in SV-COMP 2025. We detail our development which uses SVF as a library to correctly parse SV-COMP program specifications and produce witnesses statements for C programs in the ReachSafety, MemSafety and SoftwareSystems categories. We evaluate SVF-SVC’s performance in SV-COMP 2025 and pave the way for its participation in future editions.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-031-90660-2_21.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-031-90660-2_21",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Yulei Sui"
      ]
    },
    {
      "id": "openalex-w4415101244",
      "source": "openalex-discovery",
      "title": "SIMT/GPU Data Race Verification using ISCC and Intermediary Code Representations: A Case Study",
      "authors": [
        {
          "name": "Andrew Osterhout",
          "affiliation": ""
        },
        {
          "name": "Ganesh Gopalakrishnan",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "It is often difficult to write code that you can ensure will be executed in the right order when programing for parallel compute tasks. Due to the way that today's parallel compute hardware, primarily Graphical Processing Units (GPUs), allows you to write code. It is easy to write code that may result in one thread reading or modifying data before it should, thus resulting in a data race. It would be useful to have a tool that could verify that the code will execute as expected. However, most static analysis done at the language level has to be completely retooled to work on a different languages. Therefore, it would be of great use to be able to perform verification and analysis on the Memory Model of a parallel compute code, in a lower level intermediary representations that most languages pass through on their way to something that the GPU hardware can understand. This body of work aims to deal with the question of if there is still enough of the information in the intermediary representations to be able to perform memory model verification to check for data races. To determine this we plan to analyze as a case study the GeSpMM Sparse Matrix Multiplication Algorithm, implemented in CUDA C++ with the LLVM compiler and Julia with CUDA.jl.",
      "paperUrl": "https://arxiv.org/pdf/2503.08946",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2503.08946",
      "tags": [
        "C++",
        "CUDA",
        "GPU",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Ganesh Gopalakrishnan"
      ]
    },
    {
      "id": "openalex-w4411267975",
      "source": "openalex-discovery",
      "title": "Relaxing Alias Analysis: Exploring the Unexplored Space",
      "authors": [
        {
          "name": "Michel Weber",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Theodoros Theodoridis",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Zhendong Su",
          "affiliation": "ETH Zurich"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "Alias analysis is a fundamental compiler analysis that powers numerous optimizations. While research has focused on deriving more precise alias information assuming that the compiler will optimize better, recent work shows a negligible, or even negative, performance impact of alias information. In this work, we shift the perspective from refining to relaxing alias information, i.e. , removing information, to complement existing work and challenge that assumption systematically. Our study on a state-of-the-art compiler, LLVM, running the SPEC CPU 2017 benchmark suite, shows (1) a small overall impact —removing alias analysis entirely has little impact on the final binaries, (2) few influential queries —only a small fraction, namely ∼3%, of the alias information leads to changes in the final binary, and (3) lost potential —random relaxations can reduce execution time by 21% and binary size by 39% for certain cases, suggesting that compilers could better utilize alias information. Through this work, we advocate that it is beneficial for future research to avoid simply refining the general precision of alias analysis, but also to explore how to find and refine the most relevant queries, and how to more effectively utilize alias information.",
      "paperUrl": "https://doi.org/10.1145/3729254",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Theodoros Theodoridis"
      ]
    },
    {
      "id": "openalex-w4416003937",
      "source": "openalex-discovery",
      "title": "RISC-V Vectorization Coverage for HPC: A TSVC-Based Analysis",
      "authors": [
        {
          "name": "Hung-Ming Lai",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Pei‐Hung Lin",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Maya Gokhale",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Ivy Peng",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Hiren Patel",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The RISC-V Vector Extension (RVV) introduces scalable, vector-length agnostic operations with strong potential for high-performance computing (HPC). This paper presents a TSVC-based instruction coverage analysis of RVV to evaluate current compiler auto-vectorization support. We compile TSVC with GNU and LLVM under both vector-length agnostic (VLA) and vector-length specific (VLS) modes and analyze the emitted instructions against the RVV 1.0 specification. Our results quantify instruction usage across key groups, identify missed instructions, and classify the causes of failed vectorization, including compiler backend limitations, absent use cases in TSVC, and nontrivial or unsupported patterns. We also highlight TSVC’s limitations, including ambiguous kernel vectorizability and missing representations of modern HPC-relevant patterns. Finally, we suggest directions for enhancing benchmark suites to better reflect RVV capabilities and guide compiler development for HPC workloads.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767535",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "Performance"
      ],
      "matchedAuthors": [
        "Hung-Ming Lai",
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w7117991769",
      "source": "openalex-discovery",
      "title": "RISC-V RVV Support in Clang-Based Compiler",
      "authors": [
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Maker Innovations Series | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Clang can be viewed from two distinct perspectives. First, clang is a comprehensive toolchain, which supports compilation of C/C++/CUDA/Objective-С/Objective-C++/HIP source code into a machine code for a wide range of platforms. These include, but are not limited to, x86 (32/64-bit), ARM (32/64-bit), RISC-V(32/64-bit), Power PC (32/64-bit), as well as GPU architectures such as NVPTX (NVIDIA) and AMDGCN (AMD), both supporting 32-bit and 64-bit execution. Clang is designed to be highly portable and is compatible with multiple operating systems, including Linux, Windows, macOS, iOS, Android, FreeBSD, OpenBSD, and more. It offers competitive compile time/memory consumption/optimization level/feature set, making it a strong alternative to GCC-based toolchains. It provides extensive support for modern programming language standards, including full support for C99 and partial support for C11, C17, C23, and the evolving C2y standard. Similarly, for C++, it fully supports C++17, with partial implementations of C++20, C++23, and the upcoming C++2c standard. Clang also includes comprehensive support for OpenMP (fully implementing version 5.0 and partially supporting versions 5.1, 5.2, and 6.0). Additionally, it supports many nonstandard GCC extensions, making it a viable drop-in replacement for GCC-based compilers in numerous development environments.",
      "paperUrl": "https://doi.org/10.1007/979-8-8688-2169-1_2",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "GPU"
      ],
      "matchedAuthors": [
        "Alexey Bataev"
      ]
    },
    {
      "id": "openalex-w7117965258",
      "source": "openalex-discovery",
      "title": "RISC-V RVV Specific LLVM-Based Optimizations",
      "authors": [
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Maker Innovations Series | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The RISC-V Vector Extension (RVV) version 1.0 comes with some interesting challenges for compiler tools. LLVM tackles these challenges with some clever methods. The main idea behind LLVM’s strategy is using scalable vector types. This concept was first created for ARM’s Scalable Vector Extension (SVE), but it has been adapted successfully for RVV.",
      "paperUrl": "https://doi.org/10.1007/979-8-8688-2169-1_4",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "Alexey Bataev"
      ]
    },
    {
      "id": "openalex-w4414746986",
      "source": "openalex-discovery",
      "title": "RAPTOR: Practical Numerical Profiling of Scientific Applications",
      "authors": [
        {
          "name": "Faveo Hoerold",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Ivan R. Ivanov",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Akash Dhruv",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "William S. Moses",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Anshu Dubey",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Mohamed Wahib",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Jens Domke",
          "affiliation": "RIKEN Center for Computational Science"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The proliferation of low-precision units in modern high-performance architectures increasingly burdens domain scientists. Historically, the choice in HPC was easy: can we get away with 32 bit floating-point operations and lower bandwidth requirements, or is FP64 necessary? Driven by Artificial Intelligence, vendors introduce novel low-precision units for vector and tensor operations, and FP64 capabilities stagnate or are reduced. This forces scientists to re-evaluate their codes, but a trivial search-and-replace approach to go from FP64 to FP16 will not suffice. We introduce RAPTOR: a numerical profiling tool to guide scientists in their search for code regions where precision lowering is feasible. Using LLVM, we transparently replace high-precision computations using low-precision units, or emulate a user-defined precision. RAPTOR is a novel, feature-rich approach -- with focus on ease of use -- to change, profile, and reason about numerical requirements and instabilities, which we demonstrate with four real-world multi-physics Flash-X applications.",
      "paperUrl": "https://doi.org/10.1145/3712285.3759810",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Ivan R. Ivanov",
        "Jens Domke",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4406567738",
      "source": "openalex-discovery",
      "title": "QIR-EE",
      "authors": [
        {
          "name": "Elaine Wong",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Vicente Leyton Ortega",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "S. R. Johnson",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Daniel Claudino",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Austin Adams",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Sharmin Afrose",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Meenambika Gowrishankar",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Travis S. Humble",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2025",
      "venue": "OSTI OAI (U.S. Department of Energy Office of Scientific and Technical Information) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The QIR Execution Engine library provides interfaces for easily processing Quantum Intermediate Representation code using the LLVM execution engine.",
      "paperUrl": "https://www.osti.gov/biblio/2500354",
      "sourceUrl": "https://doi.org/10.11578/qiree/dc.20250114.1",
      "tags": [],
      "matchedAuthors": [
        "Austin Adams"
      ]
    },
    {
      "id": "openalex-w4407857728",
      "source": "openalex-discovery",
      "title": "Proteus: Portable Runtime Optimization of GPU Kernel Execution with Just-in-Time Compilation",
      "authors": [
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Konstantinos Parasyris",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "David Beckingsale",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In High-performance computing (HPC) fast application execution is the primary objective. HPC software is written in high-performance languages (C/C++, Fortran) and is statically compiled Ahead-of-Time (AOT) using optimizing compilers to generate fast code. AOT compilation optimizes source code with only limited information available at compile time, which precludes possible optimization leveraging runtime information. We propose Proteus, an easy-to-use, portable, and lightweight Just-In-Time (JIT) compilation approach to optimize GPU kernels at runtime. Proteus dynamically extracts, compiles, and optimizes language-agnostic LLVM IR to reduce compilation overhead while enhancing portability and versatility compared to language-specific solutions. Using a minimally intrusive annotation-based interface, Proteus specializes GPU kernels for input arguments and launch parameters. Evaluation on a diverse set of programs on AMD and NVIDIA GPUs shows that Proteus achieves significant end-to-end speedup, up to 2.8× for AMD and 1.78× on NVIDIA, over AOT optimization, while outperforming CUDA-specific Jitify with an average 1.23× speedp, thanks to reduced overhead and faster binary code in certain cases.",
      "paperUrl": "https://doi.org/10.1145/3696443.3708939",
      "sourceUrl": "",
      "tags": [
        "C++",
        "CUDA",
        "GPU",
        "IR",
        "JIT",
        "Performance"
      ],
      "matchedAuthors": [
        "Giorgis Georgakoudis",
        "Konstantinos Parasyris"
      ]
    },
    {
      "id": "openalex-w4417409444",
      "source": "openalex-discovery",
      "title": "Portable Targeted Sampling Framework Using LLVM",
      "authors": [
        {
          "name": "Ziwei Qiu",
          "affiliation": ""
        },
        {
          "name": "Mahyar Samani",
          "affiliation": ""
        },
        {
          "name": "Jason Lowe-Power",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Evaluating architectural ideas on realistic workloads is increasingly challenging due to the prohibitive cost of detailed simulation and the lack of portable sampling tools. Existing targeted sampling techniques are often tied to specific binaries, incur significant overhead, and make rapid validation across systems infeasible. To address these limitations, we introduce Nugget, a flexible framework that enables portable sampling across simulators, hardware, architectural differences, and libraries. Nugget leverages LLVM IR to perform binary-independent interval analysis, then generates lightweight, cross-platform executable snippets (nuggets), that can be validated natively on real hardware before use in simulation. This approach decouples samples from specific binaries, dramatically reduces analysis overhead, and allows researchers to iterate on sampling methodologies while efficiently validating samples across diverse systems.",
      "paperUrl": "https://arxiv.org/pdf/2509.02873",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2509.02873",
      "tags": [
        "IR",
        "Libraries"
      ],
      "matchedAuthors": [
        "Jason Lowe-Power",
        "Mahyar Samani"
      ]
    },
    {
      "id": "openalex-w4412605608",
      "source": "openalex-discovery",
      "title": "PolyMorphous: An MLIR-Based Polyhedral Compiler with Loop Transformation Primitives",
      "authors": [
        {
          "name": "Jinman Zhao",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Seyed Aryan Vahabpour",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Xinru Yue",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Kai-Ting Amy Wang",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Tarek S. Abdelrahman",
          "affiliation": "University of Toronto"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present PolyMorphous, an MLIR-based polyhedral compiler that exposes a set of loop-based scheduling primitives, providing users with ample control over optimizations for input code. The primitives are expressed in a new Schedule dialect that is based on MLIR's Transform dialect. PolyMorphous' polyhedral engine collapses the primitives into a polyhedral schedule and checks for its legality. If necessary, and possible, the engine corrects an illegal schedule into a legal one. PolyMorphous is evaluated using the PolyBench suite. The evaluation validates PolyMorphous' approach in two ways. First, it shows that PolyMorphous enables users to explore the optimization space, and that it results in well-optimized code. Second, it shows that PolyMorphous' correction of illegal schedules enables users to optimize code with fewer primitives and improves their productivity by alleviating the need for manual correction. Specifically, the evaluation shows that PolyMorphous optimized code performs as well as code optimized by Pluto+ with its optimization flags tuned to achieve the best performance for each benchmark. Further, for several benchmarks, PolyMorphous optimized code performs better, confirming the value of providing users with control over optimizations. Averaged over all the benchmarks, PolyMorphous optimized code has a speedup of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1.24 \\mathrm{x} / 1.25 \\mathrm{x}$</tex> over that optimized by Pluto+ on Arm/X86 systems. PolyMorphous brings the approach of empowering users of polyhedral compilers with control over optimizations to a community-developed infrastructure, promoting the approach's adoptability. It does so while delivering performant code.",
      "paperUrl": "https://doi.org/10.1109/ipdps64566.2025.00041",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Kai-Ting Amy Wang"
      ]
    },
    {
      "id": "openalex-w4411359950",
      "source": "openalex-discovery",
      "title": "Overlord: A C++ Overloading Inspector",
      "authors": [
        {
          "name": "Botond István Horváth",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Richárd Szalay",
          "affiliation": "Institute for Computer Science and Control"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Institute for Computer Science and Control"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Function overloading is a well-known technique available in most major programming languages, including <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathrm{C}^{++}$</tex>, to facilitate compile-time polymorphism. Due to the complex requirements imposed by the <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{C}^{++}$</tex> Language Standard, the behaviour of overload resolution can be surprising even for experienced <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$\\mathbf{C}^{++}$</tex> developers. This is exacerbated by the fact that if an unexpected function is selected in an overloaded situation, most compilers do not explain why. In addition, an overabundance of candidates can be a compilation time bottleneck. Despite its widespread usage, there are not many tools to help programmers analyse overload usage in a software project. We developed an overloading inspector tool, Overlord, based on the open-source LLVM/Clang Compiler Infrastructure. With Overlord developers can list the possible candidate functions for a call site, and obtain step-by-step reasoning about the candidate selection process. Additionally to its comprehension functionality the tool also provides profiling data on overload resolution times to help library authors streamline the set of available overloads for improving the compilation performance.",
      "paperUrl": "https://doi.org/10.1109/icpc66645.2025.00027",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4416800559",
      "source": "openalex-discovery",
      "title": "Optimizing VLIW Instruction Scheduling with Enhanced Heuristics and Optimal Functional Unit Assignment",
      "authors": [
        {
          "name": "Sijia Zhang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Peng Zhang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Jianbin Fang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Chun Huang",
          "affiliation": "National University of Defense Technology"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "List scheduling is a classic approach for instruction scheduling in many processor architectures, including VLIW-based DSPs. However, its effectiveness heavily depends on two factors: whether the heuristics used for instruction prioritization accurately reflect the features of the target architecture and whether instruction emission can fully utilize the available hardware resources. In this paper, we explore the fundamental challenges associated with instruction scheduling in VLIW architectures and propose an optimized list scheduling approach, referred to as HHLS. We enhance the scheduling heuristic by incorporating the propagation of instruction latencies from the scheduling boundary, which leads to more precise priority evaluation and improved filling of delay slots. We also introduce an innovative functional unit assignment strategy grounded in the Hungarian algorithm to maximize per-cycle parallelism through optimal resource allocation. The implementation of HHLS was carried out in LLVM 12 on the FT-M7032 VLIW DSP, and its performance was evaluated using 30 real-world benchmarks drawn from the PolyBench and DSPLib benchmark suites. The results demonstrate that HHLS achieves an average speedup of <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1.14 \\times$</tex> in execution cycles, and 89 % of the optimized basic blocks reach the theoretical minimum cycles, with a modest average increase in compilation time overhead of 3.25 %.",
      "paperUrl": "https://doi.org/10.1109/ispa67752.2025.00069",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Jianbin Fang"
      ]
    },
    {
      "id": "openalex-w4415155181",
      "source": "openalex-discovery",
      "title": "Optimizing FDTD Solvers for Electromagnetics: A Compiler-Guided Approach with High-Level Tensor Abstractions",
      "authors": [
        {
          "name": "Yifei He",
          "affiliation": ""
        },
        {
          "name": "Måns I. Andersson",
          "affiliation": ""
        },
        {
          "name": "Stefano Markidis",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Finite Difference Time Domain (FDTD) method is a widely used numerical technique for solving Maxwell's equations, particularly in computational electromagnetics and photonics. It enables accurate modeling of wave propagation in complex media and structures but comes with significant computational challenges. Traditional FDTD implementations rely on handwritten, platform-specific code that optimizes certain kernels while underperforming in others. The lack of portability increases development overhead and creates performance bottlenecks, limiting scalability across modern hardware architectures. To address these challenges, we introduce an end-to-end domain-specific compiler based on the MLIR/LLVM infrastructure for FDTD simulations. Our approach generates efficient and portable code optimized for diverse hardware platforms.We implement the three-dimensional FDTD kernel as operations on a 3D tensor abstraction with explicit computational semantics. High-level optimizations such as loop tiling, fusion, and vectorization are automatically applied by the compiler. We evaluate our customized code generation pipeline on Intel, AMD, and ARM platforms, achieving up to $10\\times$ speedup over baseline Python implementation using NumPy.",
      "paperUrl": "https://arxiv.org/pdf/2504.09118",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2504.09118",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Yifei He"
      ]
    },
    {
      "id": "openalex-w4411267910",
      "source": "openalex-discovery",
      "title": "Optimization-Directed Compiler Fuzzing for Continuous Translation Validation",
      "authors": [
        {
          "name": "Jaeseong Kwon",
          "affiliation": "Korea Advanced Institute of Science and Technology"
        },
        {
          "name": "Byung‐Kweon Jang",
          "affiliation": "Korea Advanced Institute of Science and Technology"
        },
        {
          "name": "Juneyoung Lee",
          "affiliation": ""
        },
        {
          "name": "Kihong Heo",
          "affiliation": "Korea Advanced Institute of Science and Technology"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "Incorrect compiler optimizations can lead to unintended program behavior and security vulnerabilities. However, the enormous size and complexity of modern compilers make it challenging to ensure the correctness of optimizations. The problem becomes more severe as compiler engineers continuously add new optimizations to improve performance and support new language features. In this paper, we propose Optimuzz, a framework to effectively detect incorrect optimization bugs in such continuously changing compilers. The key idea is to combine two complementary techniques: directed grey-box fuzzing and translation validation. We design a novel optimization-directed fuzzing framework that efficiently generates input programs to trigger specific compiler optimizations. Optimuzz then use existing translation validation tools to verify the correctness of the optimizations on the input programs. We instantiate our approach for two major compilers, LLVM and TurboFan. The results show that Optimuzz can effectively detect miscompilation bugs in these compilers compared to the state-of-the-art tools. We also applied Optimuzz to the latest version of LLVM and discovered 55 new miscompilation bugs.",
      "paperUrl": "https://doi.org/10.1145/3729275",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Juneyoung Lee"
      ]
    },
    {
      "id": "openalex-w4415419738",
      "source": "openalex-discovery",
      "title": "OpenVADL: An Open Source Implementation of the Vienna Architecture Description Language",
      "authors": [
        {
          "name": "Florian Freitag",
          "affiliation": "TU Wien"
        },
        {
          "name": "Linus Halder",
          "affiliation": "TU Wien"
        },
        {
          "name": "Benedikt Huber",
          "affiliation": "TU Wien"
        },
        {
          "name": "Benjamin Kasper",
          "affiliation": "TU Wien"
        },
        {
          "name": "Michael Nestler",
          "affiliation": "TU Wien"
        },
        {
          "name": "Kevin Per",
          "affiliation": "TU Wien"
        },
        {
          "name": "Matthias Raschhofer",
          "affiliation": "TU Wien"
        },
        {
          "name": "Alexander Ripar",
          "affiliation": "TU Wien"
        },
        {
          "name": "Johannes Zottele",
          "affiliation": "TU Wien"
        },
        {
          "name": "Andreas Krall",
          "affiliation": "TU Wien"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract OpenVADL is an open source implementation of the Vienna Architecture Description Language (VADL). VADL is a processor description language (PDL) that enables the concise formal specification of processor architectures. OpenVADL automatically generates an assembler, an LLVM based compiler and a QEMU based instruction set simulator from a single VADL processor specification. Automatic generation of synthesizable specifications in a hardware description language is under development. VADL strictly separates the instruction set architecture (ISA) specification from the microarchitecture (MiA) specification. VADL’s MiA specification operates at a higher level of abstraction compared to existing PDLs. This article introduces OpenVADL, describes the generator techniques in detail and shows the performance of the generators in an empirical evaluation. The evaluation demonstrates the capabilities of OpenVADL and its efficiency. An OpenVADL generated instruction set simulator is up to 77% faster than the official human written QEMU frontend for the RISC-V RV64IM instruction set architecture.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-032-03281-2_11.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-032-03281-2_11",
      "tags": [
        "Frontend",
        "Performance"
      ],
      "matchedAuthors": [
        "Andreas Krall",
        "Benedikt Huber",
        "Kevin Per"
      ]
    },
    {
      "id": "openalex-w4415746224",
      "source": "openalex-discovery",
      "title": "Nüwa: Enhancing MLIR Fuzzing with LLM-Driven Generation and Adaptive Mutation",
      "authors": [
        {
          "name": "Binghao Cao",
          "affiliation": "Northwest University"
        },
        {
          "name": "Weiyuan Tong",
          "affiliation": "Northwest University"
        },
        {
          "name": "Zhanyong Tang",
          "affiliation": "Northwest University"
        },
        {
          "name": "Zixu Wang",
          "affiliation": "Northwest University"
        },
        {
          "name": "Hao Huang",
          "affiliation": "Northwest University"
        },
        {
          "name": "Yuheng Yan",
          "affiliation": "Northwest University"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "MLIR, a modular compiler framework, evolves quickly, with regular updates expanding its dialects and operations across LLVM versions and downstream projects. This fast development reduces the effectiveness of traditional fuzzing tools, which test only a small portion of dialects, require extensive manual work (e.g., nearly ten thousand lines of C++ code), and do not match the update speed of MLIR. To address these challenges, we propose NÜwa, the first LLM-based approach for MLIR fuzzing. Nüwa employs a two-phase strategy: first generating valid operations by encoding constraints into LLMs prompts, then synthesizing multi-operation test cases by learning inter-operation dependencies. To enhance operation coverage, it incorporates high-coverage cases from MLIR's test suite and uses LLM-driven mutations to boost diversity. A self-improvement mechanism enhances the prompts using feedback from highquality test cases, improving the LLMs' understanding of MLIR's complex semantics. Nüwa demonstrates that the generation and mutation process can be fully automated via the intrinsic capabilities of llMs (including in-context learning), while being applicable to MLIR's fast evolution. The experimental study shows that NÜwa outperforms the state-of-the-art tools MLIRSmith and MLIRod, detecting <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$2.9 \\times$</tex> more unique bugs and achieving <tex xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">$1.6 \\times$</tex> greater code coverage. To date, Nüwa has identified 55 bugs in the MLIR framework, with 18 confirmed or fixed.",
      "paperUrl": "https://doi.org/10.1109/icsme64153.2025.00055",
      "sourceUrl": "",
      "tags": [
        "C++",
        "MLIR"
      ],
      "matchedAuthors": [
        "Zixu Wang"
      ]
    },
    {
      "id": "openalex-w4416069017",
      "source": "openalex-discovery",
      "title": "Noise Injection for__Performance Bottleneck Analysis",
      "authors": [
        {
          "name": "Aurélien Delval",
          "affiliation": ""
        },
        {
          "name": "Pablo de Oliveira Castro",
          "affiliation": ""
        },
        {
          "name": "William Jalby",
          "affiliation": ""
        },
        {
          "name": "Étienne Renault",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Bottleneck evaluation plays a crucial part in performance tuning of HPC applications, as it directly influences the search for optimizations and the selection of the best hardware for a given code. In this paper, we introduce a new model-agnostic, instruction-accurate framework for bottleneck analysis based on performance noise injection. This method provides a precise analysis that complements existing techniques, particularly in quantifying unused resource slack. Specifically, we classify programs based on whether they are limited by computation, data access bandwidth, or latency by injecting additional noise instructions that target specific bottleneck sources. Our approach is built on the LLVM compiler toolchain, ensuring easy portability across different architectures and microarchitectures which constitutes an improvement over many state-of-the-art tools. We validate our framework on a range of hardware benchmarks and kernels, including a detailedstudy of a sparse-matrix--vector product (SPMXV) kernel, where we successfully detect distinct performance regimes. These insights further inform hardware selection, as demonstrated by our comparative evaluation between HBM and DDR memory systems.",
      "paperUrl": "https://arxiv.org/pdf/2509.08446",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2509.08446",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Pablo de Oliveira Castro",
        "William Jalby"
      ]
    },
    {
      "id": "openalex-w4412989167",
      "source": "openalex-discovery",
      "title": "Multiple Resumptions and Local Mutable State, Directly",
      "authors": [
        {
          "name": "Serkan Muhcu",
          "affiliation": "Technische Universität Berlin"
        },
        {
          "name": "Philipp Schuster",
          "affiliation": "University of Tübingen"
        },
        {
          "name": "Michel Steuwer",
          "affiliation": "Technische Universität Berlin"
        },
        {
          "name": "Jonathan Immanuel Brachthäuser",
          "affiliation": "University of Tübingen"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue ICFP)",
      "type": "research-paper",
      "abstract": "While enabling use cases such as backtracking search and probabilistic programming, multiple resumptions have the reputation of being incompatible with efficient implementation techniques, such as stack switching. This paper sets out to resolve this conflict and thus bridge the gap between expressiveness and performance. To this end, we present a compilation strategy and runtime system for lexical effect handlers with support for multiple resumptions and stack-allocated mutable state. By building on garbage-free reference counting and associating stacks with stable prompts, our approach enables constant-time continuation capture and resumption when resumed exactly once, as well as constant-time state access. Nevertheless, we also support multiple resumptions by copying stacks when necessary. We practically evaluate our approach by implementing an LLVM backend for the Effekt language. A performance comparison with state-of-the-art systems, including dynamic and lexical effect handler implementations, suggests that our approach achieves competitive performance and the increased expressiveness only comes with limited overhead.",
      "paperUrl": "https://doi.org/10.1145/3747529",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "Performance"
      ],
      "matchedAuthors": [
        "Michel Steuwer"
      ]
    },
    {
      "id": "openalex-w4414790426",
      "source": "openalex-discovery",
      "title": "Mojo: MLIR-based Performance-Portable HPC Science Kernels on GPUs for the Python Ecosystem",
      "authors": [
        {
          "name": "William F. Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Tatiana Melnichenko",
          "affiliation": "Knoxville College"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Wael Elwasif",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Philip W. Fackler",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Rafael Ferreira da Silva",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Keita Teranishi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We explore the performance and portability of the novel Mojo language for scientific computing workloads on GPUs. As the first language based on the LLVM's Multi-Level Intermediate Representation (MLIR) compiler infrastructure, Mojo aims to close performance and productivity gaps by combining Python's interoperability and CUDA-like syntax for compile-time portable GPU programming. We target four scientific workloads: a seven-point stencil (memory-bound), BabelStream (memory-bound), miniBUDE (compute-bound), and Hartree-Fock (compute-bound with atomic operations); and compare their performance against vendor baselines on NVIDIA H100 and AMD MI300A GPUs. We show that Mojo's performance is competitive with CUDA and HIP for memory-bound kernels, whereas gaps exist on AMD GPUs for atomic operations and for fast-math compute-bound kernels on both AMD and NVIDIA GPUs. Although the learning curve and programming requirements are still fairly low-level, Mojo can close significant gaps in the fragmented Python ecosystem in the convergence of scientific computing and AI.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767573",
      "sourceUrl": "",
      "tags": [
        "AI",
        "CUDA",
        "GPU",
        "Infrastructure",
        "MLIR",
        "Mojo",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w7108439211",
      "source": "openalex-discovery",
      "title": "Minicursos do SSCAD 2025",
      "authors": [
        {
          "name": "Daniel Cordeiro",
          "affiliation": ""
        },
        {
          "name": "Calebe P. Bianchini",
          "affiliation": ""
        },
        {
          "name": "Ricardo dos Santos Ferreira",
          "affiliation": ""
        },
        {
          "name": "Gustavo Leite",
          "affiliation": ""
        },
        {
          "name": "Carlos E. C. Barbosa",
          "affiliation": ""
        },
        {
          "name": "Hervé Yviquel",
          "affiliation": ""
        },
        {
          "name": "Sandro Rigo",
          "affiliation": ""
        },
        {
          "name": "Aleardo Manacero",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Esta edição do Livro de Minicursos do SSCAD reúne o material didático elaborado pelos autores dos três minicursos apresentados durante o XXVI Simpósio em Sistemas Computacionais de Alto Desempenho, realizado de 28 a 31 de outubro de 2025, em Bonito, MS. O primeiro minicurso propõe uma abordagem prática para o uso de Modelos de Linguagem de Grande Escala (Large Language Models, LLMs) como ferramenta de apoio à produção de material didático e à pesquisa nas áreas de Arquitetura de Computadores e Computação de Alto Desempenho. O capítulo apresenta conceitos fundamentais de engenharia de prompt, detalha as funcionalidades dos principais softwares de notebooks interativos — Jupyter Notebook e Google Colab — utilizados para o desenvolvimento e execução de código, e traz exemplos interativos de sistemas construídos com LLMs. Os exemplos exploram temas como ciência de dados, aprendizado de máquina, computação de alto desempenho, além de abordar arquiteturas de computadores avançadas, dedicadas e específicas, métodos de avaliação, medição e predição de desempenho. O segundo minicurso oferece um tutorial detalhado sobre a adição de novas instruções ao backend RISCV da infraestrutura do compilador LLVM. O tutorial descreve o passo a passo de um projeto de uma pequena extensão chamada “xmatrix”, que introduz 32 registradores matriciais dedicados de 512 bits (matrizes de 4 × 4 elementos de 32 bits) e um conjunto restrito de operações aritméticas e de carga/armazenamento. Por fim, o terceiro minicurso aborda os mecanismos do Open MPI 5.0 — uma das APIs mais empregadas no desenvolvimento de aplicações paralelas em ambientes de alto desempenho—que simplificam as atividades de depuração e ajustes finos (tuning). O conteúdo inicia-se com uma breve introdução à MPI (Message Passing Interface), seguida de uma explanação detalhada da Modular Component Architecture (MCA), a qual concede ao desenvolvedor um conjunto extensivo de opções para ajuste e depuração de programas MPI. O capítulo termina com a exposição dos aspectos práticos do processo de tuning, incluindo a aplicação das ferramentas Valgrind e Memchecker para a depuração de aplicações paralelas. Acreditamos que este livro permitirá que estudantes, pesquisadores, profissionais e entusiastas das áreas de Arquitetura de Computadores e Processamento de Alto Desempenho tenham acesso consistente e aprofundado ao conhecimento apresentado no SSCAD 2025, oferecendo um recurso sólido, claro e duradouro para o aprofundamento dos estudos, mesmo àqueles que não puderam participar do evento presencialmente.",
      "paperUrl": "https://books-sol.sbc.org.br/index.php/sbc/catalog/download/184/840/1748",
      "sourceUrl": "https://doi.org/10.5753/sbc.18460.5",
      "tags": [
        "Backend"
      ],
      "matchedAuthors": [
        "Sandro Rigo"
      ]
    },
    {
      "id": "openalex-w4404403133",
      "source": "openalex-discovery",
      "title": "MimIR: An Extensible and Type-Safe Intermediate Representation for the DSL Age",
      "authors": [
        {
          "name": "Roland Leißa",
          "affiliation": "University of Mannheim"
        },
        {
          "name": "Marcel Ullrich",
          "affiliation": "Saarland University"
        },
        {
          "name": "Joachim Meyer",
          "affiliation": "Saarland University"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue POPL)",
      "type": "research-paper",
      "abstract": "Traditional compilers, designed for optimizing low-level code, fall short when dealing with modern, computation-heavy applications like image processing, machine learning, or numerical simulations. Optimizations should understand the primitive operations of the specific application domain and thus happen on that level. Domain-specific languages (DSLs) fulfill these requirements. However, DSL compilers reinvent the wheel over and over again as standard optimizations, code generators, and general infrastructure &amp; boilerplate code must be reimplemented for each DSL compiler. This paper presents MImIR, an extensible, higher-order intermediate representation. At its core, MImIR is a pure type system and, hence, a form of a typed lambda calculus. Developers can declare the signatures of new (domain-specific) operations, called axioms . An axiom can be the declaration of a function, a type constructor, or any other entity with a possibly polymorphic, polytypic, and/or dependent type. This way, developers can extend MImIR at any low or high level and bundle them in a plugin . Plugins extend the compiler and take care of optimizing and lowering the plugins' axioms. We show the expressiveness and effectiveness of MImIR in three case studies: Low-level plugins that operate at the same level of abstraction as LLVM, a regular-expression matching plugin, and plugins for linear algebra and automatic differentiation. We show that in all three studies, MImIR produces code that has state-of-the-art performance.",
      "paperUrl": "https://doi.org/10.1145/3704840",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Roland Leißa",
        "Sebastian Hack"
      ]
    },
    {
      "id": "openalex-w4407857938",
      "source": "openalex-discovery",
      "title": "Memory Safety Instrumentations in Practice: Usability, Performance, and Security Guarantees",
      "authors": [
        {
          "name": "Tina Jung",
          "affiliation": "Saarland University"
        },
        {
          "name": "Fabian Ritter",
          "affiliation": "Saarland University"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Memory safety violations due to C's undefined behavior, although well researched, still cause security breaches year by year. The most dangerous reported violations are spatial safety violations, where objects are accessed outside of their bounds. A wide variety of spatial safety sanitizers promise easy usage, broad security guarantees, and a low execution time overhead. However, only few of them are actually used. Instead of proposing yet another sanitizer, we dig deep into Low-Fat Pointers and SoftBound, two approaches to generate fast-to-execute safe programs with strong safety guarantees, and identify pain points in their usage. We found that seemingly small simplifying assumptions or limitations of the approaches often lead to spurious error reports. On top of analyzing usability issues, we set up a framework that abstracts common tasks of memory safety instrumentations, such as finding locations for checks and eliminating redundant checks. This abstraction allows us to draw a fair comparison between approaches when it comes to execution time and the number of safe accesses. We use this framework to give novel insights into how many accesses are provably safe, and where to attribute execution time overhead. Our findings help future research on memory safety instrumentations by identifying issues that current approaches face in their practical application. We make our LLVM-based instrumentation framework available to reduce the effort required to implement new instrumentations and to ease comparisons to Low-Fat Pointers and SoftBound.",
      "paperUrl": "https://doi.org/10.1145/3696443.3708926",
      "sourceUrl": "",
      "tags": [
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Sebastian Hack"
      ]
    },
    {
      "id": "openalex-w4411260251",
      "source": "openalex-discovery",
      "title": "Link-Time Optimization of Dynamic Casts in C++ Programs",
      "authors": [
        {
          "name": "Xufan Lu",
          "affiliation": "Instituto Politécnico de Lisboa"
        },
        {
          "name": "Nuno P. Lopes",
          "affiliation": "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "A core design principle of C++ is that users should only incur costs for features they actually use, both in terms of performance and code size. A notable exception to this rule is the run-time type information (RTTI) data, used for dynamic downcasts, exceptions, and run-time type introspection. For classes that define at least one virtual method, compilers generate RTTI data that uniquely identifies the type, including a string for the type name. In large programs with complex type inheritance hierarchies, this RTTI data can grow substantially in size. Moreover, dynamic casting algorithms are linear in the type hierarchy size, causing some programs to spend considerable time on these casts. The common workaround is to use the -fno-rtti compiler flag, which disables RTTI data generation. However, this approach has significant drawbacks, such as disabling polymorphic exceptions and dynamic casts, and requiring the flag to be applied across the entire program due to ABI changes. In this paper, we propose a new link-time optimization to mitigate both the performance and size overhead associated with dynamic casts and RTTI data. Our optimization replaces costly library calls for downcasts with short instruction sequences and eliminates unnecessary RTTI data by modifying vtables to remove RTTI slots. Our prototype, implemented in the LLVM compiler, demonstrates an average speedup of 1.4",
      "paperUrl": "https://doi.org/10.1145/3729266",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Performance"
      ],
      "matchedAuthors": [
        "Nuno P. Lopes"
      ]
    },
    {
      "id": "openalex-w4412610451",
      "source": "openalex-discovery",
      "title": "Leveraging Compilation Statistics for Compiler Phase Ordering",
      "authors": [
        {
          "name": "Jiayu Zhao",
          "affiliation": "University of Leeds"
        },
        {
          "name": "Chunwei Xia",
          "affiliation": "University of Leeds"
        },
        {
          "name": "Zheng Wang",
          "affiliation": "University of Leeds"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Choosing the optimal order and combination of compiler optimization passes - known as phase ordering - can enhance the performance of compiled binaries. However, existing approaches struggle to capture the subtle interaction between compiler passes and waste time on low-profitable pass sequences. We introduce CITROEN, a better approach for compiler phase ordering. CITROEN leverages pass-related compilation statistics to reject low-profitable compiler pass sequences to reduce the overhead of phase ordering search. It employs Bayesian optimization to navigate the search space, using compilation statistics instead of traditional tuning parameters to build an online cost model that provides both the performance prediction and the prediction uncertainty of compilation configurations. It dynamically allocates search iterations across source files to optimize search time in multi-file programs. We evaluate CITROEN by integrating it with the LLVM compiler and applying it to benchmarks from cBench and SPEC CPU 2017. CITROEN outperforms existing autotuning methods, discovering high-performing configurations quicker with fewer search iterations.",
      "paperUrl": "https://doi.org/10.1109/ipdps64566.2025.00054",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Zheng Wang"
      ]
    },
    {
      "id": "openalex-w7118002521",
      "source": "openalex-discovery",
      "title": "LLVM SLP Vectorizer",
      "authors": [
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Maker Innovations Series | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Superword-Level Parallelism (SLP) vectorization is an advanced optimization technique designed to exploit parallelism in straight-line code segments. This approach focuses on identifying and combining independent, compatible scalar instructions into their vector equivalents, potentially yielding significant performance improvements.",
      "paperUrl": "https://doi.org/10.1007/979-8-8688-2169-1_6",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Alexey Bataev"
      ]
    },
    {
      "id": "openalex-w7117961722",
      "source": "openalex-discovery",
      "title": "LLVM Loop Vectorizer",
      "authors": [
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Maker Innovations Series | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Loop Vectorizer is a well-known optimization technique designed to enhance performance by transforming scalar code into vectorized form. This technique specifically targets loops, which are often the most time-consuming constructs in a program. By replacing scalar instructions with vector instructions, loop vectorization can significantly improve execution time.",
      "paperUrl": "https://doi.org/10.1007/979-8-8688-2169-1_5",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Alexey Bataev"
      ]
    },
    {
      "id": "openalex-w7118015169",
      "source": "openalex-discovery",
      "title": "LLVM Compiler for RISC-V Architecture",
      "authors": [
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Maker Innovations Series | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/979-8-8688-2169-1",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Alexey Bataev"
      ]
    },
    {
      "id": "openalex-w7124844187",
      "source": "openalex-discovery",
      "title": "LLVM Compiler Generator in OpenVADL",
      "authors": [
        {
          "name": "Kevin Per",
          "affiliation": "TU Wien"
        }
      ],
      "year": "2025",
      "venue": "reposiTUm (TU Wien) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Das Entwickeln von Prozessoren ist eine arbeitsintensive Aufgabe. Jede Änderung erfordert Änderungen der Toolchain. Gleichzeitig benötigt schnelle Entwicklung auch schnelle Entwicklungsiterationen. Die Vienna Architecture Description Language (VADL) ist eine Prozessorbeschreibungssprache, wo die Spezifikation als goldenes Modell dient. Die quelloffene Implementierung, OpenVADL, generiert mehrere Artifakte für das schnelle Entwickeln von Prozessoren aus der Spezifikation. Diese Arbeit präsentiert den Compiler Generator für einen LLVM-basierten C Compiler. Die größte Herausforderung ist die Ableitung von Instruktionsselektionsmustern aus den Instruktionsverhaltensgraphen in einer VADL Spezifikation. Unser Ansatz analysiert und klassifiziert jede Instruktion anhand des Instruktionsverhaltens, um dann systematisch Muster für jede Klasse zu generieren.Schließlich werden die generierten gegen öffentlich verfügbare Compiler für die Architekturen RV32IM, RV64IM und AArch64 verglichen.",
      "paperUrl": "https://doi.org/10.34726/hss.2025.122088",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Kevin Per"
      ]
    },
    {
      "id": "openalex-w4407848466",
      "source": "openalex-discovery",
      "title": "LLM-Vectorizer: LLM-Based Verified Loop Vectorizer",
      "authors": [
        {
          "name": "Jubi Taneja",
          "affiliation": "Microsoft (United States)"
        },
        {
          "name": "Avery Laird",
          "affiliation": "University of Toronto"
        },
        {
          "name": "Cong Yan",
          "affiliation": "Microsoft (United States)"
        },
        {
          "name": "Madanlal Musuvathi",
          "affiliation": "Microsoft (United States)"
        },
        {
          "name": "Shuvendu K. Lahiri",
          "affiliation": "Microsoft (United States)"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state-machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high-performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.",
      "paperUrl": "https://doi.org/10.1145/3696443.3708929",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Jubi Taneja"
      ]
    },
    {
      "id": "openalex-w4407942537",
      "source": "openalex-discovery",
      "title": "LLM Compiler: Foundation Language Models for Compiler Optimization",
      "authors": [
        {
          "name": "Chris Cummins",
          "affiliation": "Menlo School"
        },
        {
          "name": "Volker Seeker",
          "affiliation": "Alpha Omega Alpha Medical Honor Society"
        },
        {
          "name": "Dejan Grubisic",
          "affiliation": "Alpha Omega Alpha Medical Honor Society"
        },
        {
          "name": "Baptiste Rozière",
          "affiliation": ""
        },
        {
          "name": "Jonas Gehring",
          "affiliation": ""
        },
        {
          "name": "Gabriel Synnaeve",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": "Alpha Omega Alpha Medical Honor Society"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce LLM Compiler, a suite of robust, openly available, pre-trained models specifically designed for compiler tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The models have been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and have undergone instruction fine-tuning to interpret compiler behavior. To demonstrate the utility of these research tools, we also present fine-tuned versions of the models with enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. Our aim is to provide scalable, cost-effective foundational models for further research and development in compiler optimization by both academic researchers and industry practitioners. Since we released LLM Compiler the community has quantized, repackaged, and downloaded the models over 250k times.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3708493.3712691",
      "sourceUrl": "https://doi.org/10.1145/3708493.3712691",
      "tags": [
        "GPU",
        "IR"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w4417451429",
      "source": "openalex-discovery",
      "title": "LEGO: A Layout Expression Language for Code Generation of Hierarchical Mapping",
      "authors": [
        {
          "name": "Amir Mohammad Tavakkoli",
          "affiliation": ""
        },
        {
          "name": "Cosmin E. Oancea",
          "affiliation": ""
        },
        {
          "name": "Mary Hall",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "ArXiv.org | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We describe LEGO, a new approach to optimizing data movement whereby code is expressed as a layout-independent computation and composed with layouts for data and computation. This code generator organization derives complex indexing expressions associated with hierarchical parallel code and data movement for GPUs. LEGO maps from layout specification to indexing expressions, and can be integrated into existing compilers and code templates. It facilitates the exploration of data layouts in combination with other optimizations. We demonstrate LEGO's integration with the Triton and MLIR compilers, and with CUDA templates. We show that LEGO is capable of deriving performance competitive with Triton, and shows broad applicability for data and thread layout mapping optimizations in its integration with CUDA and MLIR.",
      "paperUrl": "https://arxiv.org/pdf/2505.08091",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2505.08091",
      "tags": [
        "CUDA",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Amir Mohammad Tavakkoli"
      ]
    },
    {
      "id": "openalex-w4413926451",
      "source": "openalex-discovery",
      "title": "Key Operator Vectorization for LeNet and ResNet Based on Buddy Compiler",
      "authors": [
        {
          "name": "Juncheng Chen",
          "affiliation": "Beijing University of Technology"
        },
        {
          "name": "Weiwei Chen",
          "affiliation": "Beijing University of Technology"
        },
        {
          "name": "Zhi Cai",
          "affiliation": "Beijing University of Chemical Technology"
        }
      ],
      "year": "2025",
      "venue": "Applied Sciences | Vol. 15 (Issue 17)",
      "type": "research-paper",
      "abstract": "Deep learning has emerged as a prominent focus in both academia and industry, with a wide range of models being applied across diverse domains. Fast and efficient model inference is essential for the practical deployment of deep learning models. Under specific hardware constraints, accelerating inference remains a key research challenge. Common techniques for model acceleration include quantization, pruning, and vectorization. Although quantization and pruning primarily reduce model precision or complexity to enhance efficiency, this paper concentrates on vectorization, a technique that accelerates models by increasing the parallelism of operator execution. Based on the open-source Buddy-MLIR project, this work implements vectorization optimizations for Matmul, Conv2d, and Max Pooling operations to improve inference performance. These optimizations are designed as compiler passes and integrated into the Buddy-MLIR framework, offering a general solution for vectorizing such operators. Two optimization approaches are proposed: general vectorization and adaptive vectorization. Compared to the standard MLIR lowering pipeline and the fully optimized LLVM backend, the proposed general and adaptive vectorization methods reduce the inference latency of LeNet-5 by 26.7% and 37.3%, respectively. For the more complex ResNet-18 model, these methods achieve latency reductions of 79.9% and 82.6%, respectively.",
      "paperUrl": "https://www.mdpi.com/2076-3417/15/17/9523/pdf?version=1756547094",
      "sourceUrl": "https://doi.org/10.3390/app15179523",
      "tags": [
        "Backend",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Weiwei Chen"
      ]
    },
    {
      "id": "openalex-w4414988634",
      "source": "openalex-discovery",
      "title": "Interactive Bitvector Reasoning using Verified Bit-Blasting",
      "authors": [
        {
          "name": "Henrik Böving",
          "affiliation": ""
        },
        {
          "name": "Siddharth Bhat",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Alex C. Keizer",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Alex Keizer",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Léon Frenot",
          "affiliation": "École Normale Supérieure de Lyon"
        },
        {
          "name": "Abdalrhman Mohamed",
          "affiliation": "Stanford University"
        },
        {
          "name": "Léo Stefanesco",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Harun Khan",
          "affiliation": "Stanford University"
        },
        {
          "name": "Joshua Clune",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Clark Barrett",
          "affiliation": "Stanford University"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "Bit-blasting SMT solvers enable efficient automatic reasoning about bitvectors, which are fundamental for the verification of compiler backends, cryptographic algorithms, hardware designs and other soft- or hardware tasks. Despite the clear demand for efficient bitvector reasoning infrastructure and the impressive advancements in state-of-the-art bit-blasting SMT solvers such as Bitwuzla, effective bitvector reasoning within interactive theorem provers (ITPs) remains a challenge, hindering their use for mechanized proofs. Incomplete bitvector libraries, unavailable or only partially integrated decision procedures, complex and hard-to-bitblast operations, and limited integration with the host language prevent the wide adoption of bitvector reasoning in proving contexts. We introduce bv_decide: the first end-to-end verified bitblaster designed for interactive bitvector reasoning in a dependently-typed ITP . Our verified bitblaster is scalable, comes with a complete end-to-end proof (trusting only the Lean compiler and kernel), and is available as a proof tactic that allows interactive reasoning right from within a programming language, in our case Lean. We use Lean’s Functional But In-Place (FBIP) paradigm to efficiently encode our core data structures (e.g., AIGs), demonstrating that fast execution of an SMT solver need not come at the expense of rigorous formalization. We enable dependable interactive verification of user-written-code by basing Lean’s C-Style standard dataypes UInt/SInt on our bitvector type, adding a lowering from enums and structs to bitvectors to enable transparent bit-blasting support for composed types, and by offering an interactive tactic that either solves a goal or provides a counter-example. Moreover, we present the design of Lean’s canonical bitvector library, which supports all operations (with reasoning principles) for the SMT-LIB 2.7 standard (including overflow modeling), is fast-to-execute, and offers a comprehensive API and automation for bit-width-independent reasoning. We thoroughly evaluate our bit-blaster on a comprehensive set of benchmarks, including the full SMT-LIB dataset, where bv_decide solves more theorems than the state-of-the-art in verified bit-blasting, CoqQFBV. We also verify over 7000 SMT statements extracted from LLVM, providing the largest mechanized verification of LLVM rewrites to date, to our knowledge. By making bit-blasting bitvector reasoning a polished, well-supported, and interactive feature of modern ITPs, we enable effective, dependable white-box reasoning for bitvector-level verification.",
      "paperUrl": "https://doi.org/10.1145/3763167",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "Infrastructure",
        "Libraries",
        "Rust"
      ],
      "matchedAuthors": [
        "Alex Keizer",
        "Siddharth Bhat",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w7108469289",
      "source": "openalex-discovery",
      "title": "Implementing new RISC-V Instructions with the LLVM Compiler Infrastructure",
      "authors": [
        {
          "name": "Gustavo Leite",
          "affiliation": ""
        },
        {
          "name": "Carlos E. C. Barbosa",
          "affiliation": ""
        },
        {
          "name": "Hervé Yviquel",
          "affiliation": ""
        },
        {
          "name": "Sandro Rigo",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://books-sol.sbc.org.br/index.php/sbc/catalog/download/184/842/1751?inline=1",
      "sourceUrl": "https://doi.org/10.5753/sbc.18460.5.2",
      "tags": [
        "Infrastructure"
      ],
      "matchedAuthors": [
        "Sandro Rigo"
      ]
    },
    {
      "id": "openalex-w4416004510",
      "source": "openalex-discovery",
      "title": "Implementing OpenMP Offload Support in the AMD Next Generation Fortran Compiler",
      "authors": [
        {
          "name": "Dominik Adamski",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Sergio Afonso",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "A. Banerjee",
          "affiliation": "Advanced Micro Devices (United Kingdom)"
        },
        {
          "name": "Pranav Bhandarkar",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Kareem Ergawy",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Andrew Gozillon",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Michael Klemm",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Jan Leyonberg",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Dan Palermo",
          "affiliation": "Advanced Micro Devices (United States)"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern-day supercomputers are massively parallel, heterogeneous systems, many of which employ graphics processing units (GPUs) to accelerate applications. While C/C++, and also Python, gain traction in the high-performance computing (HPC) domain, Fortran continues to have a large developer base with new high-performance code written every day. The OpenMP application programming interface (API) is a key ingredient to provide multithreading and support for offloading execution to GPUs for HPC applications. To meet this need, AMD is developing the AMD Next Generation Fortran compiler (“AMD Flang”) to replace the existing “Classic Flang” compiler in the ROCm™ 7.0 release. This paper describes the general compilation pipeline of the AMD Next Generation Fortran Compiler. It shows how the compiler generates code for OpenMP target directives and their map clauses. The paper closes with a discussion of transformations in intermediate representation, such as implementing DO CONCURRENT using OpenMP intermediate code.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767478",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Flang",
        "Performance"
      ],
      "matchedAuthors": [
        "Michael Klemm",
        "Pranav Bhandarkar"
      ]
    },
    {
      "id": "openalex-w4411552520",
      "source": "openalex-discovery",
      "title": "IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation",
      "authors": [
        {
          "name": "Yuyang Rong",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Zhanghan Yu",
          "affiliation": "University of California, Davis"
        },
        {
          "name": "Zhenkai Weng",
          "affiliation": "University of California, Davis"
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Hao Chen",
          "affiliation": "University of California, Davis"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern compilers, such as LLVM, are complex. Due to their complexity, manual testing is unlikely to suffice, yet formal verification is difficult to scale. End-to-end fuzzing can be used, but it has difficulties in discovering LLVM backend problems for two reasons. First, frontend preprocessing and middle optimization shield the backend from seeing diverse inputs. Second, branch coverage cannot provide effective feedback as LLVM backend contains much reusable code. In this paper, we implement IRFuzzer to investigate the need of specialized fuzzing of the LLVM compiler backend. We focus on two approaches to improve the fuzzer: guaranteed input validity using constrained mutations to improve input diversity and new metrics to improve feedback quality. The mutator in IRFuzzer can generate a wide range of LLVM IR inputs, including structured control flow, vector types, and function definitions. The system instruments coding patterns in the compiler to monitor the execution status of instruction selection. The instrumentation not only provides new coverage feedback on the matcher table but also guides the mutator on architecture-specific intrinsics. We ran IRFuzzer on 29 mature LLVM backend targets. IRFuzzer discovered 78 new, confirmed bugs in LLVM upstream, none of which existing fuzzers could discover. This demonstrates that IRFuzzer is far more effective than existing fuzzers. Upon receiving our bug report, the developers have fixed 57 bugs and back-ported five fixes to LLVM 15, which shows that specialized fuzzing provides actionable insights to LLVM developers.",
      "paperUrl": "https://doi.org/10.1109/icse55347.2025.00130",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "Frontend",
        "IR",
        "Testing"
      ],
      "matchedAuthors": [
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w4411260257",
      "source": "openalex-discovery",
      "title": "First-Class Verification Dialects for MLIR",
      "authors": [
        {
          "name": "Mathieu Fehr",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Yuyou Fan",
          "affiliation": "University of Utah"
        },
        {
          "name": "Hugo Pompougnac",
          "affiliation": "Université Grenoble Alpes"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "MLIR is a toolkit supporting the development of extensible and composable intermediate representations (IRs) called dialects ; it was created in response to rapid changes in hardware platforms, programming languages, and application domains such as machine learning. MLIR supports development teams creating compilers and compiler-adjacent tools by factoring out common infrastructure such as parsers and printers. A major limitation of MLIR is that it is syntax-focused: it has no support for directly encoding the semantics of operations in its dialects. Thus, at present, the parts of MLIR tools that depend on semantics—optimizers, analyzers, verifiers, transformers—must all be engineered by hand. Our work makes formal semantics a first-class citizen in the MLIR ecosystem. We designed and implemented a collection of semantics-supporting MLIR dialects for encoding the semantics of compiler IRs. These dialects support a separation of concerns between three domains of expertise when building formal-methods-based tooling for compilers. First, compiler developers define their dialect’s semantics as a lowering (compilation transformation) from their dialect to one or more of ours. Second, SMT solver experts provide tools to optimize domain-specific high-level semantics and lower them to SMT queries. Third, tool builders create dialect-independent verification tools. We validate our work by defining semantics for five key MLIR dialects, defining a state-of-the-art SMT encoding for memory-based semantics, and building three dialect-agnostic tools, which we used to find five miscompilation bugs in upstream MLIR, verify a canonicalization pass, and also formally verify transfer functions for two dataflow analyses: “known bits” (that finds individual bits that are always zero or one in all executions) and “demanded bits” (that finds don’t-care bits). The transfer functions that we verify are improved versions of those in upstream MLIR; they detect on average 36.6% more known bits in real-world MLIR programs compared to the upstream implementation.",
      "paperUrl": "https://doi.org/10.1145/3729309",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Mathieu Fehr",
        "Tobias Grosser",
        "Yuyou Fan"
      ]
    },
    {
      "id": "openalex-w7116338778",
      "source": "openalex-discovery",
      "title": "Filling Performance Portability and High-Productivity Gaps for Scientific Applications with Julia and JACC",
      "authors": [
        {
          "name": "William Fredie Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Alexis Huante",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "J.L. Muñoz-Cobo",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jhonny Gonzalez",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Yuan Tang",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Claire Winogrodzki",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Philip Fackler",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Keita Teranishi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The performance gap in high-productivity languages such as Python, Julia, R, and Matlab is still an open research area in which significant investments are being made. However, the challenge of writing code once that performs efficiently across disparate hardware is made even greater by architectural heterogeneity. Our work focuses on JACC [3], a performance portable CPU/GPU library implementation for the scientific LLVM-based Julia programming language. Similar to Kokkos and SYCL in C++, JACC provides a simple high-level metaprogramming API for Julia. JACC leverages CPU and GPU vendor-specific backends, and thus its proposition is to reuse rather than reinvent current investments in the language. In addition, JACC is complementary to existing performance-portable solutions in Julia, such as KernelAbstractions.jl [2], which targets mainly GPUs using a fine-granularity programming model, similar to CUDA and HIP. Due to its novelty, JACC must still be evaluated across multidisciplinary science domains and heterogeneous hardware architectures.",
      "paperUrl": "https://doi.org/10.1145/3750720.3757298",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "C++",
        "CUDA",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w7111099953",
      "source": "openalex-discovery",
      "title": "Extending a RISC-V Core with Sub-FP8 Support for Machine Learning",
      "authors": [
        {
          "name": "Kathryn Chapman",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Fu-Jian Shen",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jhih-Kuan Lin",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq-Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern deep learning workloads demand massive computational resources, making energy-efficient computing paradigms essential. Currently, compiler and hardware support for emerging formats like FP8 (E5M2/E4M3), FP6 (E3M2/E2M3), and FP4 remain limited in the RISC-V ecosystem. This paper presents a scalar sub-FP8 ISA extension for RISC-V, with full LLVM toolchain and hardware support. Our design decouples format specification from instructions using the fcsr to store format information. Post-synthesis evaluation confirms hardware efficiency, using only an increase of 0.9% registers and 2.3% additional LUTs in CVA6, while delivering 20× lower cache miss rates for FP8 GEMM versus FP32 operations. In addition, we introduce future work on a sub-FP8 vector extension, with a focus on support for FP8, FP6, and FP4.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3742873.3755990",
      "sourceUrl": "https://doi.org/10.1145/3742873.3755990",
      "tags": [],
      "matchedAuthors": [
        "Fu-Jian Shen",
        "Jenq-Kuen Lee",
        "Kathryn Chapman"
      ]
    },
    {
      "id": "openalex-w4411260208",
      "source": "openalex-discovery",
      "title": "Exploiting Undefined Behavior in C/C++ Programs for Optimization: A Study on the Performance Impact",
      "authors": [
        {
          "name": "Lucian Popescu",
          "affiliation": "University of Lisbon"
        },
        {
          "name": "Nuno P. Lopes",
          "affiliation": "Instituto de Engenharia de Sistemas e Computadores Investigação e Desenvolvimento"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "The C and C++ languages define hundreds of cases as having undefined behavior (UB). These include, for example, corner cases where different CPU architectures disagree on the semantics of an instruction and the language does not want to force a specific implementation (e.g., shift by a value larger than the bitwidth). Another class of UB involves errors that the language chooses not to detect because it would be too expensive or impractical, such as dereferencing out-of-bounds pointers. Although there is a common belief within the compiler community that UB enables certain optimizations that would not be possible otherwise, no rigorous large-scale studies have been conducted on this subject. At the same time, there is growing interest in eliminating UB from programming languages to improve security. In this paper, we present the first comprehensive study that examines the performance impact of exploiting UB in C and C++ applications across multiple CPU architectures. Using LLVM, a compiler known for its extensive use of UB for optimizations, we demonstrate that, for the benchmarks and UB categories that we evaluated, the end-to-end performance gains are minimal. Moreover, when performance regresses, it can often be recovered through small improvements to optimization algorithms or by using link-time optimizations.",
      "paperUrl": "https://doi.org/10.1145/3729260",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Optimizations",
        "Performance",
        "Programming Languages",
        "Security"
      ],
      "matchedAuthors": [
        "Lucian Popescu",
        "Nuno P. Lopes"
      ]
    },
    {
      "id": "openalex-w4414585315",
      "source": "openalex-discovery",
      "title": "Evaluating LLVM OpenMP Offload Optimizations on NVIDIA GH200 Grace Hopper Superchip and AMD Instinct™ MI300A Accelerator Architectures",
      "authors": [
        {
          "name": "Kevin Sala",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Stephen L. Olivier",
          "affiliation": "Sandia National Laboratories"
        },
        {
          "name": "Rahulkumar Gayatri",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Shilei Tian",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-032-06343-4_10",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "Johannes Doerfert",
        "Kevin Sala",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4410034770",
      "source": "openalex-discovery",
      "title": "Equivalence Checking of a libm Port",
      "authors": [
        {
          "name": "Mark Baranowski",
          "affiliation": "University of Utah"
        },
        {
          "name": "Zvonimir Rakamarić",
          "affiliation": "University of Utah"
        },
        {
          "name": "Ganesh Gopalakrishnan",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract Recent advances in satisfiability modulo theories have brought practical software verification within reach. The advent of the LLVM project presents a common representation which allows verification between programs written in different languages such as C and Rust. New programming languages such as Rust often have less complete standard libraries. In this work, we explore using the SMACK software verifier to check the equivalence of math library routines from the libm library and their translations into native Rust programs.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-031-90653-4_12.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-031-90653-4_12",
      "tags": [
        "Libraries",
        "Programming Languages",
        "Rust"
      ],
      "matchedAuthors": [
        "Ganesh Gopalakrishnan"
      ]
    },
    {
      "id": "openalex-w7117851873",
      "source": "openalex-discovery",
      "title": "Eliminate Branches by Melding IR Instructions",
      "authors": [
        {
          "name": "Yuze Li",
          "affiliation": ""
        },
        {
          "name": "Srinivasan Ramachandra Sharma",
          "affiliation": ""
        },
        {
          "name": "Charitha Saumya",
          "affiliation": ""
        },
        {
          "name": "A.E. Butt",
          "affiliation": ""
        },
        {
          "name": "Kirshanthan Sundararajah",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "ArXiv.org | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Branch mispredictions cause catastrophic performance penalties in modern processors, leading to performance loss. While hardware predictors and profile-guided techniques exist, data-dependent branches with irregular patterns remain challenging. Traditional if-conversion eliminates branches via software predication but faces limitations on architectures like x86. It often fails on paths containing memory instructions or incurs excessive instruction overhead by fully speculating large branch bodies. This paper presents Melding IR Instructions (MERIT), a compiler transformation that eliminates branches by aligning and melding similar operations from divergent paths at the IR instruction level. By observing that divergent paths often perform structurally similar operations with different operands, MERIT adapts sequence alignment to discover merging opportunities and employs safe operand-level guarding to ensure semantic correctness without hardware predication. Implemented as an LLVM pass and evaluated on 102 programs from four benchmark suites, MERIT achieves a geometric mean speedup of 10.9% with peak improvements of 32x compared to hardware branch predictor, demonstrating the effectiveness with reduced static instruction overhead.",
      "paperUrl": "https://arxiv.org/pdf/2512.22390",
      "sourceUrl": "http://arxiv.org/abs/2512.22390",
      "tags": [
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Charitha Saumya"
      ]
    },
    {
      "id": "openalex-w4416004083",
      "source": "openalex-discovery",
      "title": "Dynamic Thread Coarsening for CPU and GPU OpenMP Code",
      "authors": [
        {
          "name": "Ivan R. Ivanov",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Jens Domke",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Toshio Endo",
          "affiliation": "Institute of Science Tokyo"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Thread coarsening is a well known optimization technique for GPUs. It enables instruction-level parallelism, reduces redundant computation, and can provide better memory access patterns. However, the presence of divergent control flow - cases where uniformity of branch conditions among threads cannot be proven at compile time - diminishes its effectiveness. In this work, we implement multi-level thread coarsening for CPU and GPU OpenMP code, by implementing a generic thread coarsening transformation on LLVM IR. We introduce dynamic convergence - a new technique that generates both coarsened and non-coarsened versions of divergent regions in the code and allows for the uniformity check to happen at runtime instead of compile time. We performed evalution on HecBench for GPU and LULESH for CPU. We found that best case speedup without dynamic convergence was 4.6% for GPUs and 2.9% for CPUs, while our approach achieved 7.5% for GPUs and 4.3% for CPUs.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767482",
      "sourceUrl": "",
      "tags": [
        "GPU",
        "IR"
      ],
      "matchedAuthors": [
        "Ivan R. Ivanov",
        "Jens Domke",
        "Johannes Doerfert",
        "Toshio Endo"
      ]
    },
    {
      "id": "openalex-w4411450172",
      "source": "openalex-discovery",
      "title": "Directed Testing in MLIR: Unleashing Its Potential by Overcoming the Limitations of Random Fuzzing",
      "authors": [
        {
          "name": "Weiyuan Tong",
          "affiliation": "Northwest University"
        },
        {
          "name": "Zixu Wang",
          "affiliation": "Northwest University"
        },
        {
          "name": "Zhanyong Tang",
          "affiliation": "Northwest University"
        },
        {
          "name": "Jianbin Fang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Yuqun Zhang",
          "affiliation": "Southern University of Science and Technology"
        },
        {
          "name": "Guixin Ye",
          "affiliation": "Northwest University"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on software engineering. | Vol. 2 (Issue FSE)",
      "type": "research-paper",
      "abstract": "MLIR is a new way of creating compiler infrastructures that can be easily reused and extended. Current MLIR fuzzing methods focus primarily on test case generation or mutation using randomly selected passes. However, they often overlook the hierarchical structure of MLIR, resulting in inefficiencies in bug detection, especially for issues triggered by downstream dialects. Random testing lacks a focused approach to exploring the code space, resulting in wasted resources on normal components and overlooking bug-prone areas. To address these limitations, we introduce MLIRTracer, a top-down fuzzing approach that targets the highest level of MLIR programs (tosa IR) with a directed testing strategy. Our method systematically traverses the hierarchical code space of MLIR, from tosa IR to the lower levels, while prioritizing tests of bug-prone areas through directed exploration. MLIRTracer has successfully detected 73 bugs, with 61 already resolved by the MLIR developers.",
      "paperUrl": "https://doi.org/10.1145/3729372",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "IR",
        "MLIR",
        "Testing"
      ],
      "matchedAuthors": [
        "Jianbin Fang",
        "Zixu Wang"
      ]
    },
    {
      "id": "openalex-w4408466405",
      "source": "openalex-discovery",
      "title": "Differentiable multi-physics solvers for extreme-scale geophysics simulations on GPUs",
      "authors": [
        {
          "name": "Ludovic Räss",
          "affiliation": "University of Lausanne"
        },
        {
          "name": "Ivan Utkin",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Albert de Montserrat",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Boris Kaus",
          "affiliation": ""
        },
        {
          "name": "Paul Tackley",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "William S. Moses",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Thibault Duretz",
          "affiliation": "Goethe University Frankfurt"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Although geodynamics and ice flow dynamics address distinct physical systems, they share significant computational and modelling challenges. Both require vast, data-intensive simulations on next-generation high-performance computing (HPC) platforms. With limited observational data, these models must be rigorously constrained to improve their predictive power. Our work focuses on differentiable modelling of Earth&amp;#8217;s largest ice sheets and high-resolution 3D geodynamic processes, such as magmatic systems and the formation of the Alps.We are developing differentiable multi-physics solvers for extreme-scale geophysical simulations on GPUs - &amp;#8706;GPU4GEO. These high-performance, scalable tools leverage advanced programming techniques, particularly automatic differentiation (AD) within the Julia programming language. Using Enzyme.jl, an AD tool integrated with the LLVM compiler, we combine differentiation with compiler optimisations. This approach enables highly efficient reverse-mode AD, achieving near-theoretical peak performance.Building on the GPU4GEO PASC project (2020&amp;#8211;2024), we are extending pseudo-transient solvers with differentiable modelling capabilities. The modular GPU4GEO software stack, composed of specialised Julia packages, provides solvers for diverse physical systems and customisable building blocks. By integrating Enzyme.jl into the entire stack, we enable high-performance AD on GPUs while maintaining support for distributed-memory parallelism via MPI. These developments ensure scalability on flagship supercomputers and facilitate efficient exploration of geophysical processes.This collaborative effort targets applications requiring large-scale simulations to address critical scientific challenges. The resulting computational tools are optimised for next-generation GPU architectures, offering transformative potential for geodynamics and glaciology research.",
      "paperUrl": "https://doi.org/10.5194/egusphere-egu25-16577",
      "sourceUrl": "",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4416004321",
      "source": "openalex-discovery",
      "title": "DiOMP-Offloading: Toward Portable Distributed Heterogeneous OpenMP",
      "authors": [
        {
          "name": "Baodi Shan",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Mauricio Araya–Polo",
          "affiliation": "Total (France)"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-performance computing faces rising core counts, increasing heterogeneity, and growing memory bandwidth. These trends complicate programmability, portability, and scalability, while traditional MPI + OpenMP struggles with distributed GPU memory and portable performance. We present DiOMP-Offloading, a framework unifying OpenMP target offloading with a Partitioned Global Address Space (PGAS) model. Built on LLVM-OpenMP and GASNet-EX, it centrally manages global memory and supports symmetric/asymmetric GPU allocations, enabling remote put/get operations. DiOMP also integrates OMPCCL, a portable device-side collective layer that harmonizes allocation lifecycles and address translation across vendor backends. By eliminating separate MPI + X stacks and abstracting replicated device memory and communication logic, DiOMP improves scalability and programmability. Experiments on large-scale NVIDIA A100, Grace Hopper, and AMD MI250X platforms show superior micro-benchmark and application performance, demonstrating that DiOMP-Offloading offers a more portable, scalable, and efficient path for heterogeneous supercomputing.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767505",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Baodi Shan",
        "Barbara Chapman"
      ]
    },
    {
      "id": "openalex-w4411486154",
      "source": "openalex-discovery",
      "title": "DX100: Programmable Data Access Accelerator for Indirection",
      "authors": [
        {
          "name": "Alireza Khadem",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Kamalavasan Kamalakkannan",
          "affiliation": "Los Alamos National Laboratory"
        },
        {
          "name": "Zhenyan Zhu",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Akash Poptani",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Yufeng Gu",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Jered Dominguez-Trujillo",
          "affiliation": "Los Alamos National Laboratory"
        },
        {
          "name": "Nishil Talati",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Daichi Fujiki",
          "affiliation": ""
        },
        {
          "name": "Scott Mahlke",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Galen Shipman",
          "affiliation": "Los Alamos National Laboratory"
        },
        {
          "name": "Reetuparna Das",
          "affiliation": "University of Michigan–Ann Arbor"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Indirect memory accesses frequently appear in applications where memory bandwidth is a critical bottleneck. Prior indirect memory access proposals, such as indirect prefetchers, runahead execution, fetchers, and decoupled access/execute architectures, primarily focus on improving memory access latency by loading data ahead of computation but still rely on the DRAM controllers to reorder memory requests and enhance memory bandwidth utilization. DRAM controllers have limited visibility to future memory accesses due to the small capacity of request buffers and the restricted memory-level parallelism of conventional core and memory systems. We introduce DX100, a programmable data access accelerator for indirect memory accesses. DX100 is shared across cores to offload bulk indirect memory accesses and associated address calculation operations. DX100 reorders, interleaves, and coalesces memory requests to improve DRAM row-buffer hit rate and memory bandwidth utilization. DX100 provides a general-purpose ISA to support diverse access types, loop patterns, conditional accesses, and address calculations. To support this accelerator without significant programming efforts, we discuss a set of MLIR compiler passes that automatically transform legacy code to utilize DX100. Experimental evaluations on 12 benchmarks spanning scientific computing, database, and graph applications show that DX100 achieves performance improvements of 2.6x over a multicore baseline and 2.0x over the state-of-the-art indirect prefetcher.",
      "paperUrl": "https://doi.org/10.1145/3695053.3731015",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Scott Mahlke"
      ]
    },
    {
      "id": "openalex-w4416233844",
      "source": "openalex-discovery",
      "title": "DLAFI: Software-Based Fault Injection for Permanent Faults in Deep Learning Accelerators",
      "authors": [
        {
          "name": "S. M. Hadi Sadati",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Abraham Chan",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Udit Kumar Agarwal",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Karthik Pattabiraman",
          "affiliation": "University of British Columbia"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Deep learning accelerators (DLAs) are used in safety-critical applications, making their reliability an important goal. Permanent faults arising due to wear and tear and manufacturing defects are a particular concern for the reliability of DLAs. Unfortunately, existing permanent fault injection methods are either slow (hardware simulations) or inaccurate (software-level). We introduce DLAFI, an LLVM-based fault injection framework that accurately simulates the hardware behavior of systolic arrays (SAs)-the core compute components of DLAs, while achieving comparable speed as software-level injection. DLAFI models the SA’s scheduling strategy to dynamically map machine learning (ML) operations to the SA’s processing elements. Compared with hardware simulation-based fault injection, DLAFI enables the analysis of higher complexity ML applications such as object detection and large language models, and is three orders of magnitude faster overall. Using DLAFI, we evaluate the resilience of various ML workloads across SA sizes and scheduling strategies, and find that larger SAs reduce fault impact, balanced schedulers can reduce resilience, faults in final layers exhibit higher vulnerability, and vision models are more resilient than language models.",
      "paperUrl": "https://doi.org/10.1109/issre66568.2025.00055",
      "sourceUrl": "",
      "tags": [
        "ML"
      ],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w7127111771",
      "source": "openalex-discovery",
      "title": "CppInterOp: Advancing Interactive C++ for High Energy Physics",
      "authors": [
        {
          "name": "Aaron Jomy",
          "affiliation": ""
        },
        {
          "name": "Baidyanath Kundu",
          "affiliation": ""
        },
        {
          "name": "Wim Lavrijsen",
          "affiliation": ""
        },
        {
          "name": "Alexander Penev",
          "affiliation": ""
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Springer Link (Chiba Institute of Technology) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Cling C++ interpreter has transformed language bindings by enabling incremental compilation at runtime. This allows Python to interact with C++ on demand and lazily construct bindings between the two. The emergence of Clang-Repl, within the LLVM compiler framework, as a potential alternative to Cling highlights the need for a unified framework for interactive C++ technologies. We present CppInterOp, a C++ interoperability library that leverages Cling and LLVM’s Clang-Repl to provide a minimalist and backward-compatible API facilitating seamless language interoperability. This provides downstream interactive C++ tools with the compiler as a service by embedding Clang and LLVM as libraries in their codebases. By enabling dynamic Python interactions with static C++ codebases, CppInterOp enhances computational efficiency and rapid development in high-energy physics. The library offers reflection and ondemand JIT compilation APIs enabling cppyy, an automatic, run-time, Python- C++ bindings generator. We also demonstrate CppInterOp’s utility in diverse computing environments through its adoption as the runtime engine for xeuscpp, a Jupyter kernel designed for C++. CppInterOp is a general-purpose library inspired by the developments in the ROOT framework, which pushed the frontiers of interactive C++. It aims to extend this approach and serve as an integral component of ROOT, enhancing performance and resilience. This article introduces CppInterOp to the HEP community and showcases how it optimizes cross-language execution and computational tasks in high-energy physics, making it a valuable tool for researchers and developers.",
      "paperUrl": "https://doi.org/10.1051/epjconf/202533701165/pdf",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "JIT",
        "Libraries",
        "Performance"
      ],
      "matchedAuthors": [
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w4407758903",
      "source": "openalex-discovery",
      "title": "Comparison of Vectorization Capabilities of Different Compilers for X86 and ARM CPUs",
      "authors": [
        {
          "name": "Nazmus Sakib",
          "affiliation": ""
        },
        {
          "name": "Tarun Prabhu",
          "affiliation": ""
        },
        {
          "name": "Nandakishore Santhi",
          "affiliation": ""
        },
        {
          "name": "John Shalf",
          "affiliation": ""
        },
        {
          "name": "Abdel‐Hameed A. Badawy",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Most modern processors contain vector units that simultaneously perform the same arithmetic operation over multiple sets of operands. The ability of compilers to automatically vectorize code is critical to effectively using these units. Understanding this capability is important for anyone writing compute-intensive, high-performance, and portable code. We tested the ability of several compilers to vectorize code on x86 and ARM. We used the TSVC2 suite, with modifications that made it more representative of real-world code. On x86, GCC reported 54% of the loops in the suite as having been vectorized, ICX reported 50%, and Clang, 46%. On ARM, GCC reported 56% of the loops as having been vectorized, ACFL reported 54%, and Clang, 47%. We found that the vectorized code did not always outperform the unvectorized code. In some cases, given two very similar vectorizable loops, a compiler would vectorize one but not the other. We also report cases where a compiler vectorized a loop on only one of the two platforms. Based on our experiments, we cannot definitively say if any one compiler is significantly better than the others at vectorizing code on any given platform.",
      "paperUrl": "https://arxiv.org/pdf/2502.11906",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2502.11906",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Tarun Prabhu"
      ]
    },
    {
      "id": "openalex-w4407764185",
      "source": "openalex-discovery",
      "title": "CipherGuard: Compiler-aided Mitigation against Ciphertext Side-channel Attacks",
      "authors": [
        {
          "name": "Ke Jiang",
          "affiliation": ""
        },
        {
          "name": "Sen Deng",
          "affiliation": ""
        },
        {
          "name": "Yinshuai Li",
          "affiliation": ""
        },
        {
          "name": "Shuai Wang",
          "affiliation": ""
        },
        {
          "name": "Tianwei Zhang",
          "affiliation": ""
        },
        {
          "name": "Yinqian Zhang",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Recently, the new ciphertext side channels resulting from the deterministic memory encryption in Trusted Execution Environments (TEEs), enable ciphertexts to manifest identifiable patterns when being sequentially written to the same memory address. Attackers with read access to encrypted memory in TEEs can potentially deduce plaintexts by analyzing these changing ciphertext patterns. In this paper, we design CipherGuard, a compiler-based mitigation tool to counteract ciphertext side channels with high efficiency and security guarantees. CipherGuard is based on the LLVM ecosystem, and encompasses multiple defense strategies, including software-assisted probabilistic encryption, secret-aware register allocation, and diversion-based obfuscation. The design of CipherGuard demonstrates that compiler techniques are highly effective for fine-grained control over mitigation code generation and assisted component management. Through a comprehensive evaluation, it demonstrates that CipherGuard can strengthen the security of various cryptographic implementations more efficiently than existing state-of-the-art defense, i.e., CipherFix. In its most efficient strategy, CipherGuard incurs an average performance overhead of only 1.41X, with a maximum of 1.95X.",
      "paperUrl": "https://arxiv.org/pdf/2502.13401",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2502.13401",
      "tags": [
        "Performance",
        "Rust",
        "Security"
      ],
      "matchedAuthors": [
        "Yinqian Zhang"
      ]
    },
    {
      "id": "openalex-w4393213222",
      "source": "openalex-discovery",
      "title": "ChatDBG: Augmenting Debugging with Large Language Models",
      "authors": [
        {
          "name": "Kyla H. Levin",
          "affiliation": "University of Massachusetts Amherst"
        },
        {
          "name": "Nicolas van Kempen",
          "affiliation": "University of Massachusetts Amherst"
        },
        {
          "name": "Emery D. Berger",
          "affiliation": "Amazon (United States)"
        },
        {
          "name": "Stephen N. Freund",
          "affiliation": "Williams College"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on software engineering. | Vol. 2 (Issue FSE)",
      "type": "research-paper",
      "abstract": "Debugging is a critical but challenging task for programmers. This paper proposes ChatDBG, an AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like \"why is x null?\". To handle these queries, ChatDBG grants the LLM autonomy to \"take the wheel\": it can act as an independent agent capable of querying and controlling the debugger to navigate through stacks and inspect program state. It then reports its findings and yields back control to the programmer. By leveraging the real-world knowledge embedded in LLMs, ChatDBG can diagnose issues identifiable only through the use of domain-specific reasoning. Our ChatDBG prototype integrates with standard debuggers including LLDB and GDB for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code including standalone scripts and Jupyter notebooks, demonstrates that ChatDBG can successfully analyze root causes, explain bugs, and generate accurate fixes for a wide range of real-world errors. For the Python programs, a single query led to an actionable bug fix 67% of the time; one additional follow-up query increased the success rate to 85%. ChatDBG has seen rapid uptake; it has already been downloaded more than 75,000 times.",
      "paperUrl": "https://doi.org/10.1145/3729355",
      "sourceUrl": "",
      "tags": [
        "AI",
        "C++",
        "Embedded",
        "LLDB"
      ],
      "matchedAuthors": [
        "Emery D. Berger"
      ]
    },
    {
      "id": "openalex-w4414977271",
      "source": "openalex-discovery",
      "title": "Certified Decision Procedures for Width-Independent Bitvector Predicates",
      "authors": [
        {
          "name": "Siddharth Bhat",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Léo Stefanesco",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Chris Hughes",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 9 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "Bitvectors are foundational for automated reasoning. A few interactive theorem provers (ITP), such as Lean, have strong support for deciding fixed-width bitvector predicates by means of bitblasting. However, even these ITPs provide little automation for width-independent bitvector predicates. To fill this gap, we contribute novel, mechanized decision procedures for width-independent bitvector predicates in Lean. Classical algorithms to decide fragments of width-independent bitvector theory can be viewed from the lens of model checking, where the formula corresponds to an automaton and the correctness of the formula is a safety property. However, we cannot currently use this lens in mechanized proofs, as there are no executable, fast, and formally verified model checking algorithms that can be used interactively from within ITPs. To fill this gap, we mechanize key algorithms in the model checking literature: k -induction, automata reachability, automata emptiness checking, and automata minimization. Using these mechanized algorithms, we contribute scalable, mechanized, decision procedures for width-independent bitvector predicates. Furthermore, for controlled fragments of mixtures of arithmetic and bitwise operations which occur in the deobfuscation literature, we mechanize a recent fast algorithm (MBA-Blast), which outperforms the more general procedures on this fragment. Finally, we evaluate our decision procedures on benchmarks from classical compiler problems such as Hacker’s Delight and the LLVM peephole optimizer, as well as on equivalence checking problems for program obfuscation. Our tools solve 100% of Hacker’s Delight, two of our tools solve 100% of the deobfuscation dataset, and up to 27% of peephole rewrites extracted from LLVM’s peephole rewriting test suite. Our new decision procedures provide a push-button experience for width-independent bitvector reasoning in interactive theorem provers, and, more broadly, pave the way for foundational algorithms for fast, formally verified model checking.",
      "paperUrl": "https://doi.org/10.1145/3763148",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Siddharth Bhat",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4416740438",
      "source": "openalex-discovery",
      "title": "Can OpenMP Scale Beyond the Node? A Performance Evaluation of Remote Offloading via the MPI Proxy Plugin",
      "authors": [
        {
          "name": "Jhonatan Cléto",
          "affiliation": "Universidade Estadual de Campinas (UNICAMP)"
        },
        {
          "name": "Guilherme Valarini",
          "affiliation": "Universidade Estadual de Campinas (UNICAMP)"
        },
        {
          "name": "Hervé Yviquel",
          "affiliation": "Hospital de Clínicas da Unicamp"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We evaluate the MPI Proxy Plugin (MPP), an extension to LLVM/OpenMP that enables remote offloading of target regions via MPI, allowing distributed GPU execution without modifying application code. Using benchmarks on NVIDIA H100 and AMD MI300A nodes, we show that MPP delivers competitive performance compared to traditional MPI+OpenMP, particularly for coarse-grained, compute-intensive workloads. While MPP simplifies development and enables communication–computation overlap, it introduces higher runtime overheads and task granularity requirements. Our results position MPP as a promising step toward unifying shared and distributed heterogeneous programming under the OpenMP model.",
      "paperUrl": "https://sol.sbc.org.br/index.php/sscad/article/download/37905/37683",
      "sourceUrl": "https://doi.org/10.5753/sscad.2025.16707",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Guilherme Valarini"
      ]
    },
    {
      "id": "openalex-w4407863023",
      "source": "openalex-discovery",
      "title": "Cage: Hardware-Accelerated Safe WebAssembly",
      "authors": [
        {
          "name": "Martin Fink",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Dimitrios Stavrakakis",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Dennis Sprokholt",
          "affiliation": "Technical Solutions (United States)"
        },
        {
          "name": "Soham Chakraborty",
          "affiliation": "Delft University of Technology"
        },
        {
          "name": "Jan-Erik Ekberg",
          "affiliation": "Technical Solutions (United States)"
        },
        {
          "name": "Pramod Bhatotia",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "WebAssembly (WASM) is an immensely versatile and increasingly popular compilation target. It executes applications written in several languages (e.g., C/C++) with near-native performance in various domains (e.g., mobile, edge, cloud). Despite WASM's sandboxing feature, which isolates applications from other instances and the host platform, WASM does not inherently provide any memory safety guarantees for applications written in low-level, unsafe languages. To this end, we propose Cage, a hardware-accelerated toolchain for WASM that supports unmodified applications compiled to WASM and utilizes diverse Arm hardware features aiming to enrich the memory safety properties of WASM. Precisely, Cage leverages Arm's Memory Tagging Extension (MTE) to (i) provide spatial and temporal memory safety for heap and stack allocations and (ii) improve the performance of WASM's sandboxing mechanism. Cage further employs Arm's Pointer Authentication (PAC) to prevent leaked pointers from being reused by other WASM instances, thus enhancing WASM's security properties. We implement our system based on 64-bit WASM. We provide a WASM compiler and runtime with support for Arm's MTE and PAC. On top of that, Cage's LLVM-based compiler toolchain transforms unmodified applications to provide spatial and temporal memory safety for stack and heap allocations and prevent function pointer reuse. Our evaluation on real hardware shows that Cage incurs minimal runtime (<5.8%) and memory (<3.7%) overheads and can improve the performance of WASM's sandboxing mechanism, achieving a speedup of over 5.1%, while offering efficient memory safety guarantees.",
      "paperUrl": "https://doi.org/10.1145/3696443.3708920",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Soham Chakraborty"
      ]
    },
    {
      "id": "openalex-w4412373975",
      "source": "openalex-discovery",
      "title": "COMPASS: An Agent for MLIR Compilation Pass Pipeline Generation",
      "authors": [
        {
          "name": "Hongbin Zhang",
          "affiliation": "Institute of Software"
        },
        {
          "name": "Shihao Gao",
          "affiliation": "University of Chinese Academy of Sciences"
        },
        {
          "name": "Yang Liu",
          "affiliation": "Chinese Academy of Sciences"
        },
        {
          "name": "Mingjie Xing",
          "affiliation": "Institute of Software"
        },
        {
          "name": "Yanjun Wu",
          "affiliation": "Institute of Software"
        },
        {
          "name": "Chen Zhao",
          "affiliation": "Institute of Software"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-031-98208-8_13",
      "sourceUrl": "",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "Hongbin Zhang"
      ]
    },
    {
      "id": "openalex-w4416004523",
      "source": "openalex-discovery",
      "title": "CIRE: LLVM Analysis for Floating-Point Rounding Error Affected by Precision and Optimizations",
      "authors": [
        {
          "name": "Tanmay Tirpankar",
          "affiliation": "University of Utah"
        },
        {
          "name": "Cayden Lund",
          "affiliation": "American Fork Hospital"
        },
        {
          "name": "Ganesh Gopalakrishnan",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present CiRE, a tool that computes floating-point rounding error for basic blocks of LLVM codes via static analysis. Using CiRE, programmers can explore different mixed-precision settings and compiler optimizations while improving performance and guarding against excessive error. Our studies using CiRE have yielded the following insights: (1) often, performance as well as accuracy can be improved; (2) compilers for different languages produce code with widely varying error, even for the same expression; (3) the choice of subexpressions to target for low precision allocation has a huge impact on error and performance. CiRE can analyze expressions with 105 or more operators, thus making it capable of analyzing basic blocks generated by unrolling loops.",
      "paperUrl": "https://doi.org/10.1145/3731599.3767479",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Ganesh Gopalakrishnan"
      ]
    },
    {
      "id": "openalex-w4416955802",
      "source": "openalex-discovery",
      "title": "Bringing Automatic Differentiation to CUDA with Compiler-Based Source Transformations",
      "authors": [
        {
          "name": "Christina Koutsou",
          "affiliation": ""
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": ""
        },
        {
          "name": "David Lange",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "GPUs have become increasingly popular for their ability to perform parallel operations efficiently, driving interest in General-Purpose GPU Programming. Scientific computing, in particular, stands to benefit greatly from these capabilities. However, parallel programming systems such as CUDA introduce challenges for code transformation tools due to their reliance on low-level hardware management primitives. These challenges make implementing automatic differentiation (AD) for parallel systems particularly complex. CUDA is being widely adopted as an accelerator technology in many scientific algorithms from machine learning to physics simulations. Enabling AD for such codes builds a new valuable capability necessary for advancing scientific computing. Clad is an LLVM/Clang plugin for automatic differentiation that performs source-to-source transformation by traversing the compiler’s internal high-level data structures, and generates a function capable of computing derivatives of a given function at compile time. In this paper, we explore how we recently extended Clad to support GPU kernels and functions, as well as kernel launches and CUDA host functions. We will discuss the underlying techniques and real-world applications in scientific computing. Finally, we will examine current limitations and potential future directions for GPU-accelerated differentiation.",
      "paperUrl": "https://pos.sissa.it/491/024/pdf",
      "sourceUrl": "https://doi.org/10.22323/1.491.0024",
      "tags": [
        "Clang",
        "CUDA",
        "GPU"
      ],
      "matchedAuthors": [
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w4413779626",
      "source": "openalex-discovery",
      "title": "BePilot: An AI Programming Assistant for Compiler Backend Development",
      "authors": [
        {
          "name": "Ming Zhong",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Xin Sun",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Fang Lv",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Lulin Wang",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Hongna Geng",
          "affiliation": "SK Group (Japan)"
        },
        {
          "name": "Lin Qiu",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Huimin Cui",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Xiaobing Feng",
          "affiliation": "Institute of Computing Technology"
        }
      ],
      "year": "2025",
      "venue": "ACM Transactions on Software Engineering and Methodology | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compiler backends are tasked with generating executable machine code for various processors. As the diversity of processors continues to grow, it is imperative for programmers to tailor specific compiler backends to accommodate each one. However, compiler backend development remains a labor-intensive and time-consuming process, with limited automation tools available. Although large language models (LLMs) have demonstrated strong abilities in code completion and generation tasks, the lack of appropriate datasets for compiler backend development limits the application of LLMs in this field. this paper, we introduce ComBack++, a multilingual dataset covering C/C++, Machine Description, and TableGen, with 184 backends from GCC and LLVM, four backend-specific tasks. Based on ComBack++, we present BePilot, a compiler backend-specific LLM available in two sizes: BePilot-1.5B and BePilot-7B. We also introduce CB-Retriever, a retriever that constructs few-shot prompts via in-context learning to improve vanilla LLM performance in resource-constrained settings. Experimental results show that BePilot-1.5B and BePilot-7B achieve significantly higher accuracy across four tasks in ComBack++ compared to twelve baseline LLMs (125M – 34B parameters). In addition, CB-Retriever consistently boosts the accuracy of six mainstream LLMs. Both BePilot-1.5B and BePilot-7B, as well as vanilla LLMs augmented with CB-Retriever, outperform the traditional manual compiler backend development approach (Fork-Flow) in efficiency across all four tasks in ComBack++. Furthermore, human evaluation by four experienced compiler backend developers confirms that BePilot not only improves development efficiency over Fork-Flow, but also surpasses commercial AI programming assistants such as GPT-4o-mini and Gemini2-Flash in terms of code quality. These findings confirm that BePilot and CB-Retriever can substantially enhance compiler backend development efficiency.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3764585",
      "sourceUrl": "https://doi.org/10.1145/3764585",
      "tags": [
        "AI",
        "Backend",
        "C++",
        "Performance"
      ],
      "matchedAuthors": [
        "Huimin Cui",
        "Xiaobing Feng"
      ]
    },
    {
      "id": "openalex-w4409333498",
      "source": "openalex-discovery",
      "title": "An MLIR-Based Compilation Framework for CGRA Application Deployment",
      "authors": [
        {
          "name": "Yuxuan Wang",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Cristian Tirelli",
          "affiliation": "Università della Svizzera italiana"
        },
        {
          "name": "Lara Orlandic",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Juan Sapriza",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Rubén Rodríguez Rodríguez",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Giovanni Ansaloni",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Laura Pozzi",
          "affiliation": "Università della Svizzera italiana"
        },
        {
          "name": "David Atienza",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        }
      ],
      "year": "2025",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-031-87995-1_3",
      "sourceUrl": "",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "Giovanni Ansaloni",
        "Laura Pozzi"
      ]
    },
    {
      "id": "openalex-w4406785318",
      "source": "openalex-discovery",
      "title": "ASDF: A Compiler for Qwerty, a Basis-Oriented Quantum Programming Language",
      "authors": [
        {
          "name": "Austin J. Adams",
          "affiliation": ""
        },
        {
          "name": "Sharjeel Khan",
          "affiliation": ""
        },
        {
          "name": "Arjun S. Bhamra",
          "affiliation": ""
        },
        {
          "name": "Ryan R. Abusaada",
          "affiliation": ""
        },
        {
          "name": "Anthony M. Cabrera",
          "affiliation": ""
        },
        {
          "name": "Cameron C. Hoechst",
          "affiliation": ""
        },
        {
          "name": "Travis S. Humble",
          "affiliation": ""
        },
        {
          "name": "Jeffrey Young",
          "affiliation": ""
        },
        {
          "name": "Thomas M. Conte",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Qwerty is a high-level quantum programming language built on bases and functions rather than circuits. This new paradigm introduces new challenges in compilation, namely synthesizing circuits from basis translations and automatically specializing adjoint or predicated forms of functions. This paper presents ASDF, an open-source compiler for Qwerty that answers these challenges in compiling basis-oriented languages. Enabled with a novel high-level quantum IR implemented in the MLIR framework, our compiler produces OpenQASM 3 or QIR for either simulation or execution on hardware. Our compiler is evaluated by comparing the fault-tolerant resource requirements of generated circuits with other compilers, finding that ASDF produces circuits with comparable cost to prior circuit-oriented compilers.",
      "paperUrl": "https://arxiv.org/pdf/2501.13262",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2501.13262",
      "tags": [
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Sharjeel Khan",
        "Thomas M. Conte"
      ]
    },
    {
      "id": "openalex-w4407245202",
      "source": "openalex-discovery",
      "title": "A Multi-level Compiler Backend for Accelerated Micro-kernels Targeting RISC-V ISA Extensions",
      "authors": [
        {
          "name": "Alexandre Lopoukhine",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Federico Ficarelli",
          "affiliation": "Cineca"
        },
        {
          "name": "Christos Vasiladiotis",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Anton Lydike",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Josse Van Delm",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Alban Dutilleul",
          "affiliation": "École Normale Supérieure de Rennes"
        },
        {
          "name": "Luca Benini",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Marian Verhelst",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-performance micro-kernels must fully exploit today's diverse and specialized hardware to deliver peak performance to DNNs. While higher-level optimizations for DNNs are offered by numerous compilers (e.g., MLIR, TVM, OpenXLA), performance-critical micro-kernels are left to specialized code generators or handwritten assembly. Even though widely-adopted compilers (e.g., LLVM, GCC) offer tuned backends, their CPU-focused input abstraction, unstructured IR, and general-purpose best-effort design inhibit tailored code generation for innovative hardware. We think it is time to widen the classical hourglass backend and embrace progressive lowering across a diverse set of structured abstractions to bring domain-specific code generation to compiler backends. We demonstrate this concept by implementing a custom backend for a RISC-V-based accelerator with hardware loops and streaming registers, leveraging knowledge about the hardware at levels of abstraction that match its custom ISA. We use incremental register allocation over structured IRs, while dropping classical spilling heuristics, and show up to 90% FPU utilization across key DNN kernels. By breaking the backend hourglass model, we reopen the path from domain-specific abstractions to specialized hardware.",
      "paperUrl": "https://arxiv.org/pdf/2502.04063",
      "sourceUrl": "https://doi.org/10.1145/3696443.3708952",
      "tags": [
        "Backend",
        "IR",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Lydike",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4416618284",
      "source": "openalex-discovery",
      "title": "A Full Stack Framework for High Performance Quantum-Classical Computing",
      "authors": [
        {
          "name": "Xin Zhan",
          "affiliation": ""
        },
        {
          "name": "K. Grace Johnson",
          "affiliation": ""
        },
        {
          "name": "Aniello Esposito",
          "affiliation": ""
        },
        {
          "name": "Barbara Chapman",
          "affiliation": ""
        },
        {
          "name": "Marco Fiorentino",
          "affiliation": ""
        },
        {
          "name": "Kirk Bresniker",
          "affiliation": ""
        },
        {
          "name": "Raymond G. Beausoleil",
          "affiliation": ""
        },
        {
          "name": "Masoud Mohseni",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "To address the growing needs for scalable High Performance Computing (HPC) and Quantum Computing (QC) integration, we present our HPC-QC full stack framework and its hybrid workload development capability with modular hardware/device-agnostic software integration approach. The latest development in extensible interfaces for quantum programming, dispatching, and compilation within existing mature HPC programming environment are demonstrated. Our HPC-QC full stack enables high-level, portable invocation of quantum kernels from commercial quantum SDKs within HPC meta-program in compiled languages (C/C++ and Fortran) as well as Python through a quantum programming interface library extension. An adaptive circuit knitting hypervisor is being developed to partition large quantum circuits into sub-circuits that fit on smaller noisy quantum devices and classical simulators. At the lower-level, we leverage Cray LLVM-based compilation framework to transform and consume LLVM IR and Quantum IR (QIR) from commercial quantum software frontends in a retargetable fashion to different hardware architectures. Several hybrid HPC-QC multi-node multi-CPU and GPU workloads (including solving linear system of equations, quantum optimization, and simulating quantum phase transitions) have been demonstrated on HPE EX supercomputers to illustrate functionality and execution viability for all three components developed so far. This work provides the framework for a unified quantum-classical programming environment built upon classical HPC software stack (compilers, libraries, parallel runtime and process scheduling).",
      "paperUrl": "https://arxiv.org/pdf/2510.20128",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2510.20128",
      "tags": [
        "C++",
        "Frontend",
        "GPU",
        "IR",
        "Libraries",
        "Performance",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Barbara Chapman"
      ]
    },
    {
      "id": "openalex-w4413918453",
      "source": "openalex-discovery",
      "title": "A Comparative Study of Static Program Slicing Tools for C",
      "authors": [
        {
          "name": "Gábor Spaits",
          "affiliation": ""
        },
        {
          "name": "Kristóf Umann",
          "affiliation": ""
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        }
      ],
      "year": "2025",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Program slicing techniques have been a hot topic in the field of program analysis since the early 1980s. These techniques are most often used to find all program statements that might effect the state of a so-called slicing criterion. A slicing criterion is usually defined as a variable statement pair. For instance, if a division by zero error occurs program slicing may help reduce the program to the set of those statements that contributed to the bug. In this case, the slicing criterion would be the denominator at the point of the division. There are many approaches to program slicing, most notably dynamic and static. There have been numerous studies on the theoretical capabilities of many dynamic and static slicers and many practical results were drawn on certain benchmarks. With that said no study takes a look at currently available slicing tools on real software. In our paper, we intend to fill this gap by measuring the performance of tools like LLVM-Slicer, Frama-C and Unravel. We also show a methodology for selecting slicing criterion against which slices can be measured, which has historically been a pain point of this field of study.",
      "paperUrl": "https://doi.org/10.1109/mipro65660.2025.11131933",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Kristóóf Umann",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4401019067",
      "source": "openalex-discovery",
      "title": "eCC++ : A Compiler Construction Framework for Embedded Domain-Specific Languages",
      "authors": [
        {
          "name": "Marc González",
          "affiliation": "Universitat Politècnica de Catalunya"
        },
        {
          "name": "Joel Denny",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Seyong Lee",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Keita Teranishi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "eCC++ is a new compiler construction framework for embedding domain-specific programming languages within C++. That is, the host language is C++, the guest language is the DSL to be embedded in C++, and eCC++ is the tool that enables the embedding. The eCC++ framework is composed of three main components: a front-end, an API for program verification based on a functional and declarative intermediate representation (IR), and a Multi-Level Intermediate Representation (MLIR) code generator. The eCC++ front-end consists of a library of C++ classes and operators that can be used to define the guest language. Guest sources are compiled with any standard C++ compiler, and when run, the resulting executable generates an eCC++ IR representation of the program, which can be verified within the eCC++ framework. Finally, eCC++ allows for high-level and domain-specific optimizations before generating MLIR. In summary, eCC++ aims to act as a generic front-end that enables embedding guest languages into C++, and provides necessary compiler technology for program verification, targeting the existing capabilities in the MLIR infrastructure. The paper evaluates the eCC++ expressiveness and usability describing the process of embedding GraphIt, a high-performance graph language in C++.",
      "paperUrl": "https://www.osti.gov/servlets/purl/2438824",
      "sourceUrl": "https://doi.org/10.1109/ipdpsw63119.2024.00129",
      "tags": [
        "C++",
        "Embedded",
        "Infrastructure",
        "IR",
        "MLIR",
        "Optimizations",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4404057240",
      "source": "openalex-discovery",
      "title": "Work-in-Progress:ACPO: An AI-Enabled Compiler Framework",
      "authors": [
        {
          "name": "Amir H. Ashouri",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Muhammad Asif Manzoor",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Minh Hai Vu",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Raymond Zhang",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Ziwen Wang",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Angel Zhang",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Bryan Chan",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Tomasz Czajkowski",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Yaoqing Gao",
          "affiliation": "Huawei Technologies (Canada)"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper presents ACPO: An AI-Enabled Compiler Framework; a novel framework that provides LLVM with simple and comprehensive tools to enable employing ML models for different optimization passes. We showcase a couple of use cases of ACPO by ML-enabling the Loop Unroll (LU) and Function Inlining (FI) passes and experimental results reveal that by including both models, ACPO can provide a combined speedup of 2.4% on Cbench when compared with LLVM’s O3.",
      "paperUrl": "http://dx.doi.org/10.1109/cases60062.2024.00011",
      "sourceUrl": "https://doi.org/10.1109/cases60062.2024.00011",
      "tags": [
        "AI",
        "ML"
      ],
      "matchedAuthors": [
        "Bryan Chan"
      ]
    },
    {
      "id": "openalex-w4400433830",
      "source": "openalex-discovery",
      "title": "Verifying Peephole Rewriting In SSA Compiler IRs",
      "authors": [
        {
          "name": "S. Bhat",
          "affiliation": ""
        },
        {
          "name": "Alex C. Keizer",
          "affiliation": ""
        },
        {
          "name": "C.E. Hughes",
          "affiliation": ""
        },
        {
          "name": "Andrés Goens",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "There is an increasing need for domain-specific reasoning in modern compilers. This has fueled the use of tailored intermediate representations (IRs) based on static single assignment (SSA), like in the MLIR compiler framework. Interactive theorem provers (ITPs) provide strong guarantees for the end-to-end verification of compilers (e.g., CompCert). However, modern compilers and their IRs evolve at a rate that makes proof engineering alongside them prohibitively expensive. Nevertheless, well-scoped push-button automated verification tools such as the Alive peephole verifier for LLVM-IR gained recognition in domains where SMT solvers offer efficient (semi) decision procedures. In this paper, we aim to combine the convenience of automation with the versatility of ITPs for verifying peephole rewrites across domain-specific IRs. We formalize a core calculus for SSA-based IRs that is generic over the IR and covers so-called regions (nested scoping used by many domain-specific IRs in the MLIR ecosystem). Our mechanization in the Lean proof assistant provides a user-friendly frontend for translating MLIR syntax into our calculus. We provide scaffolding for defining and verifying peephole rewrites, offering tactics to eliminate the abstraction overhead of our SSA calculus. We prove correctness theorems about peephole rewriting, as well as two classical program transformations. To evaluate our framework, we consider three use cases from the MLIR ecosystem that cover different levels of abstractions: (1) bitvector rewrites from LLVM, (2) structured control flow, and (3) fully homomorphic encryption. We envision that our mechanization provides a foundation for formally verified rewrites on new domain-specific IRs.",
      "paperUrl": "https://arxiv.org/pdf/2407.03685",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2407.03685",
      "tags": [
        "Frontend",
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4392736841",
      "source": "openalex-discovery",
      "title": "UniSparse: An Intermediate Language for General Sparse Format Customization",
      "authors": [
        {
          "name": "Jie Liu",
          "affiliation": "Cornell University"
        },
        {
          "name": "Zhongyuan Zhao",
          "affiliation": "Cornell University"
        },
        {
          "name": "Zijian Ding",
          "affiliation": "University of California, Los Angeles"
        },
        {
          "name": "Benjamin Brock",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Hongbo Rong",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Zhiru Zhang",
          "affiliation": "Cornell University"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue OOPSLA1)",
      "type": "research-paper",
      "abstract": "The ongoing trend of hardware specialization has led to a growing use of custom data formats when processing sparse workloads, which are typically memory-bound. These formats facilitate optimized software/hardware implementations by utilizing sparsity pattern- or target-aware data structures and layouts to enhance memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Additionally, because these frameworks represent formats using a limited set of per-dimension attributes, they lack the flexibility to accommodate numerous new variations of custom sparse data structures and layouts. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. Unlike the existing attribute-based frameworks, UniSparse decouples the logical representation of the sparse tensor (i.e., the data structure) from its low-level memory layout, enabling the customization of both. As a result, a rich set of format customizations can be succinctly expressed in a small set of well-defined query, mutation, and layout primitives. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats, and automatic code generation of format conversion and compute operations for heterogeneous architectures. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with specialized formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, an AMD Xilinx FPGA, and a simulated processing-in-memory (PIM) device.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3649816",
      "sourceUrl": "https://doi.org/10.1145/3649816",
      "tags": [
        "GPU",
        "Infrastructure",
        "MLIR"
      ],
      "matchedAuthors": [
        "Zhiru Zhang"
      ]
    },
    {
      "id": "openalex-w4392461068",
      "source": "openalex-discovery",
      "title": "Uncovering Hidden Dependencies: Constructing Intelligible Path Witnesses using Dataflow Analyses",
      "authors": [
        {
          "name": "Kristóf Umann",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Gábor Horváth",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2024",
      "venue": "Acta Cybernetica | Vol. 26 (Issue 3)",
      "type": "research-paper",
      "abstract": "The lack of sound, concise and comprehensive error reports emitted by a static analysis tool can cause increased fixing cost, bottleneck at the availability of experts and even may undermine the trust in static analysis as a method. This paper presents novel techniques to improve the quality of bug reports for static analysis tools that employ symbolic execution. With the combination of data and control dependency analysis, we can identify the relevance of particular code snippets that were previously missing from the report. We demonstrated the benefits of our approach by implementing an improved bug report generator algorithm for the Clang Static Analyzer. After being tested by the open source community our solution became enabled by default in the tool.",
      "paperUrl": "https://cyber.bibl.u-szeged.hu/index.php/actcybern/article/download/4345/4081",
      "sourceUrl": "https://doi.org/10.14232/actacyb.299805",
      "tags": [
        "Clang",
        "Rust",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Kristóóf Umann",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4401990465",
      "source": "openalex-discovery",
      "title": "Tutorial: LLTFI and the Art of Fault Injection",
      "authors": [
        {
          "name": "Karthik Pattabiraman",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Abraham Chan",
          "affiliation": "University of British Columbia"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Fault injection has been a well-researched area in the dependable and reliable systems community. Nevertheless, a simple framework for fault injection that combines both software and hardware errors in an adaptable way, has not previously existed before the debut of LLFI, an LLVM-based fault injector tool. We present a tutorial for LLTFI, an fault injector tool extending LLFI, which is capable of injecting faults into both machine learning and native code applications. Assuming some prior knowledge of fault injection, this tutorial covers the fundamentals, design philosophy, and application of LLTFI.",
      "paperUrl": "http://dx.doi.org/10.1109/dsn-s60304.2024.00030",
      "sourceUrl": "https://doi.org/10.1109/dsn-s60304.2024.00030",
      "tags": [],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w4404418000",
      "source": "openalex-discovery",
      "title": "TreeHouse: An MLIR-based Compilation Flow for Real-Time Tree-based Inference",
      "authors": [
        {
          "name": "Chiahui Su",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chien-Chun Ku",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Kuan-Hsun Chen",
          "affiliation": "University of Twente"
        }
      ],
      "year": "2024",
      "venue": "ACM Transactions on Embedded Computing Systems | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Tree-based ensembles stand as the prominent resource-efficient approaches for real-time inference. To optimize their performance, researchers have developed several solutions to accommodate their unique program structure, i.e., consecutive branches and eliminate the floating-point arithmetic operations, which are usually costly at the embedded computing systems. Most of them are realized at the level of source code and a standard compilation is applied subsequently. Therefore, an end-to-end compilation flow may consolidate these methods and provide a holistic optimization. In this work, we introduce TreeHouse, a compilation flow based on MLIR designed for real-time inference of tree ensembles. First, we optimize the layout of basic blocks to reduce the expected branches during inference. Moreover, we provide a solution to facilitate LLVM register allocation for further efficiency. In addressing the suboptimal performance of floating-point operations in edge systems, we incorporate methods to convert data into integer format. We implemented and evaluated TreeHouse using two tree-based ensembles: boosting trees and random forests. Overall, boosting trees exhibited performance gains ranging from 1.38 × to 8.24 × on x86, 1.70 × to 6.61 × on ARMv8, and 1.75 × to 4.52 × on RISC-V compared to state-of-the-art decision tree research. Random forests achieved performance gains of up to 1.6 × on x86, 2.03 × on ARMv8, and 2.9 × on RISC-V.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3704727",
      "sourceUrl": "https://doi.org/10.1145/3704727",
      "tags": [
        "Embedded",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4394745579",
      "source": "openalex-discovery",
      "title": "Translation Validation for JIT Compiler in the V8 JavaScript Engine",
      "authors": [
        {
          "name": "Seungwan Kwon",
          "affiliation": ""
        },
        {
          "name": "Jaeseong Kwon",
          "affiliation": "Korea Advanced Institute of Science and Technology"
        },
        {
          "name": "Wooseok Kang",
          "affiliation": "Korea Advanced Institute of Science and Technology"
        },
        {
          "name": "Juneyoung Lee",
          "affiliation": "Amazon (United States)"
        },
        {
          "name": "Kihong Heo",
          "affiliation": "Korea Advanced Institute of Science and Technology"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present TurboTV, a translation validator for the JavaScript (JS) just-in-time (JIT) compiler of V8. While JS engines have become a crucial part of various software systems, their emerging adaption of JIT compilation makes it increasingly challenging to ensure their correctness. We tackle this problem with an SMT-based translation validation (TV) that checks whether a specific compilation is semantically correct. We formally define the semantics of IR of TurboFan (JIT compiler of V8) as SMT encoding. For efficient validation, we design a staged strategy for JS JIT compilers. This allows us to decompose the whole correctness checking into simpler ones. Furthermore, we utilize fuzzing to achieve practical TV. We generate a large number of JS functions using a fuzzer to trigger various optimization passes of TurboFan and validate their compilation using TurboTV. Lastly, we demonstrate that TurboTV can also be used for cross-language TV. We show that TurboTV can validate the translation chain from LLVM IR to TurboFan IR, collaborating with an off-the-shelf TV tool for LLVM. We evaluated TurboTV on various sets of JS and LLVM programs. TurboTV effectively validated a large number of compilations of TurboFan with a low false positive rate and discovered a new miscompilation in LLVM.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3597503.3639189",
      "sourceUrl": "https://doi.org/10.1145/3597503.3639189",
      "tags": [
        "IR",
        "JIT"
      ],
      "matchedAuthors": [
        "Juneyoung Lee"
      ]
    },
    {
      "id": "openalex-w4395474475",
      "source": "openalex-discovery",
      "title": "Towards a high-performance AI compiler with upstream MLIR",
      "authors": [
        {
          "name": "Renato Golin",
          "affiliation": ""
        },
        {
          "name": "Lorenzo Chelini",
          "affiliation": ""
        },
        {
          "name": "Adam Siemieniuk",
          "affiliation": ""
        },
        {
          "name": "Kavitha T. Madhu",
          "affiliation": ""
        },
        {
          "name": "Niranjan Hasabnis",
          "affiliation": ""
        },
        {
          "name": "Hans Pabst",
          "affiliation": ""
        },
        {
          "name": "Evangelos Georganas",
          "affiliation": ""
        },
        {
          "name": "Alexander Heinecke",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This work proposes a compilation flow using open-source compiler passes to build a framework to achieve ninja performance from a generic linear algebra high-level abstraction. We demonstrate this flow with a proof-of-concept MLIR project that uses input IR in Linalg-on-Tensor from TensorFlow and PyTorch, performs cache-level optimizations and lowering to micro-kernels for efficient vectorization, achieving over 90% of the performance of ninja-written equivalent programs. The contributions of this work include: (1) Packing primitives on the tensor dialect and passes for cache-aware distribution of tensors (single and multi-core) and type-aware instructions (VNNI, BFDOT, BFMMLA), including propagation of shapes across the entire function; (2) A linear algebra pipeline, including tile, fuse and bufferization strategies to get model-level IR into hardware friendly tile calls; (3) A mechanism for micro-kernel lowering to an open source library that supports various CPUs.",
      "paperUrl": "https://arxiv.org/pdf/2404.15204",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2404.15204",
      "tags": [
        "AI",
        "IR",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini",
        "Renato Golin"
      ]
    },
    {
      "id": "openalex-w4403160472",
      "source": "openalex-discovery",
      "title": "Towards a Scalable and Efficient PGAS-based Distributed OpenMP",
      "authors": [
        {
          "name": "Baodi Shan",
          "affiliation": ""
        },
        {
          "name": "Mauricio Araya–Polo",
          "affiliation": ""
        },
        {
          "name": "Barbara Chapman",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "MPI+X has been the de facto standard for distributed memory parallel programming. It is widely used primarily as an explicit two-sided communication model, which often leads to complex and error-prone code. Alternatively, PGAS model utilizes efficient one-sided communication and more intuitive communication primitives. In this paper, we present a novel approach that integrates PGAS concepts into the OpenMP programming model, leveraging the LLVM compiler infrastructure and the GASNet-EX communication library. Our model addresses the complexity associated with traditional MPI+OpenMP programming models while ensuring excellent performance and scalability. We evaluate our approach using a set of micro-benchmarks and application kernels on two distinct platforms: Ookami from Stony Brook University and NERSC Perlmutter. The results demonstrate that DiOMP achieves superior bandwidth and lower latency compared to MPI+OpenMP, up to 25% higher bandwidth and down to 45% on latency. DiOMP offers a promising alternative to the traditional MPI+OpenMP hybrid programming model, towards providing a more productive and efficient way to develop high-performance parallel applications for distributed memory systems.",
      "paperUrl": "https://arxiv.org/pdf/2409.02830",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2409.02830",
      "tags": [
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Baodi Shan",
        "Barbara Chapman"
      ]
    },
    {
      "id": "openalex-w4401454438",
      "source": "openalex-discovery",
      "title": "The Rewriting of DataRaceBench Benchmark for OpenCL Program Validations",
      "authors": [
        {
          "name": "Chia-Chen Hsu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chun-Lin Huang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chao-Lin Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Pei‐Hung Lin",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Effective detection of data races in parallel computing environments is essential for ensuring the correctness and performance of multi-threaded applications. This paper addresses the issue with OpenCL data racing analysis. Currently, for the data racing research, there is a well-established DataRaceBench benchmark, designed for OpenMP. In our research, we rewrite the OpenMP DataRaceBench benchmark for the OpenCL benchmark to provide a specialized benchmark suite for evaluating data race detection tools in OpenCL environments. In our analysis, we introduce a novel approach to detect data races in OpenCL programs by leveraging an LLVM-based analysis pass. To facilitate detailed analysis, annotations are inserted into the OpenCL kernels, which fetch data from the kernel side and return data to the host side. Then the detector on the host side utilized those events to analyze and track the interactions between different threads. Through vector clocks, we can partially order them. This methodology helps identify potential data races by analyzing patterns within these annotated sections. Experimental results demonstrate the efficacy of our approach, which can accurately detect data races across multiple threads in OpenCL by obtaining data via LLVM passes and analyzing them on the host. This work not only enhances the toolset for developers working with OpenCL but also contributes significantly to the field of parallel computing by providing a rigorous benchmarking tool for data race detection.",
      "paperUrl": "https://doi.org/10.1145/3677333.3678148",
      "sourceUrl": "",
      "tags": [
        "OpenCL",
        "Performance"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4403585711",
      "source": "openalex-discovery",
      "title": "The MLIR Transform Dialect. Your compiler is more powerful than you think",
      "authors": [
        {
          "name": "Martin Paul Lücke",
          "affiliation": ""
        },
        {
          "name": "William S. Moses",
          "affiliation": ""
        },
        {
          "name": "Michel Steuwer",
          "affiliation": ""
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "To take full advantage of a specific hardware target, performance engineers need to gain control on compilers in order to leverage their domain knowledge about the program and hardware. Yet, modern compilers are poorly controlled, usually by configuring a sequence of coarse-grained monolithic black-box passes, or by means of predefined compiler annotations/pragmas. These can be effective, but often do not let users precisely optimize their varying compute loads. As a consequence, performance engineers have to resort to implementing custom passes for a specific optimization heuristic, requiring compiler engineering expert knowledge. In this paper, we present a technique that provides fine-grained control of general-purpose compilers by introducing the Transform dialect, a controllable IR-based transformation system implemented in MLIR. The Transform dialect empowers performance engineers to optimize their various compute loads by composing and reusing existing - but currently hidden - compiler features without the need to implement new passes or even rebuilding the compiler. We demonstrate in five case studies that the Transform dialect enables precise, safe composition of compiler transformations and allows for straightforward integration with state-of-the-art search methods.",
      "paperUrl": "https://arxiv.org/pdf/2409.03864",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2409.03864",
      "tags": [
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Michel Steuwer",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4403564380",
      "source": "openalex-discovery",
      "title": "Testing the Unknown: A Framework for OpenMP Testing via Random Program Generation",
      "authors": [
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Patrick L. Chapman",
          "affiliation": "University of California, Davis"
        },
        {
          "name": "Konstantinos Parasyris",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Cindy Rubio-González",
          "affiliation": "University of California, Davis"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present a randomized differential testing approach to test OpenMP implementations. In contrast to previous work that manually creates dozens of verification and validation tests, our approach is able to randomly generate thousands of tests, exposing OpenMP implementations to a wide range of program behaviors. We represent the space of possible random OpenMP tests using a grammar and implement our method as an extension of the Varity program generator. By generating 1,800 OpenMP tests, we find various performance anomalies and correctness issues when we apply it to three OpenMP implementations: GCC, Clang, and Intel. We also present several case studies that analyze the anomalies and give more details about the classes of tests that our approach creates.",
      "paperUrl": "https://arxiv.org/pdf/2410.09191",
      "sourceUrl": "https://doi.org/10.1109/scw63240.2024.00080",
      "tags": [
        "Clang",
        "Performance",
        "Testing"
      ],
      "matchedAuthors": [
        "Giorgis Georgakoudis",
        "Ignacio Laguna",
        "Konstantinos Parasyris"
      ]
    },
    {
      "id": "openalex-w4400582589",
      "source": "openalex-discovery",
      "title": "TIPS: Tracking Integer-Pointer Value Flows for C++ Member Function Pointers",
      "authors": [
        {
          "name": "Changwei Zou",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Dongjie He",
          "affiliation": "Chongqing University"
        },
        {
          "name": "Yulei Sui",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on software engineering. | Vol. 1 (Issue FSE)",
      "type": "research-paper",
      "abstract": "C++ is crucial in software development, providing low-level memory control for performance and supporting object-oriented programming to construct modular, reusable code structures. Consequently, tackling pointer analysis for C++ becomes challenging, given the need to address these two fundamental features. A relatively unexplored research area involves the handling of C++ member function pointers. Previous efforts have tended to either disregard this feature or adopt a conservative approach, resulting in unsound or imprecise results. C++ member function pointers, handling both virtual (via virtual table indexes) and non-virtual functions (through addresses), pose a significant challenge for pointer analysis due to the mix of integers and pointers, often resulting in unsound or imprecise analysis. We introduce T ips , the first pointer analysis that effectively manages both pointers and integers, offering support for C++ member function pointers by tracking their value flows. Our evaluation on T ips demonstrates its accuracy in identifying C++ member function call targets, a task where other tools falter, across fourteen large C++ programs from SPEC CPU, Qt, LLVM, Ninja, and GoogleTest, while maintaining low analysis overhead. In addition, our micro-benchmark suite, complete with ground truth data, allows for precise evaluation of points-to information for C++ member function pointers across various inheritance scenarios, highlighting T ips ’s precision enhancements.",
      "paperUrl": "https://doi.org/10.1145/3660779",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Performance"
      ],
      "matchedAuthors": [
        "Jingling Xue",
        "Yulei Sui"
      ]
    },
    {
      "id": "openalex-w2287302287",
      "source": "openalex-discovery",
      "title": "Studying Verification Conditions for Imperative Programs",
      "authors": [
        {
          "name": "Cláudio Belo Lourenço",
          "affiliation": "INESC TEC"
        },
        {
          "name": "Si-Mohamed Lamraoui",
          "affiliation": "National Institute of Informatics"
        },
        {
          "name": "Shin Nakajima",
          "affiliation": "National Institute of Informatics"
        },
        {
          "name": "Jorge Sousa Pinto",
          "affiliation": "INESC TEC"
        }
      ],
      "year": "2024",
      "venue": "Vol. 72 (Issue None)",
      "type": "research-paper",
      "abstract": "Program verification tools use verification condition generators to produce logical formulas whose validity implies that the program is correct with respect to its specification. Different tools produce different conditions, and the underlying algorithms have not been properly exposed or explored so far. In this paper we consider a simple imperative programming language, extended with assume and assert statements, to present different ways of generating verification conditions. We study the approaches with experimental results originated by verification conditions generated from the intermediate representation of LLVM.",
      "paperUrl": "https://hdl.handle.net/1822/50793",
      "sourceUrl": "https://doi.org/10.14279/tuj.eceasst.72.1011",
      "tags": [],
      "matchedAuthors": [
        "Shin Nakajima",
        "Si-Mohamed Lamraoui"
      ]
    },
    {
      "id": "openalex-w4398191800",
      "source": "openalex-discovery",
      "title": "Strided Difference Bound Matrices",
      "authors": [
        {
          "name": "Arjun Pitchanathan",
          "affiliation": ""
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "A wide range of symbolic analysis and optimization problems can be formalized using polyhedra. Sub-classes of polyhedra, also known as sub-polyhedral domains, are sought for their lower space and time complexity. We introduce the Strided Difference Bound Matrix (SDBM) domain, which represents a sweet spot in the context of optimizing compilers. Its expressiveness and efficient algorithms are particularly well suited to the construction of machine learning compilers. We present decision algorithms, abstract domain operators and computational complexity proofs for SDBM. We also conduct an empirical study with the MLIR compiler framework to validate the domain's practical applicability. We characterize a sub-class of SDBMs that frequently occurs in practice, and demonstrate even faster algorithms on this sub-class.",
      "paperUrl": "https://arxiv.org/pdf/2405.11244",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2405.11244",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Arjun Pitchanathan",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4403368714",
      "source": "openalex-discovery",
      "title": "Scaling Symbolic Execution to Large Software Systems",
      "authors": [
        {
          "name": "Gábor Horváth",
          "affiliation": ""
        },
        {
          "name": "Réka Kovács",
          "affiliation": ""
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Static analysis is the analysis of a program without executing it, usually carried out by an automated tool. Symbolic execution is a popular static analysis technique used both in program verification and in bug detection software. It works by interpreting the code, introducing a symbol for each value unknown at compile time (e.g. user-given inputs), and carrying out calculations symbolically. The analysis engine strives to explore multiple execution paths simultaneously, although checking all paths is an intractable problem, due to the vast number of possibilities. We focus on an error finding framework called the Clang Static Analyzer, and the infrastructure built around it named CodeChecker. The emphasis is on achieving end-to-end scalability. This includes the run time and memory consumption of the analysis, bug presentation to the users, automatic false positive suppression, incremental analysis, pattern discovery in the results, and usage in continuous integration loops. We also outline future directions and open problems concerning these tools. While a rich literature exists on program verification software, error finding tools normally need to settle for survey papers on individual techniques. In this paper, we not only discuss individual methods, but also how these decisions interact and reinforce each other, creating a system that is greater than the sum of its parts. Although the Clang Static Analyzer can only handle C-family languages, the techniques introduced in this paper are mostly language-independent and applicable to other similar static analysis tools.",
      "paperUrl": "https://arxiv.org/pdf/2408.01909",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2408.01909",
      "tags": [
        "Clang",
        "Infrastructure",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Réka Kovács",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4394998527",
      "source": "openalex-discovery",
      "title": "SEER: Super-Optimization Explorer for High-Level Synthesis using E-graph Rewriting",
      "authors": [
        {
          "name": "Jianyi Cheng",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Samuel Coward",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Lorenzo Chelini",
          "affiliation": ""
        },
        {
          "name": "Rafael Barbalho",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Theo Drane",
          "affiliation": "Intel (United States)"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-level synthesis (HLS) is a process that automatically translates a software program in a high-level language into a low-level hardware description. However, the hardware designs produced by HLS tools still suffer from a significant performance gap compared to manual implementations. This is because the input HLS programs must still be written using hardware design principles.Existing techniques either leave the program source unchanged or perform a fixed sequence of source transformation passes, potentially missing opportunities to find the optimal design. We propose a super-optimization approach for HLS that automatically rewrites an arbitrary software program into efficient HLS code that can be used to generate an optimized hardware design. We developed a toolflow named SEER, based on the e-graph data structure, to efficiently explore equivalent implementations of a program at scale. SEER provides an extensible framework, orchestrating existing software compiler passes and hardware optimizers.Our work is the first attempt to exploit e-graph rewriting for large software compiler frameworks, such as MLIR. Across a set of open-source benchmarks, we show that SEER achieves up to 38× the performance within 1.4× the area of the original program. Via an Intel-provided case study, SEER demonstrates the potential to outperform manually optimized designs produced by hardware experts.",
      "paperUrl": "https://doi.org/10.1145/3620665.3640392",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini"
      ]
    },
    {
      "id": "openalex-w4392265924",
      "source": "openalex-discovery",
      "title": "Revealing Compiler Heuristics Through Automated Discovery and Optimization",
      "authors": [
        {
          "name": "Volker Seeker",
          "affiliation": ""
        },
        {
          "name": "Chris Cummins",
          "affiliation": ""
        },
        {
          "name": "Murray Cole",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Björn Franke",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Kim Hazelwood",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "<br/>Tuning compiler heuristics and parameters is well known to improve optimization outcomes dramatically. Prior works have tuned command line flags and a few expert identified heuristics. However, there are an unknown number of heuristics buried, unmarked and unexposed inside the compiler as a consequence of decades of development without auto-tuning being foremost in the minds of developers. Many may not even have been considered heuristics by the developers who wrote them. The result is that auto-tuning search and machine learning can optimize only a tiny fraction of what could be possible if all heuristics were available to tune. Manually discovering all of these heuristics hidden among millions of lines of code and exposing them to auto-tuning tools is a Herculean task that is simply not practical. What is needed is a method of automatically finding these heuristics to extract every last drop of potential optimization. In this work, we propose Heureka, a framework that automat ically identifies potential heuristics in the compiler that are highly profitable optimization targets and then automatically finds available tuning parameters for those heuristics with minimal human involvement. Our work is based on the following key insight: When modifying the output of a heuristic within an acceptable value range, the calling code using that output will still function correctly and produce semantically correct results. Building on that, we automatically manipulate the output of potential heuristic code in the compiler and decide using a Differential Testing approach if we found a heuristic or not. During output manipulation, we also explore acceptable value ranges of the targeted code. Heuristics identified in this way can then be tuned to optimize an objective function. We used Heureka to search for heuristics among eight thou sand functions from the LLVM optimization passes, which is about 2% of all available functions. We then use identified heuristics to tune the compilation of 38 applications from the NAS and Polybench benchmark suites. Compared to an -Oz baseline we reduce binary sizes by up to 11.6% considering single heuristics only and up to 19.5% when stacking the effects of multiple identified tuning targets and applying a random search with minimal search effort. Generalizing from existing analysis results, Heureka needs, on average, a little under an hour on a single machine to identify relevant heuristic targets for a previously unseen application.<br/>",
      "paperUrl": "https://www.research.ed.ac.uk/en/publications/47054d5b-b36b-4832-b4bb-073e44501b69",
      "sourceUrl": "https://doi.org/10.1109/cgo57630.2024.10444847",
      "tags": [
        "Testing"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather",
        "Kim Hazelwood"
      ]
    },
    {
      "id": "openalex-w4392265915",
      "source": "openalex-discovery",
      "title": "Retargeting and Respecializing GPU Workloads for Performance Portability",
      "authors": [
        {
          "name": "Ivan R. Ivanov",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Jens Domke",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Toshio Endo",
          "affiliation": "Tokyo Institute of Technology"
        },
        {
          "name": "William S. Moses",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In order to come close to peak performance, accelerators like GPUs require significant architecture-specific tuning that understand the availability of shared memory, parallelism, tensor cores, etc. Unfortunately, the pursuit of higher performance and lower costs have led to a significant diversification of architecture designs, even from the same vendor. This creates the need for performance portability across different GPUs, especially important for programs in a particular programming model with a certain architecture in mind. Even when the program can be seamlessly executed on a different architecture, it may suffer a performance penalty due to it not being sized appropriately to the available hardware resources such as fast memory and registers, let alone not using newer advanced features of the architecture. We propose a new approach to improving performance of (legacy) CUDA programs for modern machines by automatically adjusting the amount of work each parallel thread does, and the amount of memory and register resources it requires. By operating within the MLIR compiler infrastructure, we are able to also target AMD GPUs by performing automatic translation from CUDA and simultaneously adjust the program granularity to fit the size of target GPUs. Combined with autotuning assisted by the platform-specific compiler, our approach demonstrates 27% geomean speedup on the Rodinia benchmark suite over baseline CUDA implementation as well as performance parity between similar NVIDIA and AMD GPUs executing the same CUDA program.",
      "paperUrl": "https://doi.org/10.1109/cgo57630.2024.10444828",
      "sourceUrl": "",
      "tags": [
        "CUDA",
        "GPU",
        "Infrastructure",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Ivan R. Ivanov",
        "Jens Domke",
        "Toshio Endo",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4394975822",
      "source": "openalex-discovery",
      "title": "Reordering Functions in Mobiles Apps for Reduced Size and Faster Start-Up",
      "authors": [
        {
          "name": "Ellis Hoag",
          "affiliation": "Menlo School"
        },
        {
          "name": "Kyungwoo Lee",
          "affiliation": "Menlo School"
        },
        {
          "name": "Julián Mestre",
          "affiliation": "University of Sydney"
        },
        {
          "name": "Sergey Pupyrev",
          "affiliation": "Menlo School"
        },
        {
          "name": "Yongkang Zhu",
          "affiliation": "Menlo School"
        }
      ],
      "year": "2024",
      "venue": "ACM Transactions on Embedded Computing Systems | Vol. 23 (Issue 4)",
      "type": "research-paper",
      "abstract": "Function layout, also known as function reordering or function placement, is one of the most effective profile-guided compiler optimizations. By reordering functions in a binary, compilers can improve the performance of large-scale applications or reduce the compressed size of mobile applications. Although the technique has been extensively studied in the context of large-scale binaries, no study has thoroughly investigated function layout algorithms on mobile applications. In this article, we develop the first principled solution for optimizing function layouts in the mobile space. To this end, we identify two key optimization goals: reducing the compressed code size and improving the cold start-up time of a mobile application. Then, we propose a formal model for the layout problem, whose objective closely matches our goals, and a novel algorithm for optimizing the layout. The method is inspired by the classic balanced graph partitioning problem. We have carefully engineered and implemented the algorithm in an open-source compiler, Low-level Virtual Machine (LLVM). An extensive evaluation of the new method on large commercial mobile applications demonstrates improvements in start-up time and compressed size compared to the state-of-the-art approach. 1",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3660635",
      "sourceUrl": "https://doi.org/10.1145/3660635",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Ellis Hoag",
        "Kyungwoo Lee"
      ]
    },
    {
      "id": "openalex-w4399851369",
      "source": "openalex-discovery",
      "title": "Refined Input, Degraded Output: The Counterintuitive World of Compiler Behavior",
      "authors": [
        {
          "name": "Theodoros Theodoridis",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Zhendong Su",
          "affiliation": "ETH Zurich"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "To optimize a program, a compiler needs precise information about it. Significant effort is dedicated to improving the ability of compilers to analyze programs, with the expectation that more information results in better optimization. But this assumption does not always hold: due to unexpected interactions between compiler components and phase ordering issues, sometimes more information leads to worse optimization. This can lead to wasted research and engineering effort whenever compilers cannot efficiently leverage additional information. In this work, we systematically examine the extent to which additional information can be detrimental to compilers. We consider two types of information: dead code, i.e ., whether a program location is unreachable, and value ranges, i.e ., the possible values a variable can take at a specific program location. Given a seed program, we refine it with additional information and check whether this degrades the output. Based on this approach, we develop a fully automated and effective testing method for identifying such issues, and through an extensive evaluation and analysis, we quantify their existence and prevalence in widely used compilers. In particular, we have reported 59 cases in GCC and LLVM, of which 55 have been confirmed or fixed so far, highlighting the practical relevance and value of our findings. This work’s fresh perspective opens up a new direction in understanding and improving compilers.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3656404",
      "sourceUrl": "https://doi.org/10.1145/3656404",
      "tags": [
        "Testing"
      ],
      "matchedAuthors": [
        "Theodoros Theodoridis"
      ]
    },
    {
      "id": "openalex-w4417133267",
      "source": "openalex-discovery",
      "title": "Polynomial dialect and mlir-opt tutorial upstreamed",
      "authors": [
        {
          "name": "Jeremy Kun",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "I've been upstreaming a bit of my compiler work to the MLIR project. Yesterday, I merged in a tutorial on mlir-opt, the main debugging tool for running passes on MLIR code. This is roughly the upstreamable parts of my first MLIR tutorial entry, MLIR — Running and Testing a Lowering. Mehdi Amini also provided a lot of useful information during review that taught me some stuff I didn't know about the tool.",
      "paperUrl": "https://doi.org/10.59350/kgk2k-vnc27",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Testing"
      ],
      "matchedAuthors": [
        "Jeremy Kun"
      ]
    },
    {
      "id": "openalex-w4406014731",
      "source": "openalex-discovery",
      "title": "PCC: An End-to-End Compilation Framework for Neural Networks on Photonic-Electronic Accelerators",
      "authors": [
        {
          "name": "Bohan Hu",
          "affiliation": "University of Hong Kong"
        },
        {
          "name": "Yinyi Liu",
          "affiliation": "University of Hong Kong"
        },
        {
          "name": "Zhenguo Liu",
          "affiliation": "University of Hong Kong"
        },
        {
          "name": "Wei Zhang",
          "affiliation": "Hong Kong University of Science and Technology"
        },
        {
          "name": "Jiang Xu",
          "affiliation": "Hong Kong University of Science and Technology"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Photonic computing, known for its high bandwidth and energy efficiency, harnesses physical phenomena in the optical domain to accelerate a wide range of computational operations such as dot product, matrix multiplication, Fourier transform, 1D convolution, and more. However, the multitude of computational operations mentioned above poses challenges in mapping realistic neural network workloads onto underlying photonic hardware. This complexity requires extensive expertise and laborious programming, impeding the practical adoption and deployment of photonic acceleration. To address this gap, we propose an end-to-end compilation framework comprising a Photonic Compiler Collection (PCC). This framework automates the mapping of high-level deep neural network (DNN) specifications onto target architectures of photonic-electronic accelerators. Additionally, we present a method to streamline neural network workloads by leveraging the multilevel intermediate representation (MLIR) and compiler optimization techniques, targeting photonic-specific patterns. Moreover, we conduct a comprehensive case study illustrating the integration of a typical computational operator, the Mach-Zehnder Interferometer (MZI) mesh, into PCC. Our experimental results demonstrate that PCC achieves up to a 4x speedup on DNN workloads compared to handcrafted implementations. In summary, our proposed framework offers a practical and automated solution for compiling, optimizing, and flexibly sup-porting newer operators of photonic devices. We anticipate that our framework will significantly accelerate the development and deployment of photonic applications in real-world AI scenarios.",
      "paperUrl": "https://doi.org/10.1109/iccd63220.2024.00052",
      "sourceUrl": "",
      "tags": [
        "AI",
        "MLIR"
      ],
      "matchedAuthors": [
        "Wei Zhang"
      ]
    },
    {
      "id": "openalex-w1493341792",
      "source": "openalex-discovery",
      "title": "Model Transformations to Mitigate the Semantic Gap in Embedded Systems Verification",
      "authors": [
        {
          "name": "Björn Bartels",
          "affiliation": ""
        },
        {
          "name": "Sabine Glesner",
          "affiliation": ""
        },
        {
          "name": "Thomas Göthel",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Technische Universität Berlin – Universitätsbibliothek | Vol. 30 (Issue None)",
      "type": "research-paper",
      "abstract": "The VATES project addresses the problem of verifying embedded software by employing a novel combination of methods that are well-established on the level of declarative models, in particular process-algebraic specifications, as well as of methods that work especially well on the level of executable code. Beginning with executable code, we (automatically) extract a model in the form of a processalgebraic system description formulated in Communicating Sequential Processes (CSP). For this low-level CSP description, we can prove that it refines a high-level CSP specification which was previously developed. To relate the (Low-Level Virtual Machine) LLVM code with the low-level CSP model we designed an operational semantics of LLVM. In ongoing work we investigate the extraction algorithm with respect to preservation of semantics. Thereby, we are finally able to prove that given LLVM code formally conforms to its high-level CSP-based specification. In this paper we give an overview of results of VATES so far and show that this approach has the potential to seamlessly integrate modeling, implementation, transformation and verification stages of embedded system development.",
      "paperUrl": "https://doi.org/10.14279/tuj.eceasst.30.418",
      "sourceUrl": "",
      "tags": [
        "Embedded"
      ],
      "matchedAuthors": [
        "Sabine Glesner"
      ]
    },
    {
      "id": "openalex-w4405300598",
      "source": "openalex-discovery",
      "title": "Mneme",
      "authors": [
        {
          "name": "Konstantinos Parasyris",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "OSTI OAI (U.S. Department of Energy Office of Scientific and Technical Information) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "A simple tool allowing recording the execution of a GPU (CUDA) kernel and replaying that kernel as an independent executable. The tool operates in 3 phases. During compile time the user needs to apply a provided LLVM pass to instrument the code. The pass detects all device global variables and device functions and stores this information with the respective LLVM-IR in the global device memory. The compilation generates a record-able executable. The second phase involves running the application executable with a desired input and using LD_PRELOAD to enable recording. When recording before invoking a device kernel the pre-loaded library stores device memory in persistent storage and associates the memory with the device kernel and an LLVM IR file. At the end of the recorded execution the pre-load library generates a database in the form of a JSON file containing information regarding the LLVM-IR files and the snapshots of device memory. During the third and last phase the user can replay the execution of an kernel as a separate independent executable. Besides executing it the user can modify the LLVM IR file and auto-tune parameters such as kernel launch-bounds or kernel runtime execution parameters (e.g. Kernel Block and Grid Dimensions). Is",
      "paperUrl": "https://www.osti.gov/biblio/2477733",
      "sourceUrl": "https://doi.org/10.11578/dc.20241104.6",
      "tags": [
        "CUDA",
        "GPU",
        "IR"
      ],
      "matchedAuthors": [
        "Giorgis Georgakoudis",
        "Konstantinos Parasyris"
      ]
    },
    {
      "id": "openalex-w4402954404",
      "source": "openalex-discovery",
      "title": "Mix Testing: Specifying and Testing ABI Compatibility of C/C++ Atomics Implementations",
      "authors": [
        {
          "name": "Luke Geeson",
          "affiliation": "University College London"
        },
        {
          "name": "J G. Brotherston",
          "affiliation": "University College London"
        },
        {
          "name": "W. Dijkstra",
          "affiliation": "ARM (United Kingdom)"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Lee Smith",
          "affiliation": "ARM (United Kingdom)"
        },
        {
          "name": "Tyler Sorensen",
          "affiliation": "University of California, Santa Cruz"
        },
        {
          "name": "John Wickerson",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "The correctness of complex software depends on the correctness of both the source code and the compilers that generate corresponding binary code. Compilers must do more than preserve the semantics of a single source file: they must ensure that generated binaries can be composed with other binaries to form a final executable. The compatibility of composition is ensured using an Application Binary Interface (ABI), which specifies details of calling conventions, exception handling, and so on. Unfortunately, there are no official ABIs for concurrent programs, so different atomics mappings, although correct in isolation, may induce bugs when composed. Indeed, today, mixing binaries generated by different compilers can lead to an erroneous resulting binary. We present mix testing : a new technique designed to find compiler bugs when the instructions of a C/C++ test are separately compiled for multiple compatible architectures and then mixed together. We define a class of compiler bugs, coined mixing bugs , that arise when parts of a program are compiled separately using different mappings from C/C++ atomic operations to assembly sequences. To demonstrate the generality of mix testing, we have designed and implemented a tool, atomic-mixer , which we have used: (a) to reproduce one existing non-mixing bug that state-of-the-art concurrency testing tools are limited to being able to find (showing that atomic-mixer at least meets the capabilities of these tools), and (b) to find four previously-unknown mixing bugs in LLVM and GCC, and one prospective mixing bug in mappings proposed for the Java Virtual Machine. Lastly, we have worked with engineers at Arm to specify, for the first time, an atomics ABI for Armv8, and have used atomic-mixer to validate the LLVM and GCC compilers against it.",
      "paperUrl": "https://doi.org/10.1145/3689727",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Testing"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson",
        "Lee Smith"
      ]
    },
    {
      "id": "openalex-w4403223076",
      "source": "openalex-discovery",
      "title": "Minotaur: A SIMD-Oriented Synthesizing Superoptimizer",
      "authors": [
        {
          "name": "Z. Liu",
          "affiliation": "University of Utah"
        },
        {
          "name": "Stefan Mada",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "A superoptimizing compiler—one that performs a meaningful search of the program space as part of the optimization process—can find optimization opportunities that are missed by even the best existing optimizing compilers. We created Minotaur: a superoptimizer for LLVM that uses program synthesis to improve its code generation, focusing on integer and floating-point SIMD code. On an Intel Cascade Lake processor, Minotaur achieves an average speedup of 7.3% on the GNU Multiple Precision library (GMP)’s benchmark suite, with a maximum speedup of 13%. On SPEC CPU 2017, our superoptimizer produces an average speedup of 1.5%, with a maximum speedup of 4.5% for 638.imagick. Every optimization produced by Minotaur has been formally verified, and several optimizations that it has discovered have been implemented in LLVM as a result of our work.",
      "paperUrl": "https://doi.org/10.1145/3689766",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "John Regehr"
      ]
    },
    {
      "id": "openalex-w4400375367",
      "source": "openalex-discovery",
      "title": "Meta Large Language Model Compiler: Foundation Models of Compiler Optimization",
      "authors": [
        {
          "name": "Chris Cummins",
          "affiliation": ""
        },
        {
          "name": "Volker Seeker",
          "affiliation": ""
        },
        {
          "name": "Dejan Grubisic",
          "affiliation": ""
        },
        {
          "name": "Baptiste Rozière",
          "affiliation": ""
        },
        {
          "name": "Jonas Gehring",
          "affiliation": ""
        },
        {
          "name": "Gabriel Synnaeve",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a variety of software engineering and coding tasks. However, their application in the domain of code and compiler optimization remains underexplored. Training LLMs is resource-intensive, requiring substantial GPU hours and extensive data collection, which can be prohibitive. To address this gap, we introduce Meta Large Language Model Compiler (LLM Compiler), a suite of robust, openly available, pre-trained models specifically designed for code optimization tasks. Built on the foundation of Code Llama, LLM Compiler enhances the understanding of compiler intermediate representations (IRs), assembly language, and optimization techniques. The model has been trained on a vast corpus of 546 billion tokens of LLVM-IR and assembly code and has undergone instruction fine-tuning to interpret compiler behavior. LLM Compiler is released under a bespoke commercial license to allow wide reuse and is available in two sizes: 7 billion and 13 billion parameters. We also present fine-tuned versions of the model, demonstrating its enhanced capabilities in optimizing code size and disassembling from x86_64 and ARM assembly back into LLVM-IR. These achieve 77% of the optimising potential of an autotuning search, and 45% disassembly round trip (14% exact match). This release aims to provide a scalable, cost-effective foundation for further research and development in compiler optimization by both academic researchers and industry practitioners.",
      "paperUrl": "https://arxiv.org/pdf/2407.02524",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2407.02524",
      "tags": [
        "GPU",
        "IR"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w4400572687",
      "source": "openalex-discovery",
      "title": "Macaw: A Machine Code Toolbox for the Busy Binary Analyst",
      "authors": [
        {
          "name": "Ryan G. Scott",
          "affiliation": ""
        },
        {
          "name": "Brett Boston",
          "affiliation": ""
        },
        {
          "name": "Benjamin G. Davis",
          "affiliation": ""
        },
        {
          "name": "Iavor S. Diatchki",
          "affiliation": ""
        },
        {
          "name": "Mike Dodds",
          "affiliation": ""
        },
        {
          "name": "Joe Hendrix",
          "affiliation": ""
        },
        {
          "name": "Daniel Matichuk",
          "affiliation": ""
        },
        {
          "name": "Kevin Quick",
          "affiliation": ""
        },
        {
          "name": "Tristan Ravitch",
          "affiliation": ""
        },
        {
          "name": "Valentin Robert",
          "affiliation": ""
        },
        {
          "name": "Benjamin Selfridge",
          "affiliation": ""
        },
        {
          "name": "Andrei Ștefănescu",
          "affiliation": ""
        },
        {
          "name": "Daniel Wagner",
          "affiliation": ""
        },
        {
          "name": "Simon Winwood",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "When attempting to understand the behavior of an executable, a binary analyst can make use of many different techniques. These include program slicing, dynamic instrumentation, binary-level rewriting, symbolic execution, and formal verification, all of which can uncover insights into how a piece of machine code behaves. As a result, there is no one-size-fits-all binary analysis tool, so a binary analysis researcher will often combine several different tools. Sometimes, a researcher will even need to design new tools to study problems that existing frameworks are not well equipped to handle. Designing such tools from complete scratch is rarely time- or cost-effective, however, given the scale and complexity of modern ISAs. We present Macaw, a modular framework that makes it possible to rapidly build reliable binary analysis tools across a range of use cases. Statically typed functional programming techniques are used pervasively throughout Macaw -- these range from using functional optimization passes to encoding tricky architectural invariants at the type level to statically check correctness properties. The level of assurance that functional programming ideas afford us allow us to iterate rapidly on Macaw while still having confidence that the underlying semantics are correct. Over a decade of development, we have used Macaw to support an industrial research team in building tools for machine code-related tasks. As such, the name 'Macaw' refers not just to the framework, but also a suite of tools that are built on top of it. We describe Macaw in depth and describe the different static and dynamic analyses that it performs, many powered by an SMT-based symbolic execution engine. We put a particular focus on interoperability between machine code and higher-level languages, including binary lifting from x86 to LLVM, as well verifying the correctness of mixed C and assembly code.",
      "paperUrl": "https://arxiv.org/pdf/2407.06375",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2407.06375",
      "tags": [],
      "matchedAuthors": [
        "Tristan Ravitch"
      ]
    },
    {
      "id": "openalex-w4400410018",
      "source": "openalex-discovery",
      "title": "MPI Errors Detection using GNN Embedding and Vector Embedding over LLVM IR",
      "authors": [
        {
          "name": "Jad El Karchi",
          "affiliation": ""
        },
        {
          "name": "Hanze Chen",
          "affiliation": "Iowa State University"
        },
        {
          "name": "Ali TehraniJamsaz",
          "affiliation": "Iowa State University"
        },
        {
          "name": "Ali Jannesari",
          "affiliation": "Iowa State University"
        },
        {
          "name": "Mihail Popov",
          "affiliation": ""
        },
        {
          "name": "Emmanuelle Saillard",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Identifying errors in parallel MPI programs is a challenging task. Despite the growing number of verification tools, debugging parallel programs remains a significant challenge. This paper is the first to utilize embedding and deep learning graph neural networks (GNNs) to tackle the issue of identifying bugs in MPI programs. Specifically, we have designed and developed two models that can determine, from a code’s LLVM Intermediate Representation (IR), whether the code is correct or contains a known MPI error.We tested our models using two dedicated MPI benchmark suites for verification: MBI and MPI-CorrBench. By training and validating our models on the same benchmark suite, we achieved a prediction accuracy of 92% in detecting error types. Additionally, we trained and evaluated our models on distinct benchmark suites (e.g., transitioning from MBI to MPI-CorrBench) and achieved a promising accuracy of over 80%. Finally, we investigated the interaction between different MPI errors and quantified our models generalization capabilities over new unseen errors. This involved removing errors types during training and assessing whether our models could still predict them. The detection accuracy of removed errors vary significantly between 20% to 80%, indicating connected error patterns.",
      "paperUrl": "https://doi.org/10.1109/ipdps57955.2024.00059",
      "sourceUrl": "",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Mihail Popov"
      ]
    },
    {
      "id": "openalex-w4407784448",
      "source": "openalex-discovery",
      "title": "Leveraging LLVM OpenMP GPU Offload Optimizations for Kokkos Applications",
      "authors": [
        {
          "name": "Rahulkumar Gayatri",
          "affiliation": "National Energy Research Scientific Computing Center"
        },
        {
          "name": "Shilei Tian",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Stephen L. Olivier",
          "affiliation": "Sandia National Laboratories"
        },
        {
          "name": "Eric Wright",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "OpenMP provides a cross-vendor API for GPU offload that can serve as an implementation layer under performance portability frameworks like the Kokkos C++ library. However, recent work identified some impediments to performance with this approach arising from limitations in the API or in the available implementations. Advanced programming concepts such as hierarchical parallelism and use of dynamic shared memory were a particular area of concern. In this paper, we apply recent improvements and extensions in the LLVM/Clang OpenMP compiler and runtime library to the Kokkos backend that targets GPUs via OpenMP offload. We focus on efficient hierarchical parallelism and use of fast GPU scratch memory. We compare the performance of applications written using the Kokkos library with this improved OpenMP backend against the same programs using the CUDA and HIP backends. This evaluation shows progress toward closing the performance gaps between native and OpenMP backends and offers insights that may be useful to users and implementers of other runtime systems and programming frameworks for GPUs.",
      "paperUrl": "https://doi.org/10.1109/hipc62374.2024.00035",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "C++",
        "Clang",
        "CUDA",
        "GPU",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Johannes Doerfert",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4394995399",
      "source": "openalex-discovery",
      "title": "Leveraging IR based sequence and graph features for source-binary code alignment",
      "authors": [
        {
          "name": "Zhouqian Yu",
          "affiliation": "Beijing Information Science & Technology University"
        },
        {
          "name": "Wei Zhang",
          "affiliation": "Beijing Information Science & Technology University"
        },
        {
          "name": "Tao Xu",
          "affiliation": "Tsinghua University"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Code similarity analysis is a versatile technique that can be applied across various domains, including code clone detection, code search, malware detection, patch analysis, and vulnerability search. The core of source-binary code similarity analysis lies in effective modeling of the both of source code and binary code. Existing researches have overlooked the fact that code has both text and graph structure features, so they usually only use one of these features for modeling. In this paper, we propose a method that combines sequence features based on intermediate representation and program graph features derived from intermediate representation to enhance the capture of program semantics. Our approach involves the parallel embedding and learning of LLVM IR and the derived program graphs, leading to the extraction of the final code feature representation. Furthermore, we employ a triplet-loss network to identify code characteristic disparities and produce similarity rankings. The experimental results have demonstrated that our approach outperforms other models on the baseline, achieving a 6% improvement in accuracy for source-binary code similarity tasks.",
      "paperUrl": "https://doi.org/10.1109/nnice61279.2024.10499062",
      "sourceUrl": "",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Wei Zhang"
      ]
    },
    {
      "id": "openalex-w4399511906",
      "source": "openalex-discovery",
      "title": "LLM-Vectorizer: LLM-based Verified Loop Vectorizer",
      "authors": [
        {
          "name": "Jubi Taneja",
          "affiliation": ""
        },
        {
          "name": "Avery Laird",
          "affiliation": ""
        },
        {
          "name": "Cong Yan",
          "affiliation": ""
        },
        {
          "name": "Madan Musuvathi",
          "affiliation": ""
        },
        {
          "name": "Shuvendu K. Lahiri",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Vectorization is a powerful optimization technique that significantly boosts the performance of high performance computing applications operating on large data arrays. Despite decades of research on auto-vectorization, compilers frequently miss opportunities to vectorize code. On the other hand, writing vectorized code manually using compiler intrinsics is still a complex, error-prone task that demands deep knowledge of specific architecture and compilers. In this paper, we evaluate the potential of large-language models (LLMs) to generate vectorized (Single Instruction Multiple Data) code from scalar programs that process individual array elements. We propose a novel finite-state machine multi-agents based approach that harnesses LLMs and test-based feedback to generate vectorized code. Our findings indicate that LLMs are capable of producing high performance vectorized code with run-time speedup ranging from 1.1x to 9.4x as compared to the state-of-the-art compilers such as Intel Compiler, GCC, and Clang. To verify the correctness of vectorized code, we use Alive2, a leading bounded translation validation tool for LLVM IR. We describe a few domain-specific techniques to improve the scalability of Alive2 on our benchmark dataset. Overall, our approach is able to verify 38.2% of vectorizations as correct on the TSVC benchmark dataset.",
      "paperUrl": "https://arxiv.org/pdf/2406.04693",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2406.04693",
      "tags": [
        "Clang",
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Jubi Taneja"
      ]
    },
    {
      "id": "openalex-w4406171566",
      "source": "openalex-discovery",
      "title": "JACC: Leveraging HPC Meta-Programming and Performance Portability with the Just-in-Time and LLVM-based Julia Language",
      "authors": [
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "William F. Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Het Mankad",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Keita Teranishi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Johannes Blaschke",
          "affiliation": "Lawrence Berkeley National Laboratory"
        },
        {
          "name": "Michel Schanen",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present JACC (Julia for Accelerators), the first high-level, and performance-portable model for the just-in-time and LLVM-based Julia language. JACC provides a unified and lightweight front end across different back ends available in Julia, enabling the same Julia code to run efficiently on many HPC CPU and GPU targets. We evaluated the performance of JACC for common HPC kernels as well as for the most computationally demanding kernels used in applications, HPCCG, a supercomputing benchmark test for sparse domains, and HARVEY, a blood flow simulator to assist in the diagnosis and treatment of patients suffering from vascular diseases. We carried out the performance analysis on the most advanced US DOE supercomputers: Aurora, Frontier, and Perlmutter. Overall, we show that JACC has a negligible overhead versus vendor-specific solutions, reporting GPU speedups with no extra cost to programmability.",
      "paperUrl": "https://www.osti.gov/servlets/purl/2561236",
      "sourceUrl": "https://doi.org/10.1109/scw63240.2024.00245",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4409133366",
      "source": "openalex-discovery",
      "title": "JACC.shared: Leveraging HPC Metaprogramming and Performance Portability for Computations That Use Shared Memory GPUs",
      "authors": [
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "William F. Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Keita Teranishi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this work, we present JACC.shared, a new feature of Julia for ACCelerators (JACC), which is the performanceportable and metaprogramming model of the just-in-time and LLVM-based Julia language. This new feature allows JACC applications to leverage the high-performance computing (HPC) capabilities of high-bandwidth, on-chip GPU memory. Historically, exploiting high-bandwidth, shared-memory GPUs has not been a priority for high-level programming solutions. JACC.shared covers that gap for the first time, thereby providing a highlevel, portable, and easy-to-use solution for programmers to exploit this memory and supporting all current major accelerator architectures. Well-known HPC and AI workloads, such as multi/hyperspectral imaging and AI convolutions, have been used to evaluate JACC.shared on two exascale GPU architectures hosted by some of the most powerful US Department of Energy supercomputers: Perlmutter (NVIDIA A100) and Frontier (AMD MI250X). The performance evaluation reports speedup of up to 3.5× by adding only one line of code to the base codes, thus providing important accelerators in a simple, portable, and transparent way and elevating the programming productivity and performance-portability capabilities for Julia/JACC HPC, AI, and scientific applications.",
      "paperUrl": "www.osti.gov/servlets/purl/2561231",
      "sourceUrl": "https://doi.org/10.1109/hpec62836.2024.10938453",
      "tags": [
        "AI",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4406157215",
      "source": "openalex-discovery",
      "title": "Integrating ORNL’s HPC and Neutron Facilities with a Performance-Portable CPU/GPU Ecosystem",
      "authors": [
        {
          "name": "Steven Hahn",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Philip W. Fackler",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "William F. Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Ketan Maheshwari",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Zachary Morgan",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "A. T. Savici",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Christina Hoffmann",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Rafael Ferreira da Silva",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We explore the development of a performance-portable CPU/GPU ecosystem to integrate two of the US Department of Energy’s (DOE’s) largest scientific instruments, the Oak Ridge Leadership Computing facility and the Spallation Neutron Source (SNS), both of which are housed at Oak Ridge National Laboratory. We select a relevant data reduction workflow use-case to obtain the differential scattering cross-section from data collected by SNS’s CORELLI and TOPAZ instruments. We compare the current CPU-only production implementation using the Garnet Python multiprocess package based on the Mantid C++ framework against our proposed CPU/GPU implementation that uses the LLVM-based, just-in-time Julia scientific language and the JACC.jl performance-portable package. Two proxy apps were developed: (i) an app for extracting relevant Mantid kernels (MDNorm) in C++ and (ii) the Julia MiniVATES.jl miniapp. We present performance results for NVIDIA A100 and AMD MI100 GPUs and AMD EPYC 7513 and 7662 CPUs. The results provide insights for future generations of data reduction software that can embrace performance portability for an integrated research infrastructure across DOE’s experimental and computational facilities.",
      "paperUrl": "www.osti.gov/servlets/purl/2538328",
      "sourceUrl": "https://doi.org/10.1109/scw63240.2024.00264",
      "tags": [
        "C++",
        "GPU",
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4402386987",
      "source": "openalex-discovery",
      "title": "Implementing and Executing Static Analysis Using LLVM and CodeChecker",
      "authors": [
        {
          "name": "Gábor Horváth",
          "affiliation": ""
        },
        {
          "name": "Réka Kovács",
          "affiliation": ""
        },
        {
          "name": "Richárd Szalay",
          "affiliation": ""
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Static analysis is a method of analyzing source code without executing it. It is widely used to find bugs and code smells in industrial software. Besides other methods, the most important techniques are those based on the abstract syntax tree and those performing symbolic execution. Both of these methods found their role in modern software development as they have different advantages and limitations. In this tutorial, we present two problems from the C++ programming language: the elimination of redundant pointers, and the reporting of dangling pointers originating from incorrect use of the std::string class. These two issues have different theoretical backgrounds and finding them requires different implementation techniques. We will provide a step-by-step guide to implement the checkers (software to identify the aforementioned problems) - one based on the abstract syntax analysis method, the other exploring the possibilities of symbolic execution. The methods are explained in great detail and supported by code examples. The intended audience for this tutorial are both architects of static analysis tools and developers who want to understand the advantages and constraints of the different methods.",
      "paperUrl": "https://arxiv.org/pdf/2408.05657",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2408.05657",
      "tags": [
        "C++",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Réka Kovács",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4391951359",
      "source": "openalex-discovery",
      "title": "If-Convert as Early as You Must",
      "authors": [
        {
          "name": "Dorit Nuzman",
          "affiliation": ""
        },
        {
          "name": "Ayal Zaks",
          "affiliation": ""
        },
        {
          "name": "Ziv Ben-Zion",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Optimizing compilers employ a rich set of transformations that generate highly efficient code for a variety of source languages and target architectures. These transformations typically operate on general control flow constructs which trigger a range of optimization opportunities, such as moving code to less frequently executed paths, and more. Regular loop nests are specifically relevant for accelerating certain domains, leveraging architectural features including vector instructions, hardware-controlled loops and data flows, provided their internal control-flow is eliminated. Compilers typically apply predicating if-conversion late, in their backend, to remove control-flow undesired by the target. Until then, transformations triggered by control-flow constructs that are destined to be removed may end up doing more harm than good. We present an approach that leverages the existing powerful and general optimization flow of LLVM when compiling for targets without control-flow in loops. Rather than trying to teach various transformations how to avoid misoptimizing for such targets, we propose to introduce an aggressive if-conversion pass as early as possible, along with carefully addressing pass-ordering implications. This solution outperforms the traditional compilation flow with only a modest tuning effort, thereby offering a robust and promising compilation approach for branch-restricted targets.",
      "paperUrl": "https://doi.org/10.1145/3640537.3641562",
      "sourceUrl": "",
      "tags": [
        "Backend"
      ],
      "matchedAuthors": [
        "Ayal Zaks"
      ]
    },
    {
      "id": "openalex-w4391709377",
      "source": "openalex-discovery",
      "title": "IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation",
      "authors": [
        {
          "name": "Yuyang Rong",
          "affiliation": ""
        },
        {
          "name": "Zhanghan Yu",
          "affiliation": ""
        },
        {
          "name": "Zhenkai Weng",
          "affiliation": ""
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": ""
        },
        {
          "name": "Hao Chen",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern compilers, such as LLVM, are complex pieces of software. Due to their complexity, manual testing is unlikely to suffice, yet formal verification is difficult to scale. End-to-end fuzzing can be used, but it has difficulties in achieving high coverage of some components of LLVM. In this paper, we implement IRFuzzer to investigate the effectiveness of specialized fuzzing of the LLVM compiler backend. We focus on two approaches to improve the fuzzer: guaranteed input validity using constrained mutations and improved feedback quality. The mutator in IRFuzzer is capable of generating a wide range of LLVM IR inputs, including structured control flow, vector types, and function definitions. The system instruments coding patterns in the compiler to monitor the execution status of instruction selection. The instrumentation not only provides a new coverage feedback called matcher table coverage, but also provides an architecture specific guidance to the mutator. We show that IRFuzzer is more effective than existing fuzzers by fuzzing on 29 mature LLVM backend targets. In the process, we reported 74 confirmed new bugs in LLVM upstream, out of which 49 have been fixed, five have been back ported to LLVM 15, showing that specialized fuzzing provides useful and actionable insights to LLVM developers.",
      "paperUrl": "https://arxiv.org/pdf/2402.05256",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2402.05256",
      "tags": [
        "Backend",
        "IR",
        "Testing"
      ],
      "matchedAuthors": [
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w4394999132",
      "source": "openalex-discovery",
      "title": "Hydride: A Retargetable and Extensible Synthesis-based Compiler for Modern Hardware Architectures",
      "authors": [
        {
          "name": "Akash Kothari",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Abdul Rafae Noor",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Muchen Xu",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Hassam Uddin",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Dhruv Baronia",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Stefanos Baziotis",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Vikram Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Charith Mendis",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Sudipta Sengupta",
          "affiliation": "Seattle University"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As modern hardware architectures evolve to support increasingly diverse, complex instruction sets for meeting the performance demands of modern workloads in image processing, deep learning, etc., it has become ever more crucial for compilers to provide robust support for evolution of their internal abstractions and retargetable code generation support to keep pace with emerging instruction sets. We propose Hydride, a novel approach to compiling for complex, emerging hardware architectures. Hydride uses vendor-defined pseudocode specifications of multiple hardware ISAs to automatically design retargetable instructions for AutoLLVM IR, an extensible compiler IR which consists of (formally defined) language-independent and target-independent LLVM IR instructions to compile to those ISAs, and automatically generated instruction selection passes to lower AutoLLVM IR to each of the specified hardware ISAs. Hydride also includes a code synthesizer that automatically generates code generation support for schedule-based languages, such as Halide, to optimally generate AutoLLVM IR. Our results show that Hydride is able to represent 3,557 instructions combined in x86, Hexagon, ARM architectures using only 397 AutoLLVM IR instructions, including (Intel) SSE2, SSE4, AVX, AVX2, AVX512, (Qualcomm) Hexagon HVX, and (ARM) NEON vector ISAs. We created a new Halide compiler with Hydride using only a formal semantics of Halide IR, leveraging the auto-generated AutoLLVM IR and back-ends for the three hardware architectures. Across kernels from deep learning and image processing, this compiler is able to perform just as well as the mature, production Halide compiler on Hexagon, and outperform on x86 by 8% and ARM by 3%. Hydride also outperforms the production Halide's LLVM back end by 12% on x86, 100% on HVX, and 26% on ARM across the same kernels.",
      "paperUrl": "https://doi.org/10.1145/3620665.3640385",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Stefanos Baziotis",
        "Vikram Adve"
      ]
    },
    {
      "id": "openalex-w4396214531",
      "source": "openalex-discovery",
      "title": "Hydra: Generalizing Peephole Optimizations with Program Synthesis",
      "authors": [
        {
          "name": "Manasij Mukherjee",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue OOPSLA1)",
      "type": "research-paper",
      "abstract": "Optimizing compilers rely on peephole optimizations to simplify combinations of instructions and remove redundant instructions. Typically, a new peephole optimization is added when a compiler developer notices an optimization opportunity---a collection of dependent instructions that can be improved---and manually derives a more general rewrite rule that optimizes not only the original code, but also other, similar collections of instructions. In this paper, we present Hydra, a tool that automates the process of generalizing peephole optimizations using a collection of techniques centered on program synthesis. One of the most important problems we have solved is finding a version of each optimization that is independent of the bitwidths of the optimization's inputs (when this version exists). We show that Hydra can generalize 75% of the ungeneralized missed peephole optimizations that LLVM developers have posted to the LLVM project's issue tracker. All of Hydra's generalized peephole optimizations have been formally verified, and furthermore we can automatically turn them into C++ code that is suitable for inclusion in an LLVM pass.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3649837",
      "sourceUrl": "https://doi.org/10.1145/3649837",
      "tags": [
        "C++",
        "Optimizations"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Manasij Mukherjee"
      ]
    },
    {
      "id": "openalex-w4392265898",
      "source": "openalex-discovery",
      "title": "High-Throughput, Formal-Methods-Assisted Fuzzing for LLVM",
      "authors": [
        {
          "name": "Yuyou Fan",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "It is very difficult to thoroughly test a compiler, and as a consequence it is common for released versions of production compilers to contain bugs that cause them to crash and to emit incorrect object code. We created alive-mutate, a mutation-based fuzzing tool that takes test cases written by humans and randomly modifies them, based on the hypothesis that while compiler developers are fundamentally good at writing tests, they also tend to miss corner cases. Alive-mutate is integrated with the Alive2 translation validation tool for LLVM, which is useful because it checks the behavior of optimizations for all possible values of input variables. Alive-mutate is also integrated with the LLVM middle-end, allowing it to perform mutations, optimizations, and formal verification of the optimizations all within a single program—avoiding numerous sources of overhead. Alive-mutate's fuzzing throughput is 12x higher, on average, than a fuzzing workflow that runs mutation, optimization, and formal verification in separate processes. So far we have used alive-mutate to find and report 33 previously unknown bugs in LLVM.",
      "paperUrl": "https://doi.org/10.1109/cgo57630.2024.10444854",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Yuyou Fan"
      ]
    },
    {
      "id": "openalex-w4404133411",
      "source": "openalex-discovery",
      "title": "High-Performance FFT Code Generation via MLIR Linalg Dialect and SIMD Micro-Kernels",
      "authors": [
        {
          "name": "Yifei He",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Stefano Markidis",
          "affiliation": "KTH Royal Institute of Technology"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Fast Fourier Transform (FFT) libraries are an indispensable and critical component of any High-Performance Computing (HPC) software stack. They are used in many applications, from Partial Differential Equation (PDE) solvers to signal spectral analysis and deep-learning. To design and develop the next generation of HPC FFT libraries, it is essential to leverage modern compiler infrastructures, such as Multi-Level Intermediate Representation (MLIR) and the Low-Level Virtual Machine (LLVM), alongside advanced computer architecture features, including SIMD (Single Instruction, Multiple Data) instructions. In this work, we introduce FFTc 2.0, an MLIR-based domain-specific compilation framework for FFT. We extend the MLIR Linalg dialect with FFT-specific operations to harness high-level tensor-based abstractions for FFT. These abstractions are ideal for formulating various FFT algorithms and facilitating formula rewriting for FFT decomposition and cache-friendly optimization. We employ micro-kernels to increase the performance, particularly in response to the limited support for complex arithmetic in MLIR and the general-purpose compiler LLVM. These micro-kernels are designed to implement small-size FFT kernels for integration into larger FFTs. They utilize SIMD-friendly data layouts for complex arithmetic and feature an optimized memory access pattern, enabling performance enhancements that are not achievable with standard compiler implementations. Our method achieves performance levels comparable to or surpass those of widely used FFT HPC libraries, such as FFTW, FFTE, and Spiral FFT. Additionally, it establishes a robust software infrastructure that facilitates further optimizations and supports additional hardware backends.",
      "paperUrl": "http://dx.doi.org/10.1109/cluster59578.2024.00021",
      "sourceUrl": "https://doi.org/10.1109/cluster59578.2024.00021",
      "tags": [
        "Backend",
        "Infrastructure",
        "Libraries",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Yifei He"
      ]
    },
    {
      "id": "openalex-w4394871769",
      "source": "openalex-discovery",
      "title": "Formal Mechanised Semantics of CHERI C: Capabilities, Undefined Behaviour, and Provenance",
      "authors": [
        {
          "name": "Vadim Zaliva",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Kayvan Memarian",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Ricardo Almeida",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Jessica Clarke",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Brooks Davis",
          "affiliation": "SRI International"
        },
        {
          "name": "Alexander Richardson",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "David Chisnall",
          "affiliation": "Microsoft Research (United Kingdom)"
        },
        {
          "name": "B. K. Campbell",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Ian Stark",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Robert N. M. Watson",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Peter Sewell",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Memory safety issues are a persistent source of security vulnerabilities, with conventional architectures and the C codebase chronically prone to exploitable errors. The CHERI research project has shown how one can provide radically improved security for that existing codebase with minimal modification, using unforgeable hardware capabilities in place of machine-word pointers in CHERI dialects of C, implemented as adaptions of Clang/LLVM and GCC. CHERI was first prototyped as extensions of MIPS and RISC-V; it is currently being evaluated by Arm and others with the Arm Morello experimental architecture, processor, and platform, to explore its potential for mass-market adoption, and by Microsoft in their CHERIoT design for embedded cores. There is thus considerable practical experience with CHERI C implementation and use, but exactly what CHERI C's semantics is (or should be) remains an open question. In this paper, we present the first attempt to rigorously and comprehensively define CHERI C semantics, discuss key semantics design questions relating to capabilities, provenance, and undefined behaviour, and clarify them with semantics in multiple complementary forms: in prose, as an executable semantics adapting the Cerberus C semantics, and mechanised in Coq. This establishes a solid foundation for CHERI C, for those porting code to it, for compiler implementers, and for future semantics and verification.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3617232.3624859",
      "sourceUrl": "https://doi.org/10.1145/3617232.3624859",
      "tags": [
        "Clang",
        "Embedded",
        "Security"
      ],
      "matchedAuthors": [
        "Alexander Richardson",
        "Brooks Davis",
        "David Chisnall",
        "Robert N. M. Watson"
      ]
    },
    {
      "id": "openalex-w4395686515",
      "source": "openalex-discovery",
      "title": "Forklift: An Extensible Neural Lifter",
      "authors": [
        {
          "name": "Jordi Armengol-Estapé",
          "affiliation": ""
        },
        {
          "name": "Rodrigo C. O. Rocha",
          "affiliation": ""
        },
        {
          "name": "Jackson Woodruff",
          "affiliation": ""
        },
        {
          "name": "Pasquale Minervini",
          "affiliation": ""
        },
        {
          "name": "Michael O’Boyle",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The escalating demand to migrate legacy software across different Instruction Set Architectures (ISAs) has driven the development of assembly-to-assembly translators to map between their respective assembly languages. However, the development of these tools requires substantial engineering effort. State-of-the-art approaches use lifting, a technique where source assembly code is translated to an architecture-independent intermediate representation (IR) (for example, the LLVM IR) and use a pre-existing compiler to recompile the IR to the target ISA. However, the hand-written rules these lifters employ are sensitive to the particular compiler and optimization level used to generate the code and require significant engineering effort to support each new ISA. We propose Forklift, the first neural lifter that learns how to translate assembly to LLVM IR using a token-level encoder-decoder Transformer. We show how to incrementally add support to new ISAs by fine tuning the assembly encoder and freezing the IR decoder, improving the overall accuracy and efficiency. We collect millions of parallel LLVM IR, x86, ARM, and RISC-V programs across compilers and optimization levels to train Forklift and set up an input/output-based accuracy harness. We evaluate Forklift on two challenging benchmark suites and translate 2.5x more x86 programs than a state-of-the-art hand-written lifter and 4.4x more x86 programs than GPT-4 as well as enabling translation from new ISAs.",
      "paperUrl": "https://arxiv.org/pdf/2404.16041",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2404.16041",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Rodrigo C. O. Rocha"
      ]
    },
    {
      "id": "openalex-w4408258357",
      "source": "openalex-discovery",
      "title": "Finding bugs on tagged unions mismatches",
      "authors": [
        {
          "name": "Gábor Tóthvári",
          "affiliation": ""
        },
        {
          "name": "Kristóf Umann",
          "affiliation": ""
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Unions in C may have several data members, but all of them start at the same memory address. In essence, unions store a single value with multiple type alternatives, with the current type of the value being the active type. However, plain unions do not keep track of the active type, and reading the value with an incorrect type might be undefined behaviour in C, and definitely undefined in C++. For this reason, programmers often resort to using tagged unions, where an enumerating variable is used alongside the union – the value of the enumerating variable keeps track of the active type in the union.While this makes accessing the value inside a union safer, it is still possible to make mistakes with tagged unions. During code maintenance, the number of data members of a union might change, and the number of constants declared in the enumeration needs to be in sync. If these values are not equal, its a sign of code smell.In this paper, we investige how programmers use and misuse of tagged unions. We implemented a static analysis check in Clang- Tidy, a subproject of LLVM, which has since been accepted into the project by the community and will be available for every Clang-Tidy user starting from the next major Clang release v20.0.0 [1]. We ran our check on large, open source C and C++ projects, and were able to specify a new reliable rule for the usage and maintenance of tagged unions.",
      "paperUrl": "https://doi.org/10.1109/informatics62280.2024.10900816",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Kristóóf Umann",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4392265996",
      "source": "openalex-discovery",
      "title": "Experiences Building an MLIR-Based SYCL Compiler",
      "authors": [
        {
          "name": "Ettore Tiotto",
          "affiliation": ""
        },
        {
          "name": "Víctor Pérez",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Whitney Tsang",
          "affiliation": ""
        },
        {
          "name": "Lukáš Sommer",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Julian Oppermann",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Victor Lomüller",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Mehdi Goli",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "James Brodman",
          "affiliation": "Intel (Poland)"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Similar to other programming models, compilers for SYCL, the open programming model for heterogeneous computing based on C++, would benefit from access to higher-level intermediate representations. The loss of high-level structure and semantics caused by premature lowering to low-level intermediate representations and the inability to reason about host and device code simultaneously present major challenges for SYCL compilers. The MLIR compiler framework, through its dialect mechanism, allows to model domain-specific, high-level intermediate representations and provides the necessary facilities to address these challenges. This work therefore describes practical experience with the design and implementation of an MLIR-based SYCL compiler. By modeling key elements of the SYCL programming model in host and device code in the MLIR dialect framework, the presented approach enables the implementation of powerful device code optimizations as well as analyses across host and device code. Compared to two LLVM-based SYCL implementations, this yields speedups of up to 4.3x on a collection of SYCL benchmark applications. Finally, this work also discusses challenges encountered in the design and implementation and how these could be addressed in the future.",
      "paperUrl": "https://doi.org/10.1109/cgo57630.2024.10444866",
      "sourceUrl": "",
      "tags": [
        "C++",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Ettore Tiotto",
        "Julian Oppermann",
        "Victor Lomüller",
        "Whitney Tsang"
      ]
    },
    {
      "id": "openalex-w4404099802",
      "source": "openalex-discovery",
      "title": "Evaluation of Vectorization Methods on Arm SVE Using the Exo Language",
      "authors": [
        {
          "name": "Rin Iwai",
          "affiliation": "Toyohashi University of Technology"
        },
        {
          "name": "Emil Vatai",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Jens Domke",
          "affiliation": "RIKEN Center for Computational Science"
        },
        {
          "name": "Yukinori Sato",
          "affiliation": "Toyohashi University of Technology"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Vectorization is important for achieving high performance in numerical computations. This poster evaluates various vectorization methods for an application utilizing the Arm Scalable Vector Extensions (SVE) instructions. To fully control the vectorization “pipeline”, we use the Exo language, which is a domain-specific language designed for performance engineers. In this poster, we discuss two existing vectorization strategies, known as Vector Length Agnostic (VLA) and Vector Length Specific (VLS) approaches. To evaluate the performance effects against these vectorized code, we generate them through the Exo compiler coupled with native backend compilers on the Fujitsu A64FX processor (i.e., gcc, clang, and fcc). Our results show that the code generated by the VLA and VLS approaches, including simple loop optimization, outperforms code without explicit vectorization. We also found that VLS often provides better performance over VLA. These results indicate that code transformation through “Exocompilation” is useful for optimizing code and investigating the behavior of compilers.",
      "paperUrl": "http://dx.doi.org/10.1109/clusterworkshops61563.2024.00047",
      "sourceUrl": "https://doi.org/10.1109/clusterworkshops61563.2024.00047",
      "tags": [
        "Backend",
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Emil Vatai",
        "Jens Domke"
      ]
    },
    {
      "id": "openalex-w4406157403",
      "source": "openalex-discovery",
      "title": "Evaluating Tuning Opportunities of the LLVM/OpenMP Runtime",
      "authors": [
        {
          "name": "Smeet Chheda",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Gaurav Verma",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Shilei Tian",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Tuning parallel applications on multi-core architectures is an arduous task. Several studies have utilized auto-tuning for OpenMP applications via standardized user-facing features, namely number of threads, thread placement, binding and scheduling policy. However, they fall short on utilizing the additional parameters provided by an OpenMP implementation. In this paper, we analyze OpenMP application runtime through an exhaustive exploration of all relevant configuration options of the LLVM/OpenMP runtime.Our findings allow to identify trends in tuning potential, architecture-aware tuning suggestions, and good default configurations per architecture. We will open-source the 240,000 unique samples collected during experiments for use by the community. These runs have been conducted on three different CPU architectures vital in the HPC and datacenter community. Choice of applications includes popular benchmark suites and microbench-marks namely, NAS Parallel Benchmarks, Barcelona OpenMP Task Suite, XSBench, RSBench, SU3Bench and LULESH.We employ the Linear Models class of Machine Learning algorithms to perform analysis, explain, and form qualitative relations between features comprising of the underlying architecture, application, input size, number of threads, and considered environment variables. This is further used to recommend different configurations given an application type/architecture.",
      "paperUrl": "https://doi.org/10.1109/scw63240.2024.00131",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Barbara Chapman",
        "Johannes Doerfert",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4392265885",
      "source": "openalex-discovery",
      "title": "Enhancing Performance Through Control-Flow Unmerging and Loop Unrolling on GPUs",
      "authors": [
        {
          "name": "Alnis Murtovi",
          "affiliation": "TU Dortmund University"
        },
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Konstantinos Parasyris",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Chunhua Liao",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Bernhard Steffen",
          "affiliation": "TU Dortmund University"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compilers use a wide range of advanced optimizations to improve the quality of the machine code they generate. In most cases, compiler optimizations rely on precise analyses to be able to perform the optimizations. However, whenever a control-flow merge is performed information is lost as it is not possible to precisely reason about the program anymore. One existing solution to this issue is code duplication, which involves duplicating instructions from merge blocks to their predecessors. This paper introduces a novel and more aggressive approach to code duplication, grounded in loop unrolling and control-flow unmerging that enables subsequent optimizations that cannot be enabled by applying only one of these transformations. We implemented our approach inside LLVM, and evaluated its performance on a collection of GPU benchmarks in CUDA. Our results demonstrate that, even when faced with branch divergence, which complicates code duplication across multiple branches and increases the associated cost, our optimization technique achieves performance improvements of up to 81%.",
      "paperUrl": "https://www.osti.gov/servlets/purl/2325331",
      "sourceUrl": "https://doi.org/10.1109/cgo57630.2024.10444819",
      "tags": [
        "CUDA",
        "GPU",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Giorgis Georgakoudis",
        "Ignacio Laguna",
        "Konstantinos Parasyris"
      ]
    },
    {
      "id": "openalex-w4404952783",
      "source": "openalex-discovery",
      "title": "Enhancing Black-box Compiler Option Fuzzing with LLM through Command Feedback",
      "authors": [
        {
          "name": "Taiyan Wang",
          "affiliation": ""
        },
        {
          "name": "Ruipeng Wang",
          "affiliation": ""
        },
        {
          "name": "Yu Chen",
          "affiliation": ""
        },
        {
          "name": "Lu Yu",
          "affiliation": ""
        },
        {
          "name": "Zulie Pan",
          "affiliation": ""
        },
        {
          "name": "Min Zhang",
          "affiliation": ""
        },
        {
          "name": "Huimin Ma",
          "affiliation": ""
        },
        {
          "name": "Jinghua Zheng",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Since the compiler acts as a core component in software building, it is essential to ensure its availability and reliability through software testing and security analysis. Most research has focused on compiler robustness when compiling various test cases, while the reliability of compiler options lacks attention, especially since each option can activate a specific compiler function. Although some researchers have made efforts in testing it, the insufficient utilization of compiler command feedback messages leads to the poor efficiency, which hinders more diverse and in-depth testing.In this paper, we propose a novel solution to enhance black-box compiler option fuzzing by utilizing command feedback, such as error messages, standard output and compiled files, to guide the error fixing and option pruning via prompting large language models for suggestions. We have implemented the prototype and evaluated it on 4 versions of LLVM. Experiments show that our method significantly improves the detection of crashes, reduces false negatives, and even increase the success rate of compilation when compared to the baseline. To date, our method has identified hundreds of unique bugs, and 9 of them are previously unknown. Among these, 8 have been assigned CVE numbers, and 1 has been fixed following our report.",
      "paperUrl": "https://doi.org/10.1109/issre62328.2024.00039",
      "sourceUrl": "",
      "tags": [
        "Security",
        "Testing"
      ],
      "matchedAuthors": [
        "Yu Chen"
      ]
    },
    {
      "id": "openalex-w4392266003",
      "source": "openalex-discovery",
      "title": "Enabling Fine-Grained Incremental Builds by Making Compiler Stateful",
      "authors": [
        {
          "name": "Ruobing Han",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Jisheng Zhao",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Hyesoon Kim",
          "affiliation": "Georgia Institute of Technology"
        }
      ],
      "year": "2024",
      "venue": "Vol. 25 (Issue None)",
      "type": "research-paper",
      "abstract": "Incremental builds are commonly employed in software development, involving minor changes to existing source code that is then frequently recompiled. Speeding up incremental builds not only enhances the software development workflow but also improves CI/CD systems by enabling faster verification steps. Current solutions for incremental builds primarily rely on build systems that analyze file dependencies to avoid unnecessary recompilation of unchanged files. However, for the files that do undergo changes, these build systems simply invoke compilers to recompile them from scratch. This approach reveals a fundamental asymmetry in the system: while build systems operate in a stateful manner, compilers are stateless. As a result, incremental builds are applied only at a coarse-grained level, focusing on entire source files, rather than at a more fine-grained level that considers individual code sections. In this paper, we propose an innovative approach for enabling the fine-grained incremental build by introducing statefulness into compilers. Under this paradigm, the compiler leverages its profiling history to expedite the compilation process of modified source files, thereby reducing overall build time. Specifically, the stateful compiler retains dormant information of compiler passes executed in previous builds and uses this data to bypass dormant passes during subsequent incremental compilations. We also outline the essential changes needed to transform conventional stateless compilers into stateful ones. For practical evaluation, we modify the Clang compiler to adopt a stateful architecture and evaluate its performance on real-world C++ projects. Our comparative study indicates that the stateful version outperforms the standard Clang compiler in incremental builds, accelerating the end-to-end build process by an average of 6.72%.",
      "paperUrl": "http://dx.doi.org/10.1109/cgo57630.2024.10444865",
      "sourceUrl": "https://doi.org/10.1109/cgo57630.2024.10444865",
      "tags": [
        "C++",
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Ruobing Han"
      ]
    },
    {
      "id": "openalex-w4405550527",
      "source": "openalex-discovery",
      "title": "Efficient compiler optimization by modeling passes dependence",
      "authors": [
        {
          "name": "Jianfeng Liu",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Jianbin Fang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Ting Wang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Jing Xie",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Chun Huang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Zheng Wang",
          "affiliation": "Northwest University"
        }
      ],
      "year": "2024",
      "venue": "CCF Transactions on High Performance Computing | Vol. 6 (Issue 6)",
      "type": "research-paper",
      "abstract": "Abstract Selecting the optimal combination of compiler passes is a significant challenge to enhance performance and reduce the code size of compiled binaries. While a well-selected sequence of compiler passes can yield considerable benefits, the large number of potential combinations and the scarcity of effective ones make this task prohibitively complex. To tackle this problem, we propose a novel approach to group compiler passes into a small set of sub-sequences. This approach translates the task of identifying the right compiler passes combination into determining the appropriate combination of these sub-sequences. We apply our approach to CBench and PolyBench, demonstrating remarkable performance improvements. Our approach enhances runtime performance by 22% compared to the default LLVM ‘O3’ option, and achieves a code size reduction of 24% compared to the ‘Oz’ option. Our approach also outperforms state-of-the-art across various optimization tasks and hardware platforms.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/s42514-024-00197-9.pdf",
      "sourceUrl": "https://doi.org/10.1007/s42514-024-00197-9",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Jianbin Fang",
        "Zheng Wang"
      ]
    },
    {
      "id": "openalex-w4406262153",
      "source": "openalex-discovery",
      "title": "Design and Architecture of the IBM Quantum Engine Compiler",
      "authors": [
        {
          "name": "Michael B. Healy",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Reza Jokar",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Soolu Thomas",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "V. R. Pascuzzi",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Kit Barton",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Thomas A. Alexander",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Roy Elkabetz",
          "affiliation": "IBM Research - Haifa"
        },
        {
          "name": "Brian C. Donovan",
          "affiliation": "Cambridge Scientific (United States)"
        },
        {
          "name": "Hiroshi Horii",
          "affiliation": "IBM Research - Tokyo"
        },
        {
          "name": "Marius Hillenbrand",
          "affiliation": "IBM (Germany)"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this work, we describe the design and architecture of the open-source Quantum Engine Compiler (qe-compiler) currently used in production for IBM Quantum systems. The qe-compiler is built using LLVM's Multi-Level Intermediate Representation (MLIR) framework and includes definitions for several dialects to represent parameterized quantum computation at multiple levels of abstraction. The compiler also provides Python bindings and a diagnostic system. An open-source LALR lexer and parser built using Bison and Flex generates an Abstract Syntax Tree that is translated to a high-level MLIR dialect. An extensible hierarchical target system for modeling the heterogeneous nature of control systems at compilation time is included. Target-based and generic compilation passes are added using a pipeline interface to translate the input down to low-level intermediate representations (including LLVM IR) and can take advantage of LLVM backends and tooling to generate machine executable binaries. The qe-compiler is built to be extensible, maintainable, performant, and scalable to support the future of quantum computing.",
      "paperUrl": "https://doi.org/10.1109/qce60285.2024.00106",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "IR",
        "MLIR",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Kit Barton"
      ]
    },
    {
      "id": "openalex-w4402346403",
      "source": "openalex-discovery",
      "title": "Compressing Structured Tensor Algebra",
      "authors": [
        {
          "name": "Mahdi Ghorbani",
          "affiliation": ""
        },
        {
          "name": "Emilien Bauer",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        },
        {
          "name": "Amir Shaikhha",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Tensor algebra is a crucial component for data-intensive workloads such as machine learning and scientific computing. As the complexity of data grows, scientists often encounter a dilemma between the highly specialized dense tensor algebra and efficient structure-aware algorithms provided by sparse tensor algebra. In this paper, we introduce DASTAC, a framework to propagate the tensors's captured high-level structure down to low-level code generation by incorporating techniques such as automatic data layout compression, polyhedral analysis, and affine code generation. Our methodology reduces memory footprint by automatically detecting the best data layout, heavily benefits from polyhedral optimizations, leverages further optimizations, and enables parallelization through MLIR. Through extensive experimentation, we show that DASTAC achieves 1 to 2 orders of magnitude speedup over TACO, a state-of-the-art sparse tensor compiler, and StructTensor, a state-of-the-art structured tensor algebra compiler, with a significantly lower memory footprint.",
      "paperUrl": "https://arxiv.org/pdf/2407.13726",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2407.13726",
      "tags": [
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4406164024",
      "source": "openalex-discovery",
      "title": "Compiler-Aided Correctness Checking of CUDA-Aware MPI Applications",
      "authors": [
        {
          "name": "Alexander Hück",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Tim Ziegler",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Simon Schwitanski",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Joachim Jenke",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Christian Bischof",
          "affiliation": "Technical University of Darmstadt"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Hybrid MPI + X models, combining the Message Passing Interface (MPI) with node-level parallel programming models, increase complexity and introduce additional correctness issues. This work addresses the challenges of detecting data races in hybrid CUDA-aware MPI applications due to the asynchronous and non-blocking nature of CUDA and MPI APIs. We introduce CuSan, an LLVM compiler extension, and runtime that tracks CUDA-specific concurrency, synchronization, and memory access semantics. We integrate CuSan with MUST, a dynamic MPI correctness tool, and ThreadSanitizer (TSan), a thread-level data race detector. MUST with TSan can already detect concurrency issues for multi-threaded MPI codes. Together with CuSan, these tools allow for comprehensive correctness checking of concurrency issues in CUDA-aware MPI applications. Our evaluation of two mini-apps reveals runtime overhead of CuSan ranging from 6× to 36×, depending on the amount of memory tracked by TSan, compared to the uninstrumented version. Memory overhead consistently remains under 1.8×. CuSan is available at https://github.com/tudasc/cusan.",
      "paperUrl": "https://doi.org/10.1109/scw63240.2024.00032",
      "sourceUrl": "",
      "tags": [
        "CUDA"
      ],
      "matchedAuthors": [
        "Alexander Hück"
      ]
    },
    {
      "id": "openalex-w4393177849",
      "source": "openalex-discovery",
      "title": "Compiler generated feedback for Large Language Models",
      "authors": [
        {
          "name": "Dejan Grubisic",
          "affiliation": ""
        },
        {
          "name": "Chris Cummins",
          "affiliation": ""
        },
        {
          "name": "Volker Seeker",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We introduce a novel paradigm in compiler optimization powered by Large Language Models with compiler feedback to optimize the code size of LLVM assembly. The model takes unoptimized LLVM IR as input and produces optimized IR, the best optimization passes, and instruction counts of both unoptimized and optimized IRs. Then we compile the input with generated optimization passes and evaluate if the predicted instruction count is correct, generated IR is compilable, and corresponds to compiled code. We provide this feedback back to LLM and give it another chance to optimize code. This approach adds an extra 0.53% improvement over -Oz to the original model. Even though, adding more information with feedback seems intuitive, simple sampling techniques achieve much higher performance given 10 or more samples.",
      "paperUrl": "https://arxiv.org/pdf/2403.14714",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2403.14714",
      "tags": [
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w4392265913",
      "source": "openalex-discovery",
      "title": "Compile-Time Analysis of Compiler Frameworks for Query Compilation",
      "authors": [
        {
          "name": "Alexis Engelke",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Tobias Schwarz",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2024",
      "venue": "Vol. 30 (Issue None)",
      "type": "research-paper",
      "abstract": "Low compilation times are highly important in contexts of Just-in-time compilation. This not only applies to language runtimes for Java, WebAssembly, or JavaScript, but is also crucial for database systems that employ query compilation as the primary measure for achieving high throughput in combination with low query execution time. We present a performance comparison and detailed analysis of the compile times of the JIT compilation back-ends provided by GCC, LLVM, Cranelift, and a single-pass compiler in the context of database queries. Our results show that LLVM achieves the highest execution performance, but can compile substantially faster when tuning for low compilation time. Cranelift achieves a similar run-time performance to unoptimized LLVM, but compiles just 20–35% faster and is outperformed by the single-pass compiler, which compiles code 16x faster than Cranelift at similar execution performance.",
      "paperUrl": "http://dx.doi.org/10.1109/cgo57630.2024.10444856",
      "sourceUrl": "https://doi.org/10.1109/cgo57630.2024.10444856",
      "tags": [
        "JIT",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexis Engelke",
        "Tobias Schwarz"
      ]
    },
    {
      "id": "openalex-w1693750927",
      "source": "openalex-discovery",
      "title": "Clang and Coccinelle: Synergising program analysis tools for CERT C Secure Coding Standard certification",
      "authors": [
        {
          "name": "Mads Chr. Olesen",
          "affiliation": "Aalborg University"
        },
        {
          "name": "René Rydhof Hansen",
          "affiliation": "Aalborg University"
        },
        {
          "name": "Julia Lawall",
          "affiliation": "University of Copenhagen"
        },
        {
          "name": "Nicolas Palix",
          "affiliation": "University of Copenhagen"
        }
      ],
      "year": "2024",
      "venue": "Technische Universität Berlin – Universitätsbibliothek | Vol. 33 (Issue None)",
      "type": "research-paper",
      "abstract": "Writing correct C programs is well-known to be hard, not least due to the many language features intrinsic to C. Writing secure C programs is even harder and, at times, seemingly impossible. To improve on this situation the US CERT has developed and published a set of coding standards, the “CERT C Secure Coding Standard”, that (in the current version) enumerates 118 rules and 182 recommendations with the aim of making C programs (more) secure. The large number of rules and recommendations makes automated tool support essential for certifying that a given system is in compliance with the standard. In this paper we report on ongoing work on integrating two state of the art analysis tools, Clang and Coccinelle, into a combined tool well suited for analysing and certifying C programs according to, e.g., the CERT C Secure Coding standard or the MISRA (the Motor Industry Software Reliability Assocation) C standard. We further argue that such a tool must be highly adaptable and customisable to each software project as well as to the certification rules required by a given standard. Clang is the C frontend for the LLVM compiler/virtual machine project which includes a comprehensive set of static analyses and code checkers. Coccinelle is a program transformation tool and bug-finder developed originally for the Linux kernel, but has been successfully used to find bugs in other Open Source projects such as WINE and OpenSSL.",
      "paperUrl": "https://doi.org/10.14279/tuj.eceasst.33.455",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Frontend"
      ],
      "matchedAuthors": [
        "Julia Lawall"
      ]
    },
    {
      "id": "openalex-w4399659505",
      "source": "openalex-discovery",
      "title": "Clacc: OpenACC for C/C++ in Clang",
      "authors": [
        {
          "name": "Joel Denny",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Seyong Lee",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Marc González",
          "affiliation": "Universitat Politècnica de Catalunya"
        },
        {
          "name": "Keita Teranishi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2024",
      "venue": "The International Journal of High Performance Computing Applications | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Clacc project has developed OpenACC compiler, runtime, and profiling interface support for C/C++ by extending Clang and LLVM. A key Clacc design feature is that it translates OpenACC to OpenMP to leverage the OpenMP offloading support that is actively being developed for Clang and LLVM. A benefit of this design is support for two compilation modes: traditional compilation mode produces a binary, and source-to-source mode produces OpenMP source. Clacc has been deployed on Oak Ridge National Laboratory’s (ORNL’s) Frontier, on which Clacc is the only OpenACC implementation for C/C++. Clacc supports x86_64, POWER9, AMD GPUs, and NVIDIA GPUs. Clacc’s OpenACC profiling interface support has been integrated with TAU, which is also deployed on Frontier. While Clacc has always supported C as a base language, Clacc also has increasing C++ support, including support for Kokkos’s OpenACC back end. Clacc itself is hosted publicly on GitHub. In this paper, we describe Clacc’s design and mapping from OpenACC directives to OpenMP. We also present a performance evaluation on ORNL’s Frontier (AMD MI250x GPU offload) and Argonne National Laboratory’s (ANL’s) Polaris (NVIDIA A100 GPU offload) for various SPEC ACCEL and Kokkos OpenACC back end benchmarks.",
      "paperUrl": "https://www.osti.gov/biblio/2438826",
      "sourceUrl": "https://doi.org/10.1177/10943420241261976",
      "tags": [
        "C++",
        "Clang",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4400459006",
      "source": "openalex-discovery",
      "title": "Catalyst: a Python JIT compiler for auto-differentiablehybrid quantum programs",
      "authors": [
        {
          "name": "David Ittah",
          "affiliation": ""
        },
        {
          "name": "Ali Asadi",
          "affiliation": ""
        },
        {
          "name": "Erick Ochoa Lopez",
          "affiliation": ""
        },
        {
          "name": "Sergei Mironov",
          "affiliation": ""
        },
        {
          "name": "Samuel Banning",
          "affiliation": ""
        },
        {
          "name": "Romain Moyard",
          "affiliation": ""
        },
        {
          "name": "Mai Jacob Peng",
          "affiliation": ""
        },
        {
          "name": "Josh Izaac",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "The Journal of Open Source Software | Vol. 9 (Issue 99)",
      "type": "research-paper",
      "abstract": "Catalyst is a software package for capturing Python-based hybrid quantum programs (that is, programs that contain both quantum and classical instructions), and just-in-time (JIT) compiling them down to an MLIR and LLVM representation and generating binary code.As a result, Catalyst enables the ability to rapidly prototype quantum algorithms in Python alongside efficient compilation, optimization, and execution of the program on classical and quantum accelerators.In addition, Catalyst allows for advanced quantum programming features essential for fault-tolerant hardware support and advanced algorithm design, such as mid-circuit measurement with arbitrary post-processing, support for classical control flow in and around quantum algorithms, built-in measurement statistics, and hardware-compatible automatic differentiation (AD).",
      "paperUrl": "https://joss.theoj.org/papers/10.21105/joss.06720.pdf",
      "sourceUrl": "https://doi.org/10.21105/joss.06720",
      "tags": [
        "JIT",
        "MLIR"
      ],
      "matchedAuthors": [
        "Mai Jacob Peng"
      ]
    },
    {
      "id": "openalex-w4405426061",
      "source": "openalex-discovery",
      "title": "Cage: Hardware-Accelerated Safe WebAssembly",
      "authors": [
        {
          "name": "Martin Fink",
          "affiliation": ""
        },
        {
          "name": "Dimitrios Stavrakakis",
          "affiliation": ""
        },
        {
          "name": "Dennis Sprokholt",
          "affiliation": ""
        },
        {
          "name": "Soham Chakraborty",
          "affiliation": ""
        },
        {
          "name": "Jan-Erik Ekberg",
          "affiliation": ""
        },
        {
          "name": "Pramod Bhatotia",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "WebAssembly (WASM) is an immensely versatile and increasingly popular compilation target. It executes applications written in several languages (e.g., C/C++) with near-native performance in various domains (e.g., mobile, edge, cloud). Despite WASM's sandboxing feature, which isolates applications from other instances and the host platform, WASM does not inherently provide any memory safety guarantees for applications written in low-level, unsafe languages. To this end, we propose Cage, a hardware-accelerated toolchain for WASM that supports unmodified applications compiled to WASM and utilizes diverse Arm hardware features aiming to enrich the memory safety properties of WASM. Precisely, Cage leverages Arm's Memory Tagging Extension (MTE) to (i) provide spatial and temporal memory safety for heap and stack allocations and (ii) improve the performance of WASM's sandboxing mechanism. Cage further employs Arm's Pointer Authentication (PAC) to prevent leaked pointers from being reused by other WASM instances, thus enhancing WASM's security properties. We implement our system based on 64-bit WASM. We provide a WASM compiler and runtime with support for Arm's MTE and PAC. On top of that, Cage's LLVM-based compiler toolchain transforms unmodified applications to provide spatial and temporal memory safety for stack and heap allocations and prevent function pointer reuse. Our evaluation on real hardware shows that Cage incurs minimal runtime (&lt;5.8%) and memory (&lt;3.7%) overheads and can improve the performance of WASM's sandboxing mechanism, achieving a speedup of over 5.1%, while offering efficient memory safety guarantees.",
      "paperUrl": "https://arxiv.org/pdf/2408.11456",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2408.11456",
      "tags": [
        "C++",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Soham Chakraborty"
      ]
    },
    {
      "id": "openalex-w4405418928",
      "source": "openalex-discovery",
      "title": "CI/CD Efforts for Validation, Verification and Benchmarking OpenMP Implementations",
      "authors": [
        {
          "name": "Aaron Jarmusch",
          "affiliation": ""
        },
        {
          "name": "Felipe Cabarcas",
          "affiliation": ""
        },
        {
          "name": "Swaroop Pophale",
          "affiliation": ""
        },
        {
          "name": "Andrew Kallai",
          "affiliation": ""
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": ""
        },
        {
          "name": "Luke Peyralans",
          "affiliation": ""
        },
        {
          "name": "Seyong Lee",
          "affiliation": ""
        },
        {
          "name": "Joel Denny",
          "affiliation": ""
        },
        {
          "name": "Sunita Chandrasekaran",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Software developers must adapt to keep up with the changing capabilities of platforms so that they can utilize the power of High- Performance Computers (HPC), including exascale systems. OpenMP, a directive-based parallel programming model, allows developers to include directives to existing C, C++, or Fortran code to allow node level parallelism without compromising performance. This paper describes our CI/CD efforts to provide easy evaluation of the support of OpenMP across different compilers using existing testsuites and benchmark suites on HPC platforms. Our main contributions include (1) the set of a Continuous Integration (CI) and Continuous Development (CD) workflow that captures bugs and provides faster feedback to compiler developers, (2) an evaluation of OpenMP (offloading) implementations supported by AMD, HPE, GNU, LLVM, and Intel, and (3) evaluation of the quality of compilers across different heterogeneous HPC platforms. With the comprehensive testing through the CI/CD workflow, we aim to provide a comprehensive understanding of the current state of OpenMP (offloading) support in different compilers and heterogeneous platforms consisting of CPUs and GPUs from NVIDIA, AMD, and Intel.",
      "paperUrl": "https://arxiv.org/pdf/2408.11777",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2408.11777",
      "tags": [
        "C++",
        "Performance",
        "Testing"
      ],
      "matchedAuthors": [
        "Andrew Kallai",
        "Johannes Doerfert"
      ]
    },
    {
      "id": "openalex-w4405181102",
      "source": "openalex-discovery",
      "title": "Boosting Practical Control-Flow Integrity with Complete Field Sensitivity and Origin Awareness",
      "authors": [
        {
          "name": "Hao Xiang",
          "affiliation": "Xidian University"
        },
        {
          "name": "Zhuoyu Cheng",
          "affiliation": "Xidian University"
        },
        {
          "name": "Jinku Li",
          "affiliation": "Xidian University"
        },
        {
          "name": "Jianfeng Ma",
          "affiliation": "Xidian University"
        },
        {
          "name": "Kangjie Lu",
          "affiliation": "Twin Cities Orthopedics"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Control-flow integrity (CFI) is a strong and efficient defense mechanism against memory-corruption attacks.The practical versions of CFI, which have been integrated into compilers, employ static analysis to collect all possibly valid target functions of indirect calls.They are however less effective because the static analysis is imprecise.While more precise CFI techniques have been proposed, such as dynamic CFI, they are not yet practical due to issues on performance, compatibility, and deployability.We believe that to be practical, CFI based on static analysis is still the promising direction.However, these years have not seen much progress on the effectiveness of such practical CFI.This paper aims to boost the effectiveness of practical CFI by dramatically optimizing the target-function sets (aka equivalence class or EC) of indirect calls.We first identify two fundamental limitations that lead to the imprecision of static indirect-call analysis: incomplete field sensitivity due to variable field indexes and the unawareness of the origins of point-to targets.We then propose two novel analysis techniques, complete field sensitivity and origin awareness, which handle variable field indexes and distinguish target origins.The techniques dramatically reduce the size of target functions.To enforce the origin awareness, we further employ Intel Memory Protection Keys to safely store the origin information.We implement our techniques as a system called ECCut.The evaluation results show that compared to the mainline LLVM CFI, ECCut achieves a substantial reduction of 94.8% and 90.3% in the average and the largest EC sizes.While compared to the state-of-the-art origin-aware CFI (i.e., OS-CFI), ECCut reduces the average and the largest EC sizes by 90.2% and 89.3% respectively.Additionally,",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3658644.3670308",
      "sourceUrl": "https://doi.org/10.1145/3658644.3670308",
      "tags": [
        "Performance",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Jinku Li"
      ]
    },
    {
      "id": "openalex-w4399850836",
      "source": "openalex-discovery",
      "title": "Boosting Compiler Testing by Injecting Real-World Code",
      "authors": [
        {
          "name": "Shaohua Li",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Theodoros Theodoridis",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Zhendong Su",
          "affiliation": "ETH Zurich"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "We introduce a novel approach for testing optimizing compilers with code from real-world applications The main idea is to construct well-formed programs by fusing multiple code snippets from various realworld projects. The key insight is backed by the fact that the large volume of real-world code exercises rich syntactical and semantic language features, which current engineering-intensive approaches like random program generators are hard to fully support. To construct well-formed programs from real-world code our approach works by (1) extracting real-world code at the granularity of function, (2) injecting function calls into seed programs, and (3) leveraging dynamic execution information to maintain the semantics and build complex data dependencies between injected functions and the seed program. With this idea, our approach complements the existing generators by boosting their expressiveness via fusing real-world code in a semantics-preserving way. We implement our idea in a tool, Creal, to test C compilers. In a nine-month testing period, we have reported 132 bugs to GCC and LLVM, two of the most popular and well-tested C compilers. At the time of writing, 121 of them have been confirmed as unknown bugs, and 101 of them have been fixed. Most of these bugs were miscompilations, and many were recognized as long-latent and critical. Our evaluation results evidently demonstrate the significant advantage of using real-world code to stress-test compilers. We believe this idea will benefit the general compiler testing direction and will be directly applicable to other compilers.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3656386",
      "sourceUrl": "https://doi.org/10.1145/3656386",
      "tags": [
        "Testing"
      ],
      "matchedAuthors": [
        "Theodoros Theodoridis"
      ]
    },
    {
      "id": "openalex-w4404515043",
      "source": "openalex-discovery",
      "title": "Binsweep: Reliably Restricting Untrusted Instruction Streams with Static Binary Analysis and Control-Flow Integrity",
      "authors": [
        {
          "name": "Matteo Oldani",
          "affiliation": "Oracle (United States)"
        },
        {
          "name": "W Blair",
          "affiliation": "Oracle (United States)"
        },
        {
          "name": "Lukas Stadler",
          "affiliation": "Oracle (United States)"
        },
        {
          "name": "Zbyněk lajchrt",
          "affiliation": "Oracle (United States)"
        },
        {
          "name": "Matthias Neugschwandtner",
          "affiliation": "Oracle (United States)"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Restricting an application's instruction stream is necessary to ensure the absence of certain functionality, which in turn is a requirement for lightweight sandboxing of untrusted code in cloud environments. Doing so at the lowest possible level, (i.e., machine code), is safest as it does not assume trusted or bug-free build toolchains. However, resolving indirect branches and instruction set architectures (ISA) with variable-length instructions are a challenge for reliable and exhaustive machine code analysis. In this paper, we present Binsweep, a system that ensures complete analysis of variable-length ISA applications in machine code. The key enabling concept is a restricted form of Control Flow Integrity (CFI) that Binsweep enforces, called BinsweepCFI. We implement BinsweepCFI as a compiler pass within the LLVM toolchain. Our evaluation over SPECint benchmarks in SPEC CPU 2017, and widely used binary programs, including the NGINX webserver, Micronaut service, and Python interpreters, demonstrates that Binsweep can verify real world programs, and BinsweepCFI can protect programs with manageable (6.55% in the worst case) performance overhead. Furthermore, we show Binsweep can verify these programs' CFGs much faster than a state of the art binary analysis tool, angr, can recover CFGs. These results demonstrate Binsweep can efficiently support admitting untrusted code buffers, hundreds of megabytes in size, to cloud sandboxes.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3689938.3694778",
      "sourceUrl": "https://doi.org/10.1145/3689938.3694778",
      "tags": [
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Matthias Neugschwandtner"
      ]
    },
    {
      "id": "openalex-w4392265901",
      "source": "openalex-discovery",
      "title": "BEC: Bit-Level Static Analysis for Reliability against Soft Errors",
      "authors": [
        {
          "name": "Yousun Ko",
          "affiliation": "Yonsei University"
        },
        {
          "name": "Bernd Burgstaller",
          "affiliation": "Yonsei University"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Soft errors are a type of transient digital signal corruption that occurs in digital hardware components such as the internal flip-flops of CPU pipelines, the register file, memory cells, and even internal communication buses. Soft errors are caused by environmental radioactivity, magnetic interference, lasers, and temperature fluctuations, either unintentionally, or as part of a deliberate attempt to compromise a system and expose confidential data. We propose a bit-level error coalescing (BEC) static program analysis and its two use cases to understand and improve program reliability against soft errors. The BEC analysis tracks each bit corruption in the register file and classifies the effect of the corruption by its semantics at compile time. The usefulness of the proposed analysis is demonstrated in two scenarios, fault injection campaign pruning, and reliability-aware program transformation. Experimental results show that bit-level analysis pruned up to 30.04 % of exhaustive fault injection campaigns (13.71 % on average), without loss of accuracy. Program vulnerability was reduced by up to 13.11 % (4.94 % on average) through bit-level vulnerability-aware instruction scheduling. The analysis has been implemented within LLVM and evaluated on the RISC-V architecture. To the best of our knowledge, the proposed BEC analysis is the first bit-level compiler analysis for program reliability against soft errors. The proposed method is generic and not limited to a specific computer architecture.",
      "paperUrl": "https://doi.org/10.1109/cgo57630.2024.10444844",
      "sourceUrl": "",
      "tags": [
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Bernd Burgstaller",
        "Yousun Ko"
      ]
    },
    {
      "id": "openalex-w4402705716",
      "source": "openalex-discovery",
      "title": "AutoPatch: Automated Generation of Hotpatches for Real-Time Embedded Devices",
      "authors": [
        {
          "name": "Mohsen Salehi",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Karthik Pattabiraman",
          "affiliation": "University of British Columbia"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Real-time embedded devices like medical or industrial devices are increasingly targeted by cyber-attacks. Prompt patching is crucial to mitigate the serious consequences of such attacks on these devices. Hotpatching is an approach to apply a patch to mission-critical embedded devices without rebooting them. However, existing hotpatching approaches require developers to manually write the hotpatch for target systems, which is time-consuming and error-prone. To address these issues, we propose AutoPatch, a new hotpatching technique that automatically generates functionally equivalent hotpatches via static analysis of the official patches. AutoPatch introduces a new software triggering approach that supports diverse embedded devices, and preserves the functionality of the official patch. In contrast to prior work, AutoPatch does not rely on hardware support for triggering patches, or on executing patches in specialized virtual machines. We implemented AutoPatch using the LLVM compiler, and evaluated its efficiency, effectiveness and generality using 62 real CVEs on four embedded devices with different specifications and architectures running popular RTOSes. We found that AutoPatch can fix more than 90% of CVEs, and resolve the vulnerability successfully. The results revealed an average total delay of less than 12.7 $\\mu s$ for fixing the vulnerabilities, representing a performance improvement of 50% over RapidPatch, a state-of-the-art approach. Further, our memory overhead, on average, was slightly lower than theirs (23%). Finally, AutoPatch was able to generate hotpatches for all four devices without any modifications.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3658644.3690255",
      "sourceUrl": "https://doi.org/10.1145/3658644.3690255",
      "tags": [
        "Embedded",
        "Performance",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w4406261842",
      "source": "openalex-discovery",
      "title": "Achieving Pareto-Optimality in Quantum Circuit Compilation via a Multi-Objective Heuristic Optimization Approach",
      "authors": [
        {
          "name": "Aleksandra Świerkowska",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Jorge Echavarria",
          "affiliation": "Bavarian Academy of Sciences and Humanities"
        },
        {
          "name": "Laura Schulz",
          "affiliation": "Leibniz Supercomputing Centre"
        },
        {
          "name": "Martin Schulz",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High Performance Computing-Quantum Computing (HPCQC) integration presents a promising yet challenging opportunity, particularly in the area of quantum circuit compilation and optimization, requiring further advancements in the field of Quantum Computing (QC). To address this, we introduce the Munich Quantum Compiler, a key component of the Munich Quantum Software Stack (MQSS). This compiler employs a heuristic-based approach to select a Pareto-optimal subset of optimizations in the form of LLVM passes for quantum circuits described in an LLVM-compliant Intermediate Representation (IR).",
      "paperUrl": "https://doi.org/10.1109/qce60285.2024.10297",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations",
        "Performance",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Martin Schulz"
      ]
    },
    {
      "id": "openalex-w4400798869",
      "source": "openalex-discovery",
      "title": "Accelerating Static Null Pointer Dereference Detection with Parallel Computing",
      "authors": [
        {
          "name": "Rulin Xu",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Luohui Chen",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Ruyi Zhang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Yuanliang Zhang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Wei Xiao",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Haifang Zhou",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Xiaoguang Mao",
          "affiliation": "National University of Defense Technology"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-precision static analysis can effectively detect Null Pointer Dereference (NPD) vulnerabilities in C language, but the performance overhead is significant. In recent years, researchers have attempted to enhance the efficiency of static analysis by leveraging multicore resources. However, due to complex dependencies in the analysis process, the parallelization of static value-flow NPD analysis for large-scale software still faces significant challenges. It is difficult to achieve a good balance between detection efficiency and accuracy, which impacts its application.This paper presents PANDA, the first parallel detector for high-precision static value-flow NPD analyzer in the C language. The core idea of PANDA is to utilize dependency analysis to ensure high precision while decoupling the strong dependencies between static value-flow analysis steps. This transforms the traditionally challenging-to-parallelize NPD analysis into two parallelizable algorithms: function summarization and combined query-based vulnerability analysis. PANDA introduces a task-level parallel framework and enhances it with a dynamic scheduling method to parallel schedule the above two key steps, significantly improving the performance and scalability of memory vulnerability detection.Fully implemented within the LLVM framework (version 15.0.7), PANDA demonstrates a significant advantage in balancing accuracy and efficiency compared to current popular open-source detection tools. In precision-targeted benchmark tests, PANDA maintains a false positive rate within 3.17% and a false negative rate within 5.16%; in historical CVE detection rate tests, its recall rate far exceeds that of comparative open-source tools. In performance evaluations, compared to its serial version, PANDA achieves up to an 11.23-fold speedup on a 16-node server, exhibiting outstanding scalability.",
      "paperUrl": "https://doi.org/10.1145/3671016.3671385",
      "sourceUrl": "",
      "tags": [
        "Performance",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Wei Xiao"
      ]
    },
    {
      "id": "openalex-w4392265932",
      "source": "openalex-discovery",
      "title": "AXI4MLIR: User-Driven Automatic Host Code Generation for Custom AXI-Based Accelerators",
      "authors": [
        {
          "name": "Nícolas Bohm Agostini",
          "affiliation": "Northeastern University"
        },
        {
          "name": "Jude Haris",
          "affiliation": "University of Glasgow"
        },
        {
          "name": "Perry Gibson",
          "affiliation": "University of Glasgow"
        },
        {
          "name": "Malith Jayaweera",
          "affiliation": "Northeastern University"
        },
        {
          "name": "Norm Rubin",
          "affiliation": "Northeastern University"
        },
        {
          "name": "Antonino Tumeo",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "José Luis Abellán",
          "affiliation": "Universidad de Murcia"
        },
        {
          "name": "José Cano",
          "affiliation": "University of Glasgow"
        },
        {
          "name": "David Kaeli",
          "affiliation": "Northeastern University"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper addresses the need for automatic and efficient generation of host driver code for arbitrary custom AXI-based accelerators targeting linear algebra algorithms, an important workload in various applications, including machine learning and scientific computing. While existing tools have focused on automating accelerator prototyping, little attention has been paid to the host-accelerator interaction. This paper introduces AXI4MLIR, an extension of the MLIR compiler framework designed to facilitate the automated generation of host-accelerator driver code. With new MLIR attributes and transformations, AXI4MLIR empowers users to specify accelerator features (including their instructions) and communication patterns and exploit the host memory hierarchy. We demonstrate AXI4MLIR's versatility across different types of accelerators and problems, showcasing significant CPU cache reference reductions (up to 56%) and up to a 1.65× speedup compared to manually optimized driver code implementations. AXI4MLIR implementation is open-source and available at: https:/7github.com/AXI4MLIR/axi4mlir.",
      "paperUrl": "https://eprints.gla.ac.uk/311284/2/311284.pdf",
      "sourceUrl": "https://doi.org/10.1109/cgo57630.2024.10444801",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "David Kaeli"
      ]
    },
    {
      "id": "openalex-w4395106400",
      "source": "openalex-discovery",
      "title": "A shared compilation stack for distributed-memory parallelism in stencil DSLs",
      "authors": [
        {
          "name": "George Bisbas",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Anton Lydike",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Emilien Bauer",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Nick Brown",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Mathieu Fehr",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Lawrence Mitchell",
          "affiliation": ""
        },
        {
          "name": "Gabriel Rodriguez‐Canal",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Maurice Jamieson",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Paul H. J. Kelly",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Michel Steuwer",
          "affiliation": "Technische Universität Berlin"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Domain Specific Languages (DSLs) increase programmer productivity and provide high performance. Their targeted abstractions allow scientists to express problems at a high level, providing rich details that optimizing compilers can exploit to target current- and next-generation supercomputers. The convenience and performance of DSLs come with significant development and maintenance costs. The siloed design of DSL compilers and the resulting inability to benefit from shared infrastructure cause uncertainties around longevity and the adoption of DSLs at scale. By tailoring the broadly-adopted MLIR compiler framework to HPC, we bring the same synergies that the machine learning community already exploits across their DSLs (e.g. Tensorflow, PyTorch) to the finite-difference stencil HPC community. We introduce new HPC-specific abstractions for message passing targeting distributed stencil computations. We demonstrate the sharing of common components across three distinct HPC stencil-DSL compilers: Devito, PSyclone, and the Open Earth Compiler, showing that our framework generates high-performance executables based upon a shared compiler ecosystem.",
      "paperUrl": "https://doi.org/10.1145/3620666.3651344",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Lydike",
        "Mathieu Fehr",
        "Michel Steuwer",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4395686619",
      "source": "openalex-discovery",
      "title": "A Two-Phase Infinite/Finite Low-Level Memory Model: Reconciling Integer–Pointer Casts, Finite Space, and undef at the LLVM IR Level of Abstraction",
      "authors": [
        {
          "name": "Calvin Beck",
          "affiliation": "University of Pennsylvania"
        },
        {
          "name": "Irene Yoon",
          "affiliation": "Institut national de recherche en sciences et technologies du numérique"
        },
        {
          "name": "Hanxi Chen",
          "affiliation": "University of Pennsylvania"
        },
        {
          "name": "Yannick Zakowski",
          "affiliation": "Université Claude Bernard Lyon 1"
        },
        {
          "name": "Steve Zdancewic",
          "affiliation": "University of Pennsylvania"
        }
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 8 (Issue ICFP)",
      "type": "research-paper",
      "abstract": "This paper provides a novel approach to reconciling complex low-level memory model features, such as pointerinteger casts, with desired refinements that are needed to justify the correctness of program transformations. The idea is to use a “two-phase” memory model, one with an unbounded memory and corresponding unbounded integer type, and one with a finite memory; the connection between the two levels is made explicit by a notion of refinement that handles out-of-memory behaviors. This approach allows for more optimizations to be performed and establishes a clear boundary between the idealized semantics of a program and the implementation of that program on finite hardware. The two-phase memory model has been incorporated into an LLVM IR semantics, demonstrating its utility in practice in the context of a low-level language with features like undef and bitcast. This yields infinite and finite memory versions of the language semantics that are proven to be in refinement with respect to out-of-memory behaviors. Each semantics is accompanied by a verified executable reference interpreter. The semantics justify optimizations, such as dead-alloca-elimination, that were previously impossible or difficult to prove correct.",
      "paperUrl": "https://doi.org/10.1145/3674652",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Steve Zdancewic"
      ]
    },
    {
      "id": "openalex-w4404954474",
      "source": "openalex-discovery",
      "title": "A Framework for Fine-Grained Program Versioning",
      "authors": [
        {
          "name": "Yishen Chen",
          "affiliation": ""
        },
        {
          "name": "Saman Amarasinghe",
          "affiliation": ""
        }
      ],
      "year": "2024",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Static dependence analysis is critical for optimizations such as vectorization and loop-invariant code motion. However, traditional static dependence analysis is often imprecise, making these optimizations less effective. To address this issue, production compilers use loop versioning to rule out some categories of memory dependencies at run time. However, loop versioning is loop-centric and usually tied to specific optimizations (e.g., loop vectorization), making it less effective for non-loop optimizations such as superword-level parallelism (SLP) vectorization. In this paper, we propose a fine-grained versioning framework to rule out program dependencies at run time. Our framework is general and not tailored to any specific optimizations. To use our system, a client optimization specifies groups of instructions (or loops) whose independence is desired but unprovable statically. In response, our system duplicates the appropriate instructions and guards the original ones with run-time checks to guarantee their independence; if the checks fail, the duplicated instructions execute instead. In a case study, we extended an existing SLP vectorizer with minimal modifications using our framework, resulting in a 1.17× speedup over Clang's vectorizers on TSVC and a 1.51× speedup on PolyBench. In both benchmarks, we encountered programs that could not be vectorized with loop versioning alone. In a second case study, we used our framework to implement a more aggressive variant of redundant load elimination than the one implemented by Clang. Our redundant load elimination results in a 1.012× speedup on the SPEC 2017 Floating Point benchmarks, with the maximum speedup being 1.064×.",
      "paperUrl": "https://doi.org/10.1109/micro61859.2024.00024",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Saman Amarasinghe",
        "Yishen Chen"
      ]
    },
    {
      "id": "openalex-w4390280319",
      "source": "openalex-discovery",
      "title": "mlirSynth: Automatic, Retargetable Program Raising in Multi-Level IR Using Program Synthesis",
      "authors": [
        {
          "name": "Alexander Brauckmann",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Elizabeth Polgreen",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Michael O’Boyle",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "MLIR is an emerging compiler infrastructure for modern hardware, but existing programs cannot take advantage of MLIR’s high-performance compilation if they are described in lower-level general purpose languages. Consequently, to avoid programs needing to be rewritten manually, this has led to efforts to automatically raise lower-level to higher-level dialects in MLIR. However, current methods rely on manually-defined raising rules, which limit their applicability and make them challenging to maintain as MLIR dialects evolve. We present mlirSynth – a novel approach which translates programs from lower-level MLIR dialects to high-level ones without manually defined rules. Instead, it uses available dialect definitions to construct a program space and searches it effectively using type constraints and equivalences. We demonstrate its effectiveness by raising C programs to two distinct high-level MLIR dialects, which enables us to use existing high-level dialect specific compilation flows. On Polybench, we show a greater coverage than previous approaches, resulting in geomean speedups of 2.5x (Intel) and 3.4x (AMD) over state-of-the-art compilation flows. mlirSynth also enables retargetability to domain-specific accelerators, resulting in a geomean speedup of 21.6x on a TPU.",
      "paperUrl": "https://www.research.ed.ac.uk/en/publications/42828cce-e80c-4716-a07d-2fb333b86264",
      "sourceUrl": "https://doi.org/10.1109/pact58117.2023.00012",
      "tags": [
        "Infrastructure",
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexander Brauckmann",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4380480242",
      "source": "openalex-discovery",
      "title": "Tiling for DMA-Based Hardware Accelerators (WIP)",
      "authors": [
        {
          "name": "Alexandre Singer",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Kai-Ting Amy Wang",
          "affiliation": "Huawei Technologies (Canada)"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Many hardware accelerator architectures use DMA units to transfer memory which may be limited by the fixed-width size of the DMA transfer, and automatic loop tilers currently do not take the limitation of these DMA units into account. We present a compiler pass, implemented in MLIR, that uses polyhedral analysis on the memory access patterns in a loop nest and constrain the possible tile sizes based on the DMA chunk width. This allows the compiler to effectively tile loops for these architectures.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3589610.3596283",
      "sourceUrl": "https://doi.org/10.1145/3589610.3596283",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "Kai-Ting Amy Wang"
      ]
    },
    {
      "id": "openalex-w4386507603",
      "source": "openalex-discovery",
      "title": "Support of Sparse Tensor Computing for MLIR HLS",
      "authors": [
        {
          "name": "Geng-Ming Liang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chao-Lin Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Robert Lai",
          "affiliation": "MediaTek (Taiwan)"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Nowadays, sparse tensor computations are widely used in machine learning. Without the multiplications in zero values, sparse tensor computation can significantly reduce the latency and power consumption. Famous frameworks like TensorFlow, PyTorch, Pandas, etc., already have the support of sparse tensor computing. MLIR also integrated this idea and implemented the compilation flow. Integrating sparse tensor computing and MLIR into High-level Synthesis (HLS) can generate more powerful RTL and further implement specified hardware. However, MLIR flow isn't well done now while translating into LLVM IR and does not fully support HLS tools, which are not supporting MLIR. In this paper, we propose a flow in MLIR to lower sparse tensor computations into HLS-readable LLVM IR, which can then be synthesized into RTL. To demonstrate the effectiveness of our proposed flow, we devise experiments by implementing matrix multiplication operations in the convolution layers, and the data is pruned to maximize the sparsity. Our proposed flow speeds up the latency by about 3.6 times, as demonstrated in our experiment with Xilinx Vitis HLS.",
      "paperUrl": "https://doi.org/10.1145/3605731.3605908",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4385428196",
      "source": "openalex-discovery",
      "title": "Support of MISRA C++ Analyzer for Reliability of Embedded Systems",
      "authors": [
        {
          "name": "Che-Chia Lin",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Wei-Hsu Chu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chia‐Hsuan Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Hui-Hsin Liao",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chun‐Chieh Yang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yi‐Ping You",
          "affiliation": "National Yang Ming Chiao Tung University"
        },
        {
          "name": "Tien-Yuan Hsieh",
          "affiliation": "Industrial Technology Research Institute"
        }
      ],
      "year": "2023",
      "venue": "ACM Transactions on Cyber-Physical Systems | Vol. 9 (Issue 1)",
      "type": "research-paper",
      "abstract": "Cyber-Physical Systems (CPS) are increasingly used in many complex applications, such as autonomous delivery drones, the automotive CPS design, power grid control systems, and medical robotics. However, existing programming languages lack certain design patterns for CPS designs, including temporal semantics and concurrency models. Future research directions may involve programming language extensions to support CPS designs. However, JSF++, MISRA, and MISRA C++ are providing specifications intended to increase the reliability of safety-critical systems. This article also describes the development of rule checkers based on the MISRA C++ specification using the Clang open-source tool, which allows for the annotation of code and the easy extension of the MISRA C++ specification to other programming languages and systems. This is potentially useful for future CPS language research extensions to work with reliability software specifications using the Clang tool. Experiments were performed using key C++ benchmarks to validate our method in comparison with the well-known Coverity commercial tool. We illustrate key rules related to class, inheritance, template, overloading, and exception handling. Open-source benchmarks that violate the rules detected by our checkers are also illustrated. A random graph generator is further used to generate diamond case with multiple inheritance test data for our software validations. The experimental results demonstrate that our method can provide information that is more detailed than that obtained using Coverity for nine open-source C++ benchmarks. Since the Clang tool is widely used, it will further allow developers to annotate their own extensions.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3611390",
      "sourceUrl": "https://doi.org/10.1145/3611390",
      "tags": [
        "C++",
        "Clang",
        "Embedded",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4362675444",
      "source": "openalex-discovery",
      "title": "Streamline Ahead-of-Time SYCL CPU Device Implementation through Bypassing SPIR-V",
      "authors": [
        {
          "name": "Wenju He",
          "affiliation": ""
        },
        {
          "name": "Yilong Guo",
          "affiliation": ""
        },
        {
          "name": "Xinmin Tian",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Hideki Saito",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Wenwan Xing",
          "affiliation": ""
        },
        {
          "name": "Feng Zou",
          "affiliation": ""
        },
        {
          "name": "Chunyang Dai",
          "affiliation": ""
        },
        {
          "name": "Maosu Zhao",
          "affiliation": ""
        },
        {
          "name": "Haonan Yang",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "International Workshop on OpenCL | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Here we present the design and implementation of our LLVM-based Ahead-Of-Time (AOT) SYCL CPU device without using SPIR-V, known as non-SPIRV CPU device. Our design of non-SPIRV CPU device is intended to highlight a general SYCL CPU implementation that aims for both debuggability and performance. Contributions:",
      "paperUrl": "https://doi.org/10.1145/3585341.3585381",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Hideki Saito",
        "Xinmin Tian"
      ]
    },
    {
      "id": "openalex-w4387356023",
      "source": "openalex-discovery",
      "title": "Stencil-HMLS: A multi-layered approach to the automatic optimisation of stencil codes on FPGA",
      "authors": [
        {
          "name": "Gabriel Rodriguez‐Canal",
          "affiliation": ""
        },
        {
          "name": "Nick Brown",
          "affiliation": ""
        },
        {
          "name": "Maurice Jamieson",
          "affiliation": ""
        },
        {
          "name": "Emilien Bauer",
          "affiliation": ""
        },
        {
          "name": "Anton Lydike",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The challenges associated with effectively programming FPGAs have been a major blocker in popularising reconfigurable architectures for HPC workloads. However new compiler technologies, such as MLIR, are providing new capabilities which potentially deliver the ability to extract domain specific information and drive automatic structuring of codes for FPGAs. In this paper we explore domain specific optimisations for stencils, a fundamental access pattern in scientific computing, to obtain high performance on FPGAs via automated code structuring. We propose Stencil-HMLS, a multi-layered approach to automatic optimisation of stencil codes and introduce the HLS dialect, which brings FPGA programming into the MLIR ecosystem. Using the PSyclone Fortran DSL, we demonstrate an improvement of 14-100$\\times$ with respect to the next best performant state-of-the-art tool. Furthermore, our approach is 14 to 92 times more energy efficient than the next most energy efficient approach.",
      "paperUrl": "https://arxiv.org/pdf/2310.01914",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2310.01914",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Lydike",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4389162055",
      "source": "openalex-discovery",
      "title": "Speeding up SMT Solving via Compiler Optimization",
      "authors": [
        {
          "name": "Benjamin Mikek",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Qirun Zhang",
          "affiliation": "Georgia Institute of Technology"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "SMT solvers are fundamental tools for reasoning about constraints in practical problems like symbolic execution and program synthesis. Faster SMT solving can improve the performance and precision of those analysis tools. Existing approaches typically speed up SMT solving by developing new heuristics inside particular solvers, which requires nontrivial engineering efforts. This paper presents a new perspective on speeding up SMT solving. We propose SMT-LLVM Optimizing Translation (SLOT), a solver-agnostic pre-processing approach that utilizes existing compiler optimizations to simplify SMT problem instances. We implement SLOT for the two most application-critical SMT theories, bitvectors, and floating-point numbers. Our extensive evaluation based on the standard SMT-LIB benchmarks shows that SLOT can substantially increase the number of solvable SMT formulas given fixed timeouts and achieve mean speedups of nearly 3× for large benchmarks.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3611643.3616357",
      "sourceUrl": "https://doi.org/10.1145/3611643.3616357",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Benjamin Mikek"
      ]
    },
    {
      "id": "openalex-w4387917859",
      "source": "openalex-discovery",
      "title": "Simulating Operational Memory Models Using Off-the-Shelf Program Analysis Tools",
      "authors": [
        {
          "name": "Dan Iorga",
          "affiliation": "Imperial College London"
        },
        {
          "name": "John Wickerson",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Software Engineering | Vol. 49 (Issue 12)",
      "type": "research-paper",
      "abstract": "Memory models allow reasoning about the correctness of multithreaded programs. Constructing and using such models is facilitated by simulators that reveal which behaviours of a given program are allowed. While extensive work has been done on simulating axiomatic memory models, there has been less work on simulation of operational models. Operational models are often considered more intuitive than axiomatic models, but are challenging to simulate due to the vast number of paths through the model’s transition system. Observing that a similar path-explosion problem is tackled by program analysis tools, we investigate the idea of reducing the decision problem of “whether a given memory model allows a given behaviour” to the decision problem of “whether a given C program is safe”, which can be handled by a variety of off-the-shelf tools. We report on our experience using multiple program analysis tools for C for this purpose—a model checker (CBMC), a symbolic execution tool (KLEE), and three coverage-guided fuzzers (libFuzzer, Centipede and AFL++)—presenting two case-studies. First, we evaluate the performance and scalability of these tools in the context of the x86 memory model, showing that fuzzers offer performance competitive with that of RMEM, a state-of-the-art bespoke memory model simulator. Second, we study a more complex, recently developed memory model for hybrid CPU/FPGA devices for which no bespoke simulator is available. We highlight how different encoding strategies can aid the various tools and show how our approach allows us to simulate the CPU/FPGA model twice as deeply as in prior work, leading to us finding and fixing several infidelities in the model. We also experimented with applying three analysis tools that won the “falsification” category in the 2023 Annual Software Verification Competition (SV-COMP). We found that these tools do not scale to our use cases, motivating us to submit example C programs arising from our work for inclusion in the set of SV-COMP benchmarks, so that they can serve as challenge examples.",
      "paperUrl": "https://ieeexplore.ieee.org/ielx7/32/4359463/10295416.pdf",
      "sourceUrl": "https://doi.org/10.1109/tse.2023.3326056",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson"
      ]
    },
    {
      "id": "openalex-w4388685623",
      "source": "openalex-discovery",
      "title": "Sidekick compilation with xDSL",
      "authors": [
        {
          "name": "Mathieu Fehr",
          "affiliation": ""
        },
        {
          "name": "Michel Weber",
          "affiliation": ""
        },
        {
          "name": "Christian Ulmann",
          "affiliation": ""
        },
        {
          "name": "Alexandre Lopoukhine",
          "affiliation": ""
        },
        {
          "name": "Martin Paul Lücke",
          "affiliation": ""
        },
        {
          "name": "Théo Degioanni",
          "affiliation": ""
        },
        {
          "name": "Michel Steuwer",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Traditionally, compiler researchers either conduct experiments within an existing production compiler or develop their own prototype compiler; both options come with trade-offs. On one hand, prototyping in a production compiler can be cumbersome, as they are often optimized for program compilation speed at the expense of software simplicity and development speed. On the other hand, the transition from a prototype compiler to production requires significant engineering work. To bridge this gap, we introduce the concept of sidekick compiler frameworks, an approach that uses multiple frameworks that interoperate with each other by leveraging textual interchange formats and declarative descriptions of abstractions. Each such compiler framework is specialized for specific use cases, such as performance or prototyping. Abstractions are by design shared across frameworks, simplifying the transition from prototyping to production. We demonstrate this idea with xDSL, a sidekick for MLIR focused on prototyping and teaching. xDSL interoperates with MLIR through a shared textual IR and the exchange of IRs through an IR Definition Language. The benefits of sidekick compiler frameworks are evaluated by showing on three use cases how xDSL impacts their development: teaching, DSL compilation, and rewrite system prototyping. We also investigate the trade-offs that xDSL offers, and demonstrate how we simplify the transition between frameworks using the IRDL dialect. With sidekick compilation, we envision a future in which engineers minimize the cost of development by choosing a framework built for their immediate needs, and later transitioning to production with minimal overhead.",
      "paperUrl": "https://arxiv.org/pdf/2311.07422",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2311.07422",
      "tags": [
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Mathieu Fehr",
        "Michel Steuwer",
        "Théo Degioanni",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4327930432",
      "source": "openalex-discovery",
      "title": "SPLENDID: Supporting Parallel LLVM-IR Enhanced Natural Decompilation for Interactive Development",
      "authors": [
        {
          "name": "Zujun Tan",
          "affiliation": "Princeton University"
        },
        {
          "name": "Yebin Chon",
          "affiliation": "Princeton University"
        },
        {
          "name": "Michael Kruse",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Ziyang Xu",
          "affiliation": "Princeton University"
        },
        {
          "name": "Brian Homerding",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Simone Campanoni",
          "affiliation": "Northwestern University"
        },
        {
          "name": "David I. August",
          "affiliation": "Princeton University"
        }
      ],
      "year": "2023",
      "venue": "Vol. 34 (Issue None)",
      "type": "research-paper",
      "abstract": "Manually writing parallel programs is difficult and error-prone. Automatic parallelization could address this issue, but profitability can be limited by not having facts known only to the programmer. A parallelizing compiler that collaborates with the programmer can increase the coverage and performance of parallelization while reducing the errors and overhead associated with manual parallelization. Unlike collaboration involving analysis tools that report program properties or make parallelization suggestions to the programmer, decompiler-based collaboration could leverage the strength of existing parallelizing compilers to provide programmers with a natural compiler-parallelized starting point for further parallelization or refinement. Despite this potential, existing decompilers fail to do this because they do not generate portable parallel source code compatible with any compiler of the source language. This paper presents SPLENDID, an LLVM-IR to C/OpenMP decompiler that enables collaborative parallelization by producing standard parallel OpenMP code. Using published manual parallelization of the PolyBench benchmark suite as a reference, SPLENDID's collaborative approach produces programs twice as fast as either Polly-based automatic parallelization or manual parallelization alone. SPLENDID's portable parallel code is also more natural than that from existing decompilers, obtaining a 39x higher average BLEU score.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3582016.3582058",
      "sourceUrl": "https://doi.org/10.1145/3582016.3582058",
      "tags": [
        "IR",
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Brian Homerding",
        "David I. August",
        "Johannes Doerfert",
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w4385890147",
      "source": "openalex-discovery",
      "title": "SEER: Super-Optimization Explorer for HLS using E-graph Rewriting with MLIR",
      "authors": [
        {
          "name": "Jianyi Cheng",
          "affiliation": ""
        },
        {
          "name": "Samuel Coward",
          "affiliation": ""
        },
        {
          "name": "Lorenzo Chelini",
          "affiliation": ""
        },
        {
          "name": "Rafael Barbalho",
          "affiliation": ""
        },
        {
          "name": "Theo Drane",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-level synthesis (HLS) is a process that automatically translates a software program in a high-level language into a low-level hardware description. However, the hardware designs produced by HLS tools still suffer from a significant performance gap compared to manual implementations. This is because the input HLS programs must still be written using hardware design principles. Existing techniques either leave the program source unchanged or perform a fixed sequence of source transformation passes, potentially missing opportunities to find the optimal design. We propose a super-optimization approach for HLS that automatically rewrites an arbitrary software program into efficient HLS code that can be used to generate an optimized hardware design. We developed a toolflow named SEER, based on the e-graph data structure, to efficiently explore equivalent implementations of a program at scale. SEER provides an extensible framework, orchestrating existing software compiler passes and hardware synthesis optimizers. Our work is the first attempt to exploit e-graph rewriting for large software compiler frameworks, such as MLIR. Across a set of open-source benchmarks, we show that SEER achieves up to 38x the performance within 1.4x the area of the original program. Via an Intel-provided case study, SEER demonstrates the potential to outperform manually optimized designs produced by hardware experts.",
      "paperUrl": "https://arxiv.org/pdf/2308.07654",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2308.07654",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini"
      ]
    },
    {
      "id": "openalex-w4385574905",
      "source": "openalex-discovery",
      "title": "Runtime-Adaptable Selective Performance Instrumentation",
      "authors": [
        {
          "name": "Sebastian Kreutzer",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Christan Iwainsky",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Marta García-Gasulla",
          "affiliation": "Barcelona Supercomputing Center"
        },
        {
          "name": "Víctor López",
          "affiliation": "Barcelona Supercomputing Center"
        },
        {
          "name": "Christian Bischof",
          "affiliation": "Technical University of Darmstadt"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Automated code instrumentation, i.e. the insertion of measurement hooks into a target application by the compiler, is an established technique for collecting reliable, fine-grained performance data. The set of functions to instrument has to be selected with care, as instrumenting every available function typically yields too large a runtime overhead, thus skewing the measurement. No “one-suits-all” selection mechanism exists, since the instrumentation decision is dependent on the measurement objective, the limit for tolerable runtime overhead and peculiarities of the target application. The Compiler-assisted Performance Instrumentation (CaPI) tool assists in creating such instrumentation configurations, by enabling the user to combine different selection mechanisms as part of a configurable selection pipeline, operating on a statically constructed whole-program call-graph. Previously, CaPI relied on a static instrumentation worktlow which made the process of refining the initial selection quite cumbersome for large-scale codes, as the application had to be recompiled after each adjustment. In this work, we present new runtime-adaptable instrumentation capabilities for CaPI which do not require recompilation when instrumentation changes are made. To this end, the XRay instrumentation feature of the LLVM compiler was extended to support the instrumentation of shared dynamic objects. An XRay-compatible runtime system was added to CaPI that instruments selected functions at program start, thereby significantly reducing the required time for selection refinements. Furthermore, an interface to the TALP tool for recording parallel efficiency metrics was implemented, alongside a specialized selection module for creating suitable coarse-grained region instrumentations.",
      "paperUrl": "https://doi.org/10.1109/ipdpsw59300.2023.00073",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Sebastian Kreutzer"
      ]
    },
    {
      "id": "openalex-w4360995224",
      "source": "openalex-discovery",
      "title": "Rewriting Deep Learning Models for Maximizing Edge TPU Utilization",
      "authors": [
        {
          "name": "Kung-Fu Chen",
          "affiliation": "Institute of Information Science, Academia Sinica"
        },
        {
          "name": "Ding‐Yong Hong",
          "affiliation": "Institute of Information Science, Academia Sinica"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Google Edge TPU is an ASIC designed to accelerate inference of deep learning models on edge devices. Edge TPU only supports a limited set of operations. In those deep learning models containing unsupported operations, the Edge TPU compiler maps the unsupported operations and their succeeding operations to execute on the CPU, even if the succeeding operations can be executed on the Edge TPU. As a result, the Edge TPU is under-utilized and the performance significantly degrades. To overcome this issue, we have developed a model rewriting tool, which leverages MLIR to replace unsupported operations in the model with supported ones while maintaining the same functionality. We also propose a general method to approximate arbitrary continuous functions to any precision using the ReLU operation. Experimental results show that our transformation achieves an average speedup of 1.66× and 4.44× over the models without rewriting on the server and edge platforms, respectively.",
      "paperUrl": "http://dx.doi.org/10.1109/icpads56603.2022.00091",
      "sourceUrl": "https://doi.org/10.1109/icpads56603.2022.00091",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Ding-Yong Hong"
      ]
    },
    {
      "id": "openalex-w4379089538",
      "source": "openalex-discovery",
      "title": "ReDSEa: Automated Acceleration of Triangular Solver on Supercloud Heterogeneous Systems",
      "authors": [
        {
          "name": "Georgios Zacharopoulos",
          "affiliation": ""
        },
        {
          "name": "Ilias Bournias",
          "affiliation": ""
        },
        {
          "name": "Verner Vlacic",
          "affiliation": ""
        },
        {
          "name": "Lukas Cavigelli",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "When utilized effectively, Supercloud heterogeneous systems have the potential to significantly enhance performance. Our ReDSEa tool-chain automates the mapping, load balancing, scheduling, parallelism, and overlapping processes for the Triangular System Solver (TS) on a heterogeneous system consisting of a Huawei Kunpeng ARM multi-core CPU and an Ascend 910 AI HW accelerator. We propose an LLVM compiler tool-chain that a) leverages compiler analysis and b) utilizes novel performance models exploring recursive, iterative, and blocked computation models. Our tool-chain facilitates a speedup of up to 16x compared to an optimized 48-core CPU-only implementation.",
      "paperUrl": "https://arxiv.org/pdf/2305.19917",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2305.19917",
      "tags": [
        "AI",
        "Performance"
      ],
      "matchedAuthors": [
        "Georgios Zacharopoulos"
      ]
    },
    {
      "id": "openalex-w4385270034",
      "source": "openalex-discovery",
      "title": "ROPfuscator: Robust Obfuscation with ROP",
      "authors": [
        {
          "name": "Giulio De Pasquale",
          "affiliation": "King's College London"
        },
        {
          "name": "Fukutomo Nakanishi",
          "affiliation": "Toshiba (Japan)"
        },
        {
          "name": "Daniele Ferla",
          "affiliation": "University of Bologna"
        },
        {
          "name": "Lorenzo Cavallaro",
          "affiliation": "University College London"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Software obfuscation is crucial in protecting intellectual property in software from reverse engineering attempts. While some obfuscation techniques originate from the obfuscation-reverse engineering arms race, others stem from different research areas, such as binary software exploitation. Return-oriented programming (ROP) became one of the most effective exploitation techniques for memory error vulnerabilities. ROP interferes with our natural perception of a process control flow, inspiring us to repurpose ROP as a robust and effective form of software obfuscation. Although previous work already explores ROP's effectiveness as an obfuscation technique, evolving reverse engineering research raises the need for principled reasoning to understand the strengths and limitations of ROP-based mechanisms against man-at-the-end (MATE) attacks. To this end, we present ROPFuscator, a compiler-driven obfuscation pass based on ROP for any programming language supported by LLVM. We incorporate opaque predicates and constants and a novel instruction hiding technique to withstand sophisticated MATE attacks. More importantly, we introduce a realistic and unified threat model to thoroughly evaluate ROPFuscator and provide principled reasoning on ROP-based obfuscation techniques that answer to code coverage, incurred overhead, correctness, robustness, and practicality challenges. The project's source code is published online to aid further research.",
      "paperUrl": "https://doi.org/10.1109/spw59333.2023.00026",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Lorenzo Cavallaro"
      ]
    },
    {
      "id": "openalex-w4226372463",
      "source": "openalex-discovery",
      "title": "RL4ReAl: Reinforcement Learning for Register Allocation",
      "authors": [
        {
          "name": "S. VenkataKeerthy",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Siddharth Jain",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Anilava Kundu",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Rohit Aggarwal",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": "Indian Institute of Technology Hyderabad"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We aim to automate decades of research and experience in register allocation,\\nleveraging machine learning. We tackle this problem by embedding a multi-agent\\nreinforcement learning algorithm within LLVM, training it with the state of the\\nart techniques. We formalize the constraints that precisely define the problem\\nfor a given instruction-set architecture, while ensuring that the generated\\ncode preserves semantic correctness. We also develop a gRPC based framework\\nproviding a modular and efficient compiler interface for training and\\ninference. Our approach is architecture independent: we show experimental\\nresults targeting Intel x86 and ARM AArch64. Our results match or out-perform\\nthe heavily tuned, production-grade register allocators of LLVM.\\n",
      "paperUrl": "https://doi.org/10.1145/3578360.3580273",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Albert Cohen",
        "Ramakrishna Upadrasta",
        "S. VenkataKeerthy",
        "Siddharth Jain"
      ]
    },
    {
      "id": "openalex-w4318541549",
      "source": "openalex-discovery",
      "title": "Propeller: A Profile Guided, Relinking Optimizer for Warehouse-Scale Applications",
      "authors": [
        {
          "name": "Han Shen",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Krzysztof Pszeniczny",
          "affiliation": "Google (Switzerland)"
        },
        {
          "name": "Rahman Lavaee",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Snehasish Kumar",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Sriraman Tallam",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Xinliang Li",
          "affiliation": "Google (United States)"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "While profile guided optimizations (PGO) and link time optimiza-tions (LTO) have been widely adopted, post link optimizations (PLO)have languished until recently when researchers demonstrated that late injection of profiles can yield significant performance improvements. However, the disassembly-driven, monolithic design of post link optimizers face scaling challenges with large binaries andis at odds with distributed build systems. To reconcile and enable post link optimizations within a distributed build environment, we propose Propeller, a relinking optimizer for warehouse scale work-loads. To enable flexible code layout optimizations, we introduce basic block sections, a novel linker abstraction. Propeller uses basic block sections to enable a new approach to PLO without disassembly. Propeller achieves scalability by relinking the binary using precise profiles instead of rewriting the binary. The overhead of relinking is lowered by caching and leveraging distributed compiler actions during code generation. Propeller has been deployed to production at Google with over tens of millions of cores executing Propeller optimized code at any time. An evaluation of internal warehouse-scale applications show Propeller improves performance by 1.1% to 8% beyond PGO and ThinLTO. Compiler tools such as Clang improve by 7% while MySQL improves by 1%. Compared to the state of the art binary optimizer, Propeller achieves comparable performance while lowering memory overheads by 30%-70% on large benchmarks.",
      "paperUrl": "https://doi.org/10.1145/3575693.3575727",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "LTO",
        "Optimizations",
        "Performance",
        "PGO"
      ],
      "matchedAuthors": [
        "Krzysztof Pszeniczny",
        "Sriraman Tallam"
      ]
    },
    {
      "id": "openalex-w4388581171",
      "source": "openalex-discovery",
      "title": "Precision and Performance Analysis of C Standard Math Library Functions on GPUs",
      "authors": [
        {
          "name": "Anton Rydahl",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Joseph Huber",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Ethan Luis Mcdonough",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "With the advent of GPU computing, executing large program sections on accelerators has become increasingly important. Efforts are being made to support the C standard library, LIBC, on GPUs via LLVM machinery. Therefore, the C standard math library, LIBM, must be supported on GPUs. So far, LLVM frontends, such as Clang, have relied on GPU vendor implementations of LIBM functionality wrapped into (mostly) LIBM-compatible forwarding functions.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3624062.3624166",
      "sourceUrl": "https://doi.org/10.1145/3624062.3624166",
      "tags": [
        "Clang",
        "Frontend",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Rydahl",
        "Johannes Doerfert",
        "Joseph Huber"
      ]
    },
    {
      "id": "openalex-w4386707659",
      "source": "openalex-discovery",
      "title": "ORAQL — Optimistic Responses to Alias Queries in LLVM",
      "authors": [
        {
          "name": "Jan Hueckelheim",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Alias analysis (AA) is a prerequisite for many compiler optimizations, which are crucial for performance especially for parallel and scientific software. AA is the subject of ongoing research, and compilers can in practice only approximate the alias information of a given program. In this paper we investigate the extent to which performance in high-performance computing (HPC) applications could be improved if better AA were available in LLVM, one of the most widely used compilers today.",
      "paperUrl": "https://doi.org/10.1145/3605573.3605644",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Jan Hueckelheim",
        "Johannes Doerfert"
      ]
    },
    {
      "id": "openalex-w4379255788",
      "source": "openalex-discovery",
      "title": "Minotaur: A SIMD-Oriented Synthesizing Superoptimizer",
      "authors": [
        {
          "name": "Zhengyang Liu",
          "affiliation": ""
        },
        {
          "name": "Stefan Mada",
          "affiliation": ""
        },
        {
          "name": "John Regehr",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "A superoptimizing compiler--one that performs a meaningful search of the program space as part of the optimization process--can find optimization opportunities that are missed by even the best existing optimizing compilers. We created Minotaur: a superoptimizer for LLVM that uses program synthesis to improve its code generation, focusing on integer and floating-point SIMD code. On an Intel Cascade Lake processor, Minotaur achieves an average speedup of 7.3\\% on the GNU Multiple Precision library (GMP)'s benchmark suite, with a maximum speedup of 13\\%. On SPEC CPU 2017, our superoptimizer produces an average speedup of 1.5\\%, with a maximum speedup of 4.5\\% for 638.imagick. Every optimization produced by Minotaur has been formally verified, and several optimizations that it has discovered have been implemented in LLVM as a result of our work.",
      "paperUrl": "https://arxiv.org/pdf/2306.00229",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2306.00229",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Zhengyang Liu"
      ]
    },
    {
      "id": "openalex-w4386528956",
      "source": "openalex-discovery",
      "title": "Maximizing Parallelism and GPU Utilization For Direct GPU Compilation Through Ensemble Execution",
      "authors": [
        {
          "name": "Shilei Tian",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "GPUs are renowned for their exceptional computational acceleration capabilities achieved through massive parallelism. However, utilizing GPUs for computation requires manual identification of code regions suitable for offloading, data transfer management, and synchronization. Recent advancements have capitalized on the LLVM/OpenMP portable target offloading interface, elevating GPU acceleration to new heights. This approach, known as the direct GPU compilation, involves compiling the entire host application for execution on the GPU, eliminating the need for explicit offloading directives. However, direct GPU compilation is limited to the thread parallelism a CPU application exposes, which is often not enough to saturate a modern GPU.",
      "paperUrl": "http://dx.doi.org/10.1145/3605731.3606016",
      "sourceUrl": "https://doi.org/10.1145/3605731.3606016",
      "tags": [
        "GPU"
      ],
      "matchedAuthors": [
        "Barbara Chapman",
        "Johannes Doerfert",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4327810317",
      "source": "openalex-discovery",
      "title": "Machine Learning-Driven Adaptive OpenMP For Portable Performance on Heterogeneous Systems",
      "authors": [
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": ""
        },
        {
          "name": "Konstantinos Parasyris",
          "affiliation": ""
        },
        {
          "name": "Chunhua Liao",
          "affiliation": ""
        },
        {
          "name": "David Beckingsale",
          "affiliation": ""
        },
        {
          "name": "S.J. Gamblin",
          "affiliation": ""
        },
        {
          "name": "Bronis de Supinski",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Heterogeneity has become a mainstream architecture design choice for building High Performance Computing systems. However, heterogeneity poses significant challenges for achieving performance portability of execution. Adapting a program to a new heterogeneous platform is laborious and requires developers to manually explore a vast space of execution parameters. To address those challenges, this paper proposes new extensions to OpenMP for autonomous, machine learning-driven adaptation. Our solution includes a set of novel language constructs, compiler transformations, and runtime support. We propose a producer-consumer pattern to flexibly define multiple, different variants of OpenMP code regions to enable adaptation. Those regions are transparently profiled at runtime to autonomously learn optimizing machine learning models that dynamically select the fastest variant. Our approach significantly reduces users' efforts of programming adaptive applications on heterogeneous architectures by leveraging machine learning techniques and code generation capabilities of OpenMP compilation. Using a complete reference implementation in Clang/LLVM we evaluate three use-cases of adaptive CPU-GPU execution. Experiments with HPC proxy applications and benchmarks demonstrate that the proposed adaptive OpenMP extensions automatically choose the best performing code variants for various adaptation possibilities, in several different heterogeneous platforms of CPUs and GPUs.",
      "paperUrl": "https://arxiv.org/pdf/2303.08873",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2303.08873",
      "tags": [
        "Clang",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Giorgis Georgakoudis",
        "Konstantinos Parasyris"
      ]
    },
    {
      "id": "openalex-w4315631944",
      "source": "openalex-discovery",
      "title": "MSWasm: Soundly Enforcing Memory-Safe Execution of Unsafe Code",
      "authors": [
        {
          "name": "Alexandra E. Michael",
          "affiliation": "University of Washington"
        },
        {
          "name": "Anitha Gollamudi",
          "affiliation": "University of Massachusetts Lowell"
        },
        {
          "name": "Jay Bosamiya",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Evan Johnson",
          "affiliation": ""
        },
        {
          "name": "Aidan Denlinger",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Craig Disselkoen",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Conrad Watt",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Bryan Parno",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Marco Patrignani",
          "affiliation": "University of Trento"
        },
        {
          "name": "Marco Vassena",
          "affiliation": "Utrecht University"
        },
        {
          "name": "Deian Stefan",
          "affiliation": "University of California, San Diego"
        }
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 7 (Issue POPL)",
      "type": "research-paper",
      "abstract": "Most programs compiled to WebAssembly (Wasm) today are written in unsafe languages like C and C++. Unfortunately, memory-unsafe C code remains unsafe when compiled to Wasm—and attackers can exploit buffer overflows and use-after-frees in Wasm almost as easily as they can on native platforms. Memory- Safe WebAssembly (MSWasm) proposes to extend Wasm with language-level memory-safety abstractions to precisely address this problem. In this paper, we build on the original MSWasm position paper to realize this vision. We give a precise and formal semantics of MSWasm, and prove that well-typed MSWasm programs are, by construction, robustly memory safe. To this end, we develop a novel, language-independent memory-safety property based on colored memory locations and pointers. This property also lets us reason about the security guarantees of a formal C-to-MSWasm compiler—and prove that it always produces memory-safe programs (and preserves the semantics of safe programs). We use these formal results to then guide several implementations: Two compilers of MSWasm to native code, and a C-to-MSWasm compiler (that extends Clang). Our MSWasm compilers support different enforcement mechanisms, allowing developers to make security-performance trade-offs according to their needs. Our evaluation shows that on the PolyBenchC suite, the overhead of enforcing memory safety in software ranges from 22% (enforcing spatial safety alone) to 198% (enforcing full memory safety), and 51.7% when using hardware memory capabilities for spatial safety and pointer integrity. More importantly, MSWasm’s design makes it easy to swap between enforcement mechanisms; as fast (especially hardware-based) enforcement techniques become available, MSWasm will be able to take advantage of these advances almost for free.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3571208",
      "sourceUrl": "https://doi.org/10.1145/3571208",
      "tags": [
        "C++",
        "Clang",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Deian Stefan"
      ]
    },
    {
      "id": "openalex-w4321184919",
      "source": "openalex-discovery",
      "title": "MOD2IR: High-Performance Code Generation for a Biophysically Detailed Neuronal Simulation DSL",
      "authors": [
        {
          "name": "George Mitenkov",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Ioannis Magkanaris",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Omar Awile",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Pramod Kumbhar",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Felix Schürmann",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Advances in computational capabilities and large volumes of experimental data have established computer simulations of brain tissue models as an important pillar in modern neuroscience. Alongside, a variety of domain specific languages (DSLs) have been developed to succinctly express properties of these models, ensure their portability to different platforms, and provide an abstraction that allows scientists to work in their comfort zone of mathematical equations, delegating concerns about performance optimizations to downstream compilers. One of the popular DSLs in modern neuroscience is the NEURON MODeling Language (NMODL). Until now, its compilation process has been split into first transpiling NMODL to C++ and then using a C++ toolchain to emit the efficient machine code. This approach has several drawbacks including the reliance on different programming models to target heterogeneous hardware, maintainability of multiple compiler back-ends and the lack of flexibility to use the domain information for C++ code optimization. To overcome these limitations, we present MOD2IR, a new open-source code generation pipeline for NMODL. MOD2IR leverages the LLVM toolchain to target multiple CPU and GPU hardware platforms. Generating LLVM IR allows the vector extensions of modern CPU architectures to be targeted directly, producing optimized SIMD code. Additionally, this gives MOD2IR significant potential for further optimizations based on the domain information available when LLVM IR code is generated. We present experiments showing that MOD2IR is able to produce on-par execution performance using a single compiler back-end implementation compared to code generated via state-of-the-art C++ compilers, and can even surpass them by up to 1.26×. Moreover, MOD2IR supports JIT-execution of NMODL, yielding both efficient code and an on-the-fly execution workflow.",
      "paperUrl": "https://doi.org/10.1145/3578360.3580268",
      "sourceUrl": "",
      "tags": [
        "C++",
        "GPU",
        "IR",
        "JIT",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson",
        "George Mitenkov"
      ]
    },
    {
      "id": "openalex-w4417133229",
      "source": "openalex-discovery",
      "title": "MLIR — Lowering through LLVM",
      "authors": [
        {
          "name": "Jeremy Kun",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Table of Contents In the last article we lowered our custom poly dialect to standard MLIR dialects. In this article we'll continue lowering it to LLVM IR, exporting it out of MLIR to LLVM, and then compiling to x86 machine code. The code for this article is in this pull request, and as usual the commits are organized to be read in order. Defining a Pipeline The first step in lowering to machine code is to lower to an \"exit dialect.",
      "paperUrl": "https://doi.org/10.59350/98cw5-m4y92",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Jeremy Kun"
      ]
    },
    {
      "id": "openalex-w4417133035",
      "source": "openalex-discovery",
      "title": "MLIR — Getting Started",
      "authors": [
        {
          "name": "Jeremy Kun",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Table of Contents As we announced recently, my team at Google has started a new effort to build production-worthy engineering tools for Fully Homomorphic Encryption (FHE). One focal point of this, and one which I'll be focusing on as long as Google is willing to pay me to do so, is building out a compiler toolchain for FHE in the MLIR framework (Multi-Level Intermediate Representation). The project is called Homomorphic Encryption Intermediate",
      "paperUrl": "https://doi.org/10.59350/wtz11-hkv25",
      "sourceUrl": "",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "Jeremy Kun"
      ]
    },
    {
      "id": "openalex-w4321649880",
      "source": "openalex-discovery",
      "title": "ML-driven Hardware Cost Model for MLIR",
      "authors": [
        {
          "name": "Dibyendu Das",
          "affiliation": ""
        },
        {
          "name": "Sandya Mannarswamy",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "During early optimization passes, compilers must make predictions for machine-dependent characteristics such as execution unit utilization, number of register spills, latency, throughput etc. to generate better code. Often a hand-written static/analytical hardware cost model is built into the compiler. However, the need for more sophisticated and varied predictions has become more pronounced with the development of deep learning compilers which need to optimize dataflow graphs. Such compilers usually employ a much higher level MLIR form as an IR representation before lowering to traditional LLVM-IR. A static/analytical cost model in such a scenario is cumbersome and error prone as the opcodes represent very high level algebraic/arithmetic operations. Hence, we develop a machine learning-based cost model for high-level MLIR which can predict different target variables of interest such as CPU/GPU/xPU utilization, instructions executed, register usage etc. By considering the incoming MLIR as a text input a la NLP models we can apply well-known techniques from modern NLP research to help predict hardware characteristics more accurately. We expect such precise ML-driven hardware cost models to guide our deep learning compiler in graph level optimizations around operator fusion, local memory allocation, kernel scheduling etc. as well as in many kernel-level optimizations such as loop interchange, LICM and unroll. We report early work-in -progress results of developing such models on high-level MLIR representing dataflow graphs emitted by Pytorch/Tensorflow-like frameworks as well as lower-level dialects like affine. We show that these models can provide reasonably good estimates with low error bounds for various hardware characteristics of interest and can be a go-to mechanism for hardware cost modelling in the future.",
      "paperUrl": "https://arxiv.org/pdf/2302.11405",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2302.11405",
      "tags": [
        "GPU",
        "IR",
        "ML",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Dibyendu Das"
      ]
    },
    {
      "id": "openalex-w4386763463",
      "source": "openalex-discovery",
      "title": "ML-CGRA: An Integrated Compilation Framework to Enable Efficient Machine Learning Acceleration on CGRAs",
      "authors": [
        {
          "name": "Yixuan Luo",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Cheng Tan",
          "affiliation": "Microsoft Research (United Kingdom)"
        },
        {
          "name": "Nícolas Bohm Agostini",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Ang Li",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Antonino Tumeo",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Nirav Dave",
          "affiliation": "Microsoft Research (United Kingdom)"
        },
        {
          "name": "Tong Geng",
          "affiliation": "University of Rochester"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Coarse-Grained Reconfigurable Arrays (CGRAs) can achieve higher energy-efficiency than general-purpose processors and accelerators or fine-grained reconfigurable devices, while maintaining adaptability to different computational patterns. CGRAs have shown some success as a platform to accelerate machine learning (ML) thanks to their flexibility, which allows them to support new models not considered by fixed accelerators. However, current solutions for CGRAs employ low level instruction-based compiler approaches and lack specialized compilation infrastructures from high-level ML frameworks that could leverage semantic information from the models, limiting the ability to efficiently map them on the reconfigurable substrate. This paper proposes ML-CGRA, an integrated compilation framework based on the MLIR infrastructure that enables efficient ML acceleration on CGRAs. ML-CGRA provides an end-to-end solution for mapping ML models on CGRAs that outperforms conventional approaches by 3.15× and 6.02 × on 4×4 and 8×8 CGRAs, respectively. The framework is open-source and available from https://github.com/tancheng/mlir-cgra.",
      "paperUrl": "https://doi.org/10.1109/dac56929.2023.10247873",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "ML",
        "MLIR"
      ],
      "matchedAuthors": [
        "Ang Li"
      ]
    },
    {
      "id": "openalex-w4376607496",
      "source": "openalex-discovery",
      "title": "MAFIA: Protecting the Microarchitecture of Embedded Systems Against Fault Injection Attacks",
      "authors": [
        {
          "name": "Thomas Chamelot",
          "affiliation": "Commissariat à l'Énergie Atomique et aux Énergies Alternatives"
        },
        {
          "name": "Damien Couroussé",
          "affiliation": "Commissariat à l'Énergie Atomique et aux Énergies Alternatives"
        },
        {
          "name": "Karine Heydemann",
          "affiliation": "Centre National de la Recherche Scientifique"
        }
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems | Vol. 42 (Issue 12)",
      "type": "research-paper",
      "abstract": "Fault injection attacks represent an effective threat to embedded systems. Recently, Laurent et al. have reported that fault injection attacks can leverage faults inside the microarchitecture. However, state-of-the-art counter-measures, hardwareonly or with hardware support, do not consider the integrity of microarchitecture control signals that are the target of these faults. We present MAFIA, a microarchitecture protection against fault injection attacks. MAFIA ensures integrity of pipeline control signals through a signature-based mechanism, and ensures fine-grained control-flow integrity with a complete indirect branch support and code authenticity. We analyse the security properties of two different implementations with different security/overhead trade-offs: one with a CBC-MAC/Prince signature function, and another one with a CRC32. We present our implementation of MAFIA in a RISC-V processor, supported by a dedicated compiler toolchain based on LLVM/Clang. We report a hardware area overhead of 23.8 % and 6.5 % for the CBC-MAC/Prince and CRC32 respectively. The average code size and execution time overheads are 29.4 % and 18.4 % respectively for the CRC32 implementation and are 50 % and 39 % for the CBC-MAC/Prince.",
      "paperUrl": "https://arxiv.org/pdf/2309.02255",
      "sourceUrl": "https://doi.org/10.1109/tcad.2023.3276507",
      "tags": [
        "Clang",
        "Embedded",
        "Security"
      ],
      "matchedAuthors": [
        "Damien Couroussé",
        "Karine Heydemann"
      ]
    },
    {
      "id": "openalex-w4362675208",
      "source": "openalex-discovery",
      "title": "Leveraging MLIR for Better SYCL Compilation (Poster)",
      "authors": [
        {
          "name": "Víctor Pérez",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Ettore Tiotto",
          "affiliation": ""
        },
        {
          "name": "Whitney Tsang",
          "affiliation": ""
        },
        {
          "name": "Arnamoy Bhattacharyya",
          "affiliation": ""
        },
        {
          "name": "Lukáš Sommer",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Victor Lomüller",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "Jefferson Le Quellec",
          "affiliation": "Codeplay (United Kingdom)"
        },
        {
          "name": "James Brodman",
          "affiliation": "Intel (United States)"
        }
      ],
      "year": "2023",
      "venue": "International Workshop on OpenCL | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Recent years have raised awareness of the fact that many optimizing C++ compilers, such as Clang/LLVM, miss optimization opportunities due to the lack of a suitable high-level intermediate representation. The typical compilation flow of such a compiler would lower from a representation close to the original or pre-processed source code, e.g., an abstract syntax tree (AST), directly to a low-level, CFG- and SSA-based intermediate representation such as LLVM IR.",
      "paperUrl": "https://doi.org/10.1145/3585341.3585379",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Ettore Tiotto",
        "Victor Lomüller",
        "Whitney Tsang"
      ]
    },
    {
      "id": "openalex-w4386755534",
      "source": "openalex-discovery",
      "title": "Large Language Models for Compiler Optimization",
      "authors": [
        {
          "name": "Chris Cummins",
          "affiliation": ""
        },
        {
          "name": "Volker Seeker",
          "affiliation": ""
        },
        {
          "name": "Dejan Grubisic",
          "affiliation": ""
        },
        {
          "name": "Mostafa Elhoushi",
          "affiliation": ""
        },
        {
          "name": "Youwei Liang",
          "affiliation": ""
        },
        {
          "name": "Baptiste Rozière",
          "affiliation": ""
        },
        {
          "name": "Jonas Gehring",
          "affiliation": ""
        },
        {
          "name": "Fabian Gloeckle",
          "affiliation": ""
        },
        {
          "name": "Kim Hazelwood",
          "affiliation": ""
        },
        {
          "name": "Gabriel Synnaeve",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We explore the novel application of Large Language Models to code optimization. We present a 7B-parameter transformer model trained from scratch to optimize LLVM assembly for code size. The model takes as input unoptimized assembly and outputs a list of compiler options to best optimize the program. Crucially, during training, we ask the model to predict the instruction counts before and after optimization, and the optimized code itself. These auxiliary learning tasks significantly improve the optimization performance of the model and improve the model's depth of understanding. We evaluate on a large suite of test programs. Our approach achieves a 3.0% improvement in reducing instruction counts over the compiler, outperforming two state-of-the-art baselines that require thousands of compilations. Furthermore, the model shows surprisingly strong code reasoning abilities, generating compilable code 91% of the time and perfectly emulating the output of the compiler 70% of the time.",
      "paperUrl": "https://arxiv.org/pdf/2309.07062",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2309.07062",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather",
        "Kim Hazelwood",
        "Mostafa Elhoushi"
      ]
    },
    {
      "id": "openalex-w4386907353",
      "source": "openalex-discovery",
      "title": "Julia as a unifying end-to-end workflow language on the Frontier exascale system",
      "authors": [
        {
          "name": "William F. Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Caira Anderson",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Katrina W. Lee",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Ana Gainaru",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Rafael Ferreira da Silva",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We evaluate Julia as a single language and ecosystem paradigm powered by LLVM\\nto develop workflow components for high-performance computing. We run a\\nGray-Scott, 2-variable diffusion-reaction application using a memory-bound,\\n7-point stencil kernel on Frontier, the US Department of Energy's first\\nexascale supercomputer. We evaluate the performance, scaling, and trade-offs of\\n(i) the computational kernel on AMD's MI250x GPUs, (ii) weak scaling up to\\n4,096 MPI processes/GPUs or 512 nodes, (iii) parallel I/O writes using the\\nADIOS2 library bindings, and (iv) Jupyter Notebooks for interactive analysis.\\nResults suggest that although Julia generates a reasonable LLVM-IR, a nearly\\n50% performance difference exists vs. native AMD HIP stencil codes when running\\non the GPUs. As expected, we observed near-zero overhead when using MPI and\\nparallel I/O bindings for system-wide installed implementations. Consequently,\\nJulia emerges as a compelling high-performance and high-productivity workflow\\ncomposition language, as measured on the fastest supercomputer in the world.\\n",
      "paperUrl": "https://arxiv.org/pdf/2309.10292",
      "sourceUrl": "https://doi.org/10.1145/3624062.3624278",
      "tags": [
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4386709678",
      "source": "openalex-discovery",
      "title": "Implementing OpenMP’s SIMD Directive in LLVM’s GPU Runtime",
      "authors": [
        {
          "name": "Eric Wright",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Shilei Tian",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Sunita Chandrasekaran",
          "affiliation": "University of Delaware"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "GPUs support three levels of parallelism: thread blocks, warps (or wavefronts) within a block, and threads within a warp. Some GPU programming models allow the use of all three of these levels, such as OpenMP offloading with the teams, parallel, and simd directives. However LLVM/OpenMP does not support simd and only uses two levels, thread blocks and all threads within a block. For codes with three explicit layers of parallelism this can decrease performance and potentially require restructuring of the application. In this work we present our design and implementation of the OpenMP simd directive in LLVM's OpenMP GPU runtime, which includes both CPU-centric and GPU-centric execution models. We evaluate our prototype using kernels and a few proxy applications showing a performance improvement ranging from 1.3x to 3.5x depending on the benefit the kernels receives from such an optimization. Thus, this work enables real-world applications with three explicit layers of parallelism to expose to better exploit the full benefits of GPU architecture.",
      "paperUrl": "http://dx.doi.org/10.1145/3605573.3605640",
      "sourceUrl": "https://doi.org/10.1145/3605573.3605640",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Barbara Chapman",
        "Johannes Doerfert",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4385245129",
      "source": "openalex-discovery",
      "title": "Half&Half: Demystifying Intel’s Directional Branch Predictors for Fast, Secure Partitioned Execution",
      "authors": [
        {
          "name": "Hosein Yavarzadeh",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Mohammadkazem Taram",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Shravan Narayan",
          "affiliation": "The University of Texas at Austin"
        },
        {
          "name": "Deian Stefan",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Dean M. Tullsen",
          "affiliation": "University of California, San Diego"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper presents Half&Half, a novel software defense against branch-based side-channel attacks. Half&Half isolates the effects of different protection domains on the conditional branch predictors (CBPs) in modern Intel processors. This work presents the first exhaustive analysis of modern conditional branch prediction structures, and reveals for the first time an unknown opportunity to physically partition all CBP structures and completely prevent leakage between two domains using the shared predictor. Half&Half is a software-only solution to branch predictor isolation that requires no changes to the hardware or ISA, and only requires minor modifications to be supported in existing compilers. We implement Half&Half in the LLVM and WebAssembly compilers and show that it incurs an order of magnitude lower overhead compared to the current state-of-the-art branch-based side-channel defenses.",
      "paperUrl": "https://doi.org/10.1109/sp46215.2023.10179415",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Deian Stefan"
      ]
    },
    {
      "id": "openalex-w4379536395",
      "source": "openalex-discovery",
      "title": "HEaaN.MLIR: An Optimizing Compiler for Fast Ring-Based Homomorphic Encryption",
      "authors": [
        {
          "name": "S. Park",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Woosung Song",
          "affiliation": ""
        },
        {
          "name": "Nam SeungHyeon",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Hyeongyu Kim",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Junbum Shin",
          "affiliation": ""
        },
        {
          "name": "Juneyoung Lee",
          "affiliation": "Amazon (United States)"
        }
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 7 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "Homomorphic encryption (HE) is an encryption scheme that provides arithmetic operations on the encrypted data without doing decryption. For Ring-based HE, an encryption scheme that uses arithmetic operations on a polynomial ring as building blocks, performance improvement of unit HE operations has been achieved by two kinds of efforts. The first one is through accelerating the building blocks, polynomial operations. However, it does not facilitate optimizations across polynomial operations such as fusing two polynomial operations. The second one is implementing highly optimized HE operations in an amalgamated manner. The written codes have superior performance, but they are hard to maintain. To resolve these challenges, we propose HEaaN.MLIR, a compiler that performs optimizations across polynomial operations. Also, we propose Poly and ModArith, compiler intermediate representations (IRs) for integer polynomial arithmetic and modulus arithmetic on integer arrays. HEaaN.MLIR has compiler optimizations that are motivated by manual optimizations that HE developers do. These include optimizing modular arithmetic operations, fusing loops, and vectorizing integer arithmetic instructions. HEaaN.MLIR can parse a program consisting of the Poly and ModArith instructions and generate a high-performance, multithreaded machine code for a CPU. Our experiment shows that the compiled operations outperform heavily optimized open-source and commercial HE libraries by up to 3.06x in a single thread and 4.55x in multiple threads.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3591228",
      "sourceUrl": "https://doi.org/10.1145/3591228",
      "tags": [
        "Libraries",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Juneyoung Lee"
      ]
    },
    {
      "id": "openalex-w4384154578",
      "source": "openalex-discovery",
      "title": "GrayC: Greybox Fuzzing of Compilers and Analysers for C",
      "authors": [
        {
          "name": "Karine Even-Mendoza",
          "affiliation": "King's College London"
        },
        {
          "name": "Arindam Sharma",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Cristian Cadar",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Fuzzing of compilers and code analysers has led to a large number of bugs being found and fixed in widely-used frameworks such as LLVM, GCC and Frama-C. Most such fuzzing techniques have taken a blackbox approach, with compilers and code analysers starting to become relatively immune to such fuzzers. We propose a coverage-directed, mutation-based approach for fuzzing C compilers and code analysers, inspired by the success of this type of greybox fuzzing in other application domains. The main challenge of applying mutation-based fuzzing in this context is that naive mutations are likely to generate programs that do not compile. Such programs are not useful for finding deep bugs that affect optimisation, analysis, and code generation routines. We have designed a novel greybox fuzzer for C compilers and analysers by developing a new set of mutations to target common C constructs, and transforming fuzzed programs so that they produce meaningful output, allowing differential testing to be used as a test oracle, and paving the way for fuzzer-generated programs to be integrated into compiler and code analyser regression test suites. We have implemented our approach in GrayC, a new open-source LibFuzzer-based tool, and present experiments showing that it provides more coverage on the middle- and back-end stages of compilers and analysers compared to other mutation-based approaches, including Clang-Fuzzer, PolyGlot, and a technique similar to LangFuzz. We have used GrayC to identify 30 confirmed compiler and code analyser bugs: 25 previously unknown bugs (with 22 of them already fixed in response to our reports) and 5 confirmed bugs reported independently shortly before we found them. A further 3 bug reports are under investigation. Apart from the results above, we have contributed 24 simplified versions of coverage-enhancing test cases produced by GrayC to the Clang/LLVM test suite, targeting 78 previously uncovered functions in the LLVM codebase.",
      "paperUrl": "https://doi.org/10.1145/3597926.3598130",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Testing"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson",
        "Cristian Cadar"
      ]
    },
    {
      "id": "openalex-w4379512400",
      "source": "openalex-discovery",
      "title": "Fuzzing Loop Optimizations in Compilers for C++ and Data-Parallel Languages",
      "authors": [
        {
          "name": "Vsevolod Livinskii",
          "affiliation": "University of Utah"
        },
        {
          "name": "Dmitry Babokin",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 7 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "Compilers are part of the foundation upon which software systems are built; they need to be as correct as possible. This paper is about stress-testing loop optimizers; it presents a major reimplementation of Yet Another Random Program Generator (YARPGen), an open-source generative compiler fuzzer. This new version has found 122 bugs, both in compilers for data-parallel languages, such as the Intel® Implicit SPMD Program Compiler and the Intel® oneAPI DPC++ compiler, and in C++ compilers such as GCC and Clang/LLVM. The first main contribution of our work is a novel method for statically avoiding undefined behavior when generating loops; the resulting programs conform to the relevant language standard, enabling automated testing. The second main contribution is a collection of mechanisms for increasing the diversity of generated loop code; in our evaluation, we demonstrate that these make it possible to trigger loop optimizations significantly more often, providing opportunities to discover bugs in the optimizers.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3591295",
      "sourceUrl": "https://doi.org/10.1145/3591295",
      "tags": [
        "C++",
        "Clang",
        "Optimizations",
        "Testing"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Vsevolod Livinskii"
      ]
    },
    {
      "id": "openalex-w4387355998",
      "source": "openalex-discovery",
      "title": "Fortran performance optimisation and auto-parallelisation by leveraging MLIR-based domain specific abstractions in Flang",
      "authors": [
        {
          "name": "Nick Brown",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Maurice Jamieson",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Anton Lydike",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Emilien Bauer",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "MLIR has become popular since it was open sourced in 2019. A sub-project of\\nLLVM, the flexibility provided by MLIR to represent Intermediate\\nRepresentations (IR) as dialects at different abstraction levels, to mix these,\\nand to leverage transformations between dialects provides opportunities for\\nautomated program optimisation and parallelisation. In addition to general\\npurpose compilers built upon MLIR, domain specific abstractions have also been\\ndeveloped.\\n In this paper we explore complimenting the Flang MLIR general purpose\\ncompiler by combining with the domain specific Open Earth Compiler's MLIR\\nstencil dialect. Developing transformations to discover and extracts stencils\\nfrom Fortran, this specialisation delivers between a 2 and 10 times performance\\nimprovement for our benchmarks on a Cray supercomputer compared to using Flang\\nalone. Furthermore, by leveraging existing MLIR transformations we develop an\\nauto-parallelisation approach targeting multi-threaded and distributed memory\\nparallelism, and optimised execution on GPUs, without any modifications to the\\nserial Fortran source code.\\n",
      "paperUrl": "https://arxiv.org/pdf/2310.01882",
      "sourceUrl": "https://doi.org/10.1145/3624062.3624167",
      "tags": [
        "Flang",
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Anton Lydike",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4387725002",
      "source": "openalex-discovery",
      "title": "Fast Summary-based Whole-program Analysis to Identify Unsafe Memory Accesses in Rust",
      "authors": [
        {
          "name": "Jie Zhou",
          "affiliation": ""
        },
        {
          "name": "Mingshen Sun",
          "affiliation": ""
        },
        {
          "name": "John Criswell",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Rust is one of the most promising systems programming languages to fundamentally solve the memory safety issues that have plagued low-level software for over forty years. However, to accommodate the scenarios where Rust's type rules might be too restrictive for certain systems programming and where programmers opt for performance over security checks, Rust opens security escape hatches allowing writing unsafe source code or calling unsafe libraries. Consequently, unsafe Rust code and directly-linked unsafe foreign libraries may not only introduce memory safety violations themselves but also compromise the entire program as they run in the same monolithic address space as the safe Rust. This problem can be mitigated by isolating unsafe memory objects (those accessed by unsafe code) and sandboxing memory accesses to the unsafe memory. One category of prior work utilizes existing program analysis frameworks on LLVM IR to identify unsafe memory objects and accesses. However, they suffer the limitations of prolonged analysis time and low precision. In this paper, we tackled these two challenges using summary-based whole-program analysis on Rust's MIR. The summary-based analysis computes information on demand so as to save analysis time. Performing analysis on Rust's MIR exploits the rich high-level type information inherent to Rust, which is unavailable in LLVM IR. This manuscript is a preliminary study of ongoing research. We have prototyped a whole-program analysis for identifying both unsafe heap allocations and memory accesses to those unsafe heap objects. We reported the overhead and the efficacy of the analysis in this paper.",
      "paperUrl": "https://arxiv.org/pdf/2310.10298",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2310.10298",
      "tags": [
        "IR",
        "Libraries",
        "Performance",
        "Programming Languages",
        "Rust",
        "Security"
      ],
      "matchedAuthors": [
        "John Criswell"
      ]
    },
    {
      "id": "openalex-w4391623920",
      "source": "openalex-discovery",
      "title": "Fast Instruction Selection for Fast Digital Signal Processing",
      "authors": [
        {
          "name": "Alexander J Root",
          "affiliation": "Stanford University"
        },
        {
          "name": "M Ahmad",
          "affiliation": "Adobe Systems (United States)"
        },
        {
          "name": "Dillon Sharlet",
          "affiliation": ""
        },
        {
          "name": "Andrew L. Adams",
          "affiliation": "Adobe Systems (United States)"
        },
        {
          "name": "Shoaib Kamil",
          "affiliation": "Adobe Systems (United States)"
        },
        {
          "name": "Jonathan Ragan‐Kelley",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern vector processors support a wide variety of instructions for fixed-point digital signal processing. These instructions support a proliferation of rounding, saturating, and type conversion modes, and are often fused combinations of more primitive operations. While these are common idioms in fixed-point signal processing, it is difficult to use these operations in portable code. It is challenging for programmers to write down portable integer arithmetic in a C-like language that corresponds exactly to one of these instructions, and even more challenging for compilers to recognize when these instructions can be used. Our system, Pitchfork, defines a portable fixed-point intermediate representation, FPIR, that captures common idioms in fixed-point code. FPIR can be used directly by programmers experienced with fixed-point, or Pitchfork can automatically lift from integer operations into FPIR using a term-rewriting system (TRS) composed of verified manual and automatically-synthesized rules. Pitchfork then lowers from FPIR into target-specific fixed-point instructions using a set of target-specific TRSs. We show that this approach improves runtime performance of portably-written fixed-point signal processing code in Halide, across a range of benchmarks, by geomean 1.31x on x86 with AVX2, 1.82x on ARM Neon, and 2.44x on Hexagon HVX compared to a standard LLVM-based compiler flow, while maintaining or improving existing compile times.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3623278.3624768",
      "sourceUrl": "https://doi.org/10.1145/3623278.3624768",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Dillon Sharlet"
      ]
    },
    {
      "id": "openalex-w4384705412",
      "source": "openalex-discovery",
      "title": "Fast And Automatic Floating Point Error Analysis With CHEF-FP",
      "authors": [
        {
          "name": "Garima Singh",
          "affiliation": "Princeton University"
        },
        {
          "name": "Baidyanath Kundu",
          "affiliation": "European Organization for Nuclear Research"
        },
        {
          "name": "Harshitha Menon",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Alexander Penev",
          "affiliation": "Plovdiv University"
        },
        {
          "name": "D. J. Lange",
          "affiliation": "Princeton University"
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": "Princeton University"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As we reach the limit of Moore's Law, researchers are exploring different paradigms to achieve unprecedented performance. Approximate Computing (AC), which relies on the ability of applications to tolerate some error in the results to trade-off accuracy for performance, has shown significant promise. Despite the success of AC in domains such as Machine Learning, its acceptance in High-Performance Computing (HPC) is limited due to its stringent requirement of accuracy. We need tools and techniques to identify regions of the code that are amenable to approximations and their impact on the application output quality so as to guide developers to employ selective approximation. To this end, we propose CHEF-FP, a flexible, scalable, and easy-to-use source-code transformation tool based on Automatic Differentiation (AD) for analysing approximation errors in HPC applications.CHEF-FP uses Clad, an efficient AD tool built as a plugin to the Clang compiler and based on the LLVM compiler infrastructure, as a backend and utilizes its AD abilities to evaluate approximation errors in C++ code. CHEF-FP works at the source level by injecting error estimation code into the generated adjoints. This enables the error-estimation code to undergo compiler optimizations resulting in improved analysis time and reduced memory usage. We also provide theoretical and architectural augmentations to source code transformation-based AD tools to perform FP error analysis. In this paper, we primarily focus on analyzing errors introduced by mixed-precision AC techniques, the most popular approximate technique in HPC. We also show the applicability of our tool in estimating other kinds of errors by evaluating our tool on codes that use approximate functions. Moreover, we demonstrate the speedups achieved by CHEF-FP during analysis time as compared to the existing state-of-the-art tool as a result of its ability to generate and insert approximation error estimate code directly into the derivative source. The generated code also becomes a candidate for better compiler optimizations contributing to lesser runtime performance overhead.",
      "paperUrl": "https://doi.org/10.1109/ipdps54959.2023.00105",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "C++",
        "Clang",
        "Infrastructure",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w4367665040",
      "source": "openalex-discovery",
      "title": "FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform Libraries",
      "authors": [
        {
          "name": "Yifei He",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Artur Podobas",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Måns I. Andersson",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Stefano Markidis",
          "affiliation": "KTH Royal Institute of Technology"
        }
      ],
      "year": "2023",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-031-31209-0_6",
      "sourceUrl": "",
      "tags": [
        "Libraries",
        "MLIR"
      ],
      "matchedAuthors": [
        "Yifei He"
      ]
    },
    {
      "id": "openalex-w4390092842",
      "source": "openalex-discovery",
      "title": "Experiences Building an MLIR-based SYCL Compiler",
      "authors": [
        {
          "name": "Ettore Tiotto",
          "affiliation": ""
        },
        {
          "name": "Víctor Pérez",
          "affiliation": ""
        },
        {
          "name": "Whitney Tsang",
          "affiliation": ""
        },
        {
          "name": "Lukáš Sommer",
          "affiliation": ""
        },
        {
          "name": "Julian Oppermann",
          "affiliation": ""
        },
        {
          "name": "Victor Lomüller",
          "affiliation": ""
        },
        {
          "name": "Mehdi Goli",
          "affiliation": ""
        },
        {
          "name": "James Brodman",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Similar to other programming models, compilers for SYCL, the open programming model for heterogeneous computing based on C++, would benefit from access to higher-level intermediate representations. The loss of high-level structure and semantics caused by premature lowering to low-level intermediate representations and the inability to reason about host and device code simultaneously present major challenges for SYCL compilers. The MLIR compiler framework, through its dialect mechanism, allows to model domain-specific, high-level intermediate representations and provides the necessary facilities to address these challenges. This work therefore describes practical experience with the design and implementation of an MLIR-based SYCL compiler. By modeling key elements of the SYCL programming model in host and device code in the MLIR dialect framework, the presented approach enables the implementation of powerful device code optimizations as well as analyses across host and device code. Compared to two LLVM-based SYCL implementations, this yields speedups of up to 4.3x on a collection of SYCL benchmark applications. Finally, this work also discusses challenges encountered in the design and implementation and how these could be addressed in the future.",
      "paperUrl": "https://arxiv.org/pdf/2312.13170",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2312.13170",
      "tags": [
        "C++",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Ettore Tiotto",
        "Julian Oppermann",
        "Victor Lomüller",
        "Whitney Tsang"
      ]
    },
    {
      "id": "openalex-w4385585370",
      "source": "openalex-discovery",
      "title": "Evaluating performance and portability of high-level programming models: Julia, Python/Numba, and Kokkos on exascale nodes",
      "authors": [
        {
          "name": "William F. Godoy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Pedro Valero‐Lara",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "T. Elise Dettling",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Christian Trefftz",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Ian Jorquera",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Thomas W. Sheehy",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Ross Miller",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Marc Gonzalez-Tallada",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Valentin Churavy",
          "affiliation": "Massachusetts Institute of Technology"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We explore the performance and portability of the high-level programming models: the LLVM-based Julia and Python/Numba, and Kokkos on high-performance computing (HPC) nodes: AMD Epyc CPUs and MI250X graphical processing units (GPUs) on Frontier’s test bed Crusher system and Ampere’s Arm-based CPUs and NVIDIA’s A100 GPUs on the Wombat system at the Oak Ridge Leadership Computing Facilities. We compare the default performance of a hand-rolled dense matrix multiplication algorithm on CPUs against vendor-compiled C/OpenMP implementations, and on each GPU against CUDA and HIP. Rather than focusing on the kernel optimization per-se, we select this naive approach to resemble exploratory work in science and as a lower-bound for performance to isolate the effect of each programming model. Julia and Kokkos perform comparably with C/OpenMP on CPUs, while Julia implementations are competitive with CUDA and HIP on GPUs. Performance gaps are identified on NVIDIA A100 GPUs for Julia’s single precision and Kokkos, and for Python/Numba in all scenarios. We also comment on half-precision support, productivity, performance portability metrics, and platform readiness. We expect to contribute to the understanding and direction for high-level, high-productivity languages in HPC as the first-generation exascale systems are deployed.",
      "paperUrl": "https://www.osti.gov/biblio/1994693",
      "sourceUrl": "https://doi.org/10.1109/ipdpsw59300.2023.00068",
      "tags": [
        "CUDA",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter",
        "Valentin Churavy"
      ]
    },
    {
      "id": "openalex-w4386510480",
      "source": "openalex-discovery",
      "title": "Enhancing LLVM Optimizations for Linear Recurrence Programs on RVV",
      "authors": [
        {
          "name": "Hung-Ming Lai",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Shin Hwang",
          "affiliation": "National Taiwan University of Science and Technology"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The RISC-V Vector Extension (RVV) has emerged as a promising vector architecture for high-performance computing. It enables parallel computing capability for RISC-V CPUs by introducing additional vector instructions and vector registers. To fully utilize the potential of RVV, it is crucial to optimize the auto-vectorization capabilities of compilers. Auto-vectorization is a widely used method for parallelizing scalar programs on vector machines. It examines the input program to identify the parallelizable section and then maps it to the underlying vector architecture. This paper focuses on enhancing LLVM’s auto-vectorization for linear recurrence programs on RVV, with a specific emphasis on a well-studied computation pattern called scan (all-prefix-sums). Linear recurrence programs are prevalent in diverse computational domains, and their efficient execution is vital for achieving optimal performance. However, the auto-vectorization on current LLVM’s vectorizer is hindered by the loop-carried data dependence of recurrence programs. In this work, we propose novel techniques to address the challenges of auto-vectorizing linear recurrence programs on RVV, leveraging the unique features of RVV and extending LLVM’s vectorization capabilities. The experiment shows that our auto-vectorized plus-scan can achieved 17.64x speedup on RVV compared to LLVM 16.0.0.",
      "paperUrl": "https://doi.org/10.1145/3605731.3605904",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Hung-Ming Lai",
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4362703965",
      "source": "openalex-discovery",
      "title": "Efficient and Accurate Automatic Python Bindings with cppyy & Cling",
      "authors": [
        {
          "name": "Baidyanath Kundu",
          "affiliation": "Princeton University"
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": "European Council"
        },
        {
          "name": "W. Lavrijsen",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The simplicity of Python and the power of C++ force stark choices on a scientific software stack. There have been multiple developments to mitigate language boundaries by implementing language bindings, but the impedance mismatch between the static nature of C++ and the dynamic one of Python hinders their implementation; examples include the use of user-defined Python types with templated C++ and advanced memory management. The development of the C++ interpreter Cling has changed the way we can think of language bindings as it provides an incremental compilation infrastructure available at runtime. That is, Python can interrogate C++ on demand, and bindings can be lazily constructed at runtime. This automatic binding provision requires no direct support from library authors and offers better performance than alternative solutions, such as PyBind11. ROOT pioneered this approach with PyROOT, which was later enhanced with its successor, cppyy. However, until now, cppyy relied on the reflection layer of ROOT, which is limited in terms of provided features and performance. This paper presents the next step for language interoperability with cppyy, enabling research into uniform cross-language execution environments and boosting optimization opportunities across language boundaries. We illustrate the use of advanced C++ in Numba-accelerated Python through cppyy. We outline a path forward for re-engineering parts of cppyy to use upstream LLVM components to improve performance and sustainability. We demonstrate cppyy purely based on a C++ reflection library, InterOp, which offers interoperability primitives based on Cling and Clang-Repl.",
      "paperUrl": "https://arxiv.org/pdf/2304.02712",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2304.02712",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w4379534462",
      "source": "openalex-discovery",
      "title": "Don’t Look UB: Exposing Sanitizer-Eliding Compiler Optimizations",
      "authors": [
        {
          "name": "Raphael Isemann",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Cristiano Giuffrida",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Herbert Bos",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Erik van der Kouwe",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Klaus von Gleissenthall",
          "affiliation": "Vrije Universiteit Amsterdam"
        }
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 7 (Issue PLDI)",
      "type": "research-paper",
      "abstract": "Sanitizers are widely used compiler features that detect undefined behavior and resulting vulnerabilities by injecting runtime checks into programs. For better performance, sanitizers are often used in conjunction with optimization passes. But doing so combines two compiler features with conflicting objectives. While sanitizers want to expose undefined behavior, optimizers often exploit these same properties for performance. In this paper, we show that this clash can have serious consequences: optimizations can remove sanitizer failures, thereby hiding the presence of bugs or even introducing new ones. We present LookUB, a differential-testing based framework for finding optimizer transformations that elide sanitizer failures. We used our method to find 17 such sanitizer-eliding optimizations in Clang. Next, we used static analysis and fuzzing to search for bugs in open-source projects that were previously hidden due to sanitizer-eliding optimizations. This led us to discover 20 new bugs in Linux Containers, libmpeg2, NTFS-3G, and WINE. Finally, we present an effective mitigation strategy based on a customization of the Clang optimizer with an overhead increase of 4%.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3591257",
      "sourceUrl": "https://doi.org/10.1145/3591257",
      "tags": [
        "Clang",
        "Optimizations",
        "Performance",
        "Static Analysis",
        "Testing"
      ],
      "matchedAuthors": [
        "Cristiano Giuffrida",
        "Herbert Bos",
        "Raphael Isemann"
      ]
    },
    {
      "id": "openalex-w4366830184",
      "source": "openalex-discovery",
      "title": "DatAFLow : Toward a Data-flow-guided Fuzzer",
      "authors": [
        {
          "name": "Adrian Herrera",
          "affiliation": "Australian National University"
        },
        {
          "name": "Mathias Payer",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Antony L. Hosking",
          "affiliation": "Australian National University"
        }
      ],
      "year": "2023",
      "venue": "ACM Transactions on Software Engineering and Methodology | Vol. 32 (Issue 5)",
      "type": "research-paper",
      "abstract": "This Replicating Computational Report (RCR) describes (a) our datAFLow fuzzer and (b) how to replicate the results in “ datAFLow : Toward a Data-Flow-Guided Fuzzer.” Our primary artifact is the datAFLow fuzzer. Unlike traditional coverage-guided greybox fuzzers—which use control-flow coverage to drive program exploration— datAFLow uses data-flow coverage to drive exploration. This is achieved through a set of LLVM-based analyses and transformations. In addition to datAFLow , we also provide a set of tools, scripts, and patches for (a) statically analyzing data flows in a target program, (b) compiling a target program with the datAFLow instrumentation, (c) evaluating datAFLow on the Magma benchmark suite, and (d) evaluating datAFLow on the DDFuzz dataset. datAFLow is available at https://github.com/HexHive/datAFLow.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3587159",
      "sourceUrl": "https://doi.org/10.1145/3587159",
      "tags": [],
      "matchedAuthors": [
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w4387158113",
      "source": "openalex-discovery",
      "title": "ComPile: A Large IR Dataset from Production Sources",
      "authors": [
        {
          "name": "Aiden Grossman",
          "affiliation": "University of California, Davis"
        },
        {
          "name": "Ludger Paehler",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Konstantinos Parasyris",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Tal Ben‐Nun",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Jacob Hegna",
          "affiliation": "University of Minnesota System"
        },
        {
          "name": "William S. Moses",
          "affiliation": "International University of the Caribbean"
        },
        {
          "name": "Jose M. Monsalve Diaz",
          "affiliation": ""
        },
        {
          "name": "Mircea Trofin",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Code is increasingly becoming a core data modality of modern machine learning research impacting not only the way we write code with conversational agents like OpenAI's ChatGPT, Google's Bard, or Anthropic's Claude, the way we translate code from one language into another, but also the compiler infrastructure underlying the language. While modeling approaches may vary and representations differ, the targeted tasks often remain the same within the individual classes of models. Relying solely on the ability of modern models to extract information from unstructured code does not take advantage of 70 years of programming language and compiler development by not utilizing the structure inherent to programs in the data collection. This detracts from the performance of models working over a tokenized representation of input code and precludes the use of these models in the compiler itself. To work towards the first intermediate representation (IR) based models, we fully utilize the LLVM compiler infrastructure, shared by a number of languages, to generate a 182B token dataset of LLVM IR. We generated this dataset from programming languages built on the shared LLVM infrastructure, including Rust, Swift, Julia, and C/C++, by hooking into LLVM code generation either through the language's package manager or the compiler directly to extract the dataset of intermediate representations from production grade programs. Statistical analysis proves the utility of our dataset not only for large language model training, but also for the introspection into the code generation process itself with the dataset showing great promise for machine-learned compiler components.",
      "paperUrl": "https://arxiv.org/pdf/2309.15432",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2309.15432",
      "tags": [
        "C++",
        "Infrastructure",
        "IR",
        "Performance",
        "Programming Languages",
        "Rust",
        "Swift"
      ],
      "matchedAuthors": [
        "Aiden Grossman",
        "Johannes Doerfert",
        "Jose M Monsalve Diaz",
        "Konstantinos Parasyris",
        "Ludger Paehler",
        "Mircea Trofin",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4321496380",
      "source": "openalex-discovery",
      "title": "Code Generation for In-Place Stencils",
      "authors": [
        {
          "name": "Mohamed Essadki",
          "affiliation": "Office National d'Études et de Recherches Aérospatiales"
        },
        {
          "name": "Bertrand Michel",
          "affiliation": "Office National d'Études et de Recherches Aérospatiales"
        },
        {
          "name": "Bruno Maugars",
          "affiliation": "Office National d'Études et de Recherches Aérospatiales"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        },
        {
          "name": "Nicolas Vasilache",
          "affiliation": "Google (Switzerland)"
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Numerical simulation often resorts to iterative in-place stencils such as the Gauss-Seidel or Successive Overrelaxation (SOR) methods. Writing high performance implementations of such stencils requires significant effort and time; it also involves non-local transformations beyond the stencil kernel itself. While automated code generation is a mature technology for image processing stencils, convolutions and out-of-place iterative stencils (such as the Jacobi method), the optimization of in-place stencils requires manual craftsmanship. Building on recent advances in tensor compiler construction, we propose the first domain-specific code generator for iterative in-place stencils. Starting from a generic tensor compiler implemented in the MLIR framework, tensor abstractions are incrementally refined and lowered down to parallel, tiled, fused and vectorized code. We used our generator to implement a realistic, implicit solver for structured meshes, and demonstrate results competitive with an industrial computational fluid dynamics framework. We also compare with stand-alone stencil kernels for dense tensors.",
      "paperUrl": "https://doi.org/10.1145/3579990.3580006",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Oleksandr Zinenko"
      ]
    },
    {
      "id": "openalex-w4385585421",
      "source": "openalex-discovery",
      "title": "Clever DAE",
      "authors": [
        {
          "name": "Michele Scuttari",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Nicola Camillucci",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Daniele Cattaneo",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Giovanni Agosta",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Francesco Casella",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Stefano Cherubin",
          "affiliation": "Edinburgh Napier University"
        },
        {
          "name": "Federico Terraneo",
          "affiliation": "Politecnico di Milano"
        }
      ],
      "year": "2023",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modeling and simulation are fundamental activities in engineering to facilitate prototyping, verification and maintenance. Declarative modeling languages allow to simulate physical phenomena by expressing them in terms of Differential and Algebraic Equations (DAE) systems. In this paper, we focus on the problem of generating code for performing the numerical integration of the model equations, and in particular on the overhead introduced by external numerical solver libraries. We propose a novel methodology for minimizing the amount of equations which require to be solved through an external solver library, together with the number of computations that are required to computed the Jacobian matrix of the system. Through a prototype LLVM-based compiler, we demonstrate how this approach achieves a linear speed-up in simulation time with respect to the baseline.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3587135.3589945",
      "sourceUrl": "https://doi.org/10.1145/3587135.3589945",
      "tags": [
        "Libraries"
      ],
      "matchedAuthors": [
        "Michele Scuttari"
      ]
    },
    {
      "id": "openalex-w2962766833",
      "source": "openalex-discovery",
      "title": "Challenges and Opportunities in C/C++ Source-To-Source Compilation (Invited Paper)",
      "authors": [
        {
          "name": "Christophe Denis",
          "affiliation": "École Normale Supérieure Paris-Saclay"
        },
        {
          "name": "Pablo de Oliveira Castro",
          "affiliation": "Universidade do Porto"
        },
        {
          "name": "Eric Petit",
          "affiliation": "Université de Versailles Saint-Quentin-en-Yvelines"
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The C/C++ compilation stack (Intermediate Representations (IR), compilation passes and backends) is encumbered by a steep learning curve, which we believe can be lowered by complementing it with approaches such as source-to-source compilation. Source-to-source compilation is a technology that is widely used and quite mature in certain programming environments, such as JavaScript, but that faces a low adoption rate in others. In the particular case of C and C++ some of the identified factors include the high complexity of the languages, increased difficulty in building and maintaining C/C++ parsers, or limitations on using source code as an intermediate representation. Additionally, new technologies such as Multi-Level Intermediate Representation (MLIR) have appeared as potential competitors to source-to-source compilers at this level. In this paper, we present what we have identified as current challenges of source-to-source compilation of C and C++, as well as what we consider to be opportunities and possible directions forward. We also present several examples, implemented on top of the Clava source-to-source compiler, that use some of these ideas and techniques to raise the abstraction level of compiler research on complex compiled languages such as C or C++. The examples include automatic parallelization of for loops, high-level synthesis optimisation, hardware/software partitioning with run-time decisions, and automatic insertion of inline assembly for fast prototyping of custom instructions.",
      "paperUrl": "https://arxiv.org/pdf/1509.01347",
      "sourceUrl": "https://doi.org/10.4230/oasics.parma-ditam.2023.2",
      "tags": [
        "Backend",
        "C++",
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Christophe Denis",
        "Eric Petit",
        "Pablo de Oliveira Castro"
      ]
    },
    {
      "id": "openalex-w4362704755",
      "source": "openalex-discovery",
      "title": "Automatic Differentiation of Binned Likelihoods With Roofit and Clad",
      "authors": [
        {
          "name": "Garima Singh",
          "affiliation": ""
        },
        {
          "name": "Jonas Rembser",
          "affiliation": ""
        },
        {
          "name": "L. Moneta",
          "affiliation": ""
        },
        {
          "name": "David Lange",
          "affiliation": ""
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "RooFit is a toolkit for statistical modeling and fitting used by most experiments in particle physics. Just as data sets from next-generation experiments grow, processing requirements for physics analysis become more computationally demanding, necessitating performance optimizations for RooFit. One possibility to speed-up minimization and add stability is the use of Automatic Differentiation (AD). Unlike for numerical differentiation, the computation cost scales linearly with the number of parameters, making AD particularly appealing for statistical models with many parameters. In this paper, we report on one possible way to implement AD in RooFit. Our approach is to add a facility to generate C++ code for a full RooFit model automatically. Unlike the original RooFit model, this generated code is free of virtual function calls and other RooFit-specific overhead. In particular, this code is then used to produce the gradient automatically with Clad. Clad is a source transformation AD tool implemented as a plugin to the clang compiler, which automatically generates the derivative code for input C++ functions. We show results demonstrating the improvements observed when applying this code generation strategy to HistFactory and other commonly used RooFit models. HistFactory is the subcomponent of RooFit that implements binned likelihood models with probability densities based on histogram templates. These models frequently have a very large number of free parameters and are thus an interesting first target for AD support in RooFit.",
      "paperUrl": "https://arxiv.org/pdf/2304.02650",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2304.02650",
      "tags": [
        "C++",
        "Clang",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w4361764778",
      "source": "openalex-discovery",
      "title": "An Intermediate Language for General Sparse Format Customization",
      "authors": [
        {
          "name": "Jie Liu",
          "affiliation": "Cornell University"
        },
        {
          "name": "Zhongyuan Zhao",
          "affiliation": "Cornell University"
        },
        {
          "name": "Zijian Ding",
          "affiliation": "University of California, Los Angeles"
        },
        {
          "name": "Benjamin Brock",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Hongbo Rong",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Zhiru Zhang",
          "affiliation": "Cornell University"
        }
      ],
      "year": "2023",
      "venue": "IEEE Computer Architecture Letters | Vol. 22 (Issue 2)",
      "type": "research-paper",
      "abstract": "The inevitable trend of hardware specialization drives an increasing use of custom data formats in processing sparse workloads, which are typically memory-bound. These formats facilitate the automated generation of target-aware data layouts to improve memory access latency and bandwidth utilization. However, existing sparse tensor programming models and compilers offer little or no support for productively customizing the sparse formats. Moreover, since these frameworks adopt an attribute-based approach for format abstraction, they cannot easily be extended to support general format customization. To overcome this deficiency, we propose UniSparse, an intermediate language that provides a unified abstraction for representing and customizing sparse formats. We also develop a compiler leveraging the MLIR infrastructure, which supports adaptive customization of formats. We demonstrate the efficacy of our approach through experiments running commonly-used sparse linear algebra operations with hybrid formats on multiple different hardware targets, including an Intel CPU, an NVIDIA GPU, and a simulated processing-in-memory (PIM) device.",
      "paperUrl": "http://dx.doi.org/10.1109/lca.2023.3262610",
      "sourceUrl": "https://doi.org/10.1109/lca.2023.3262610",
      "tags": [
        "GPU",
        "Infrastructure",
        "MLIR"
      ],
      "matchedAuthors": [
        "Zhiru Zhang"
      ]
    },
    {
      "id": "openalex-w4394586852",
      "source": "openalex-discovery",
      "title": "An Empirical Evaluation of PTE Coalescing",
      "authors": [
        {
          "name": "Eliot H. Solomon",
          "affiliation": "Rice University"
        },
        {
          "name": "Yufeng Zhou",
          "affiliation": "Rice University"
        },
        {
          "name": "Alan L. Cox",
          "affiliation": "Rice University"
        }
      ],
      "year": "2023",
      "venue": "Proceedings of the International Symposium on Memory Systems | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Superpages (also known as huge pages) are an effective technique for reducing the latency of virtual-to-physical address translation on modern processors. However, the large size of the 2 MB and 1 GB superpages supported by x86-64 processors continues to present a challenge to the operating system's ability to form superpages, given the mandatory contiguity, alignment, and attribute requirements of a superpage. Recent work proposes medium-sized superpages as a potential solution, by allowing the creation of smaller superpages where 2 MB and larger superpages have not formed or will not be possible to form. Notably, AMD processors starting with the Zen microarchitecture have offered a \"PTE Coalescing\" feature where the hardware opportunistically and transparently creates, from underlying consecutive and aligned 4 KB mappings in the page table, 16 KB or 32 KB mappings to be cached in the TLB. On the surface, this feature requires no modifications to the operating system or the compiler toolchain, exploiting only coincidental contiguity and alignment. Nonetheless, there are ways that either the operating system or the toolchain can be made coalescing-aware and hence make better use of PTE Coalescing. This paper first investigates undocumented aspects of PTE Coalescing, and then evaluates some operating system and toolchain optimizations which explicitly take advantage of it. We find that an operating system that is coalescing-friendly reduces L1 ITLB misses by 50%-80% compared to an operating system that is coalescing-unaware. For a Clang compilation workload, a coalescing-friendly operating system coupled with PTE Coalescing all but eliminates L2 ITLB misses. Last but not least, we evaluate the impact of granularity (16 KB vs 32 KB) on the effectiveness of PTE Coalescing. We find that reducing the coalescing granularity from 32 KB to 16 KB leads to a 1.3x-20.5x reduction in 4 KB L2 DTLB misses in a wide variety of workloads.",
      "paperUrl": "https://doi.org/10.1145/3631882.3631902",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Alan L. Cox"
      ]
    },
    {
      "id": "openalex-w4390215346",
      "source": "openalex-discovery",
      "title": "AXI4MLIR: User-Driven Automatic Host Code Generation for Custom AXI-Based Accelerators",
      "authors": [
        {
          "name": "Nícolas Bohm Agostini",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Jude Haris",
          "affiliation": "University of Glasgow"
        },
        {
          "name": "Perry Gibson",
          "affiliation": "University of Glasgow"
        },
        {
          "name": "Malith Jayaweera",
          "affiliation": "Northeastern University"
        },
        {
          "name": "Norm Rubin",
          "affiliation": "Northeastern University"
        },
        {
          "name": "Antonino Tumeo",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "José Luis Abellán",
          "affiliation": "Universidad de Murcia"
        },
        {
          "name": "José Cano",
          "affiliation": "University of Glasgow"
        },
        {
          "name": "David Kaeli",
          "affiliation": "Northeastern University"
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper addresses the need for automatic and efficient generation of host driver code for arbitrary custom AXI-based accelerators targeting linear algebra algorithms, an important workload in various applications, including machine learning and scientific computing. While existing tools have focused on automating accelerator prototyping, little attention has been paid to the host-accelerator interaction. This paper introduces AXI4MLIR, an extension of the MLIR compiler framework designed to facilitate the automated generation of host-accelerator driver code. With new MLIR attributes and transformations, AXI4MLIR empowers users to specify accelerator features (including their instructions) and communication patterns and exploit the host memory hierarchy. We demonstrate AXI4MLIR's versatility across different types of accelerators and problems, showcasing significant CPU cache reference reductions (up to 56%) and up to a 1.65x speedup compared to manually optimized driver code implementations. AXI4MLIR implementation is open-source and available at: https://github.com/AXI4MLIR/axi4mlir.",
      "paperUrl": "https://arxiv.org/pdf/2312.14821",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2312.14821",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "David Kaeli"
      ]
    },
    {
      "id": "openalex-w4389921727",
      "source": "openalex-discovery",
      "title": "ACPO: AI-Enabled Compiler Framework",
      "authors": [
        {
          "name": "Amir H. Ashouri",
          "affiliation": ""
        },
        {
          "name": "Muhammad Asif Manzoor",
          "affiliation": ""
        },
        {
          "name": "Duc Minh Vu",
          "affiliation": ""
        },
        {
          "name": "Raymond Zhang",
          "affiliation": ""
        },
        {
          "name": "Ziwen Wang",
          "affiliation": ""
        },
        {
          "name": "Angel Zhang",
          "affiliation": ""
        },
        {
          "name": "Bryan Chan",
          "affiliation": ""
        },
        {
          "name": "Tomasz Czajkowski",
          "affiliation": ""
        },
        {
          "name": "Yaoqing Gao",
          "affiliation": ""
        },
        {
          "name": "Gao, Yaoqing",
          "affiliation": ""
        }
      ],
      "year": "2023",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework. This paper presents ACPO: An AI-Enabled Compiler Framework, a novel framework that provides LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate \\taco{a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes used in LLVM's O3. and finally, describe how ACPO can be leveraged to optimize other passes. Experimental results reveal that the ACPO model for Loop Unroll can gain on average 4%, 3%, 5.4%, and 0.2% compared to LLVM's vanilla O3 optimization when deployed on Polybench, Coral-2, CoreMark, and Graph-500, respectively. Furthermore, by including both Function Inlining and Loop Unroll models, ACPO can provide a combined speedup of 4.5% on Polybench and 2.4% on Cbench when compared with LLVM's O3, respectively.",
      "paperUrl": "https://arxiv.org/pdf/2312.09982",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2312.09982",
      "tags": [
        "AI",
        "ML",
        "Performance"
      ],
      "matchedAuthors": [
        "Bryan Chan"
      ]
    },
    {
      "id": "openalex-w4389571996",
      "source": "openalex-discovery",
      "title": "A Smart Status Based Monitoring Algorithm for the Dynamic Analysis of Memory Safety",
      "authors": [
        {
          "name": "Zhe Chen",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Rui Yan",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Yingzi Ma",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Yulei Sui",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2023",
      "venue": "ACM Transactions on Software Engineering and Methodology | Vol. 33 (Issue 4)",
      "type": "research-paper",
      "abstract": "C is a dominant programming language for implementing system and low-level embedded software. Unfortunately, the unsafe nature of its low-level control of memory often leads to memory errors. Dynamic analysis has been widely used to detect memory errors at runtime. However, existing monitoring algorithms for dynamic analysis are not yet satisfactory, as they cannot deterministically and completely detect some types of errors, such as segment confusion errors, sub-object overflows, use-after-frees and memory leaks. We propose a new monitoring algorithm, namely Smatus , short for smart status , that improves memory safety by performing comprehensive dynamic analysis. The key innovation is to maintain at runtime a small status node for each memory object. A status node records the status value and reference count of an object, where the status value denotes the liveness and segment type of this object, and the reference count tracks the number of pointer variables pointing to this object. Smatus maintains at runtime a pointer metadata for each pointer variable, to record not only the base and bound of a pointer’s referent but also the address of the referent’s status node. All the pointers pointing to the same referent share the same status node in their pointer metadata. A status node is smart in the sense that it is automatically deleted when it becomes useless (indicated by its reference count reaching zero). To the best of our knowledge, Smatus represents the most comprehensive approach of its kind. We have evaluated Smatus by using a large set of programs including the NIST Software Assurance Reference Dataset, MSBench, MiBench, SPEC and stress testing benchmarks. In terms of effectiveness (detecting different types of memory errors), Smatus outperforms state-of-the-art tools, Google’s AddressSanitizer, SoftBoundCETS and Valgrind, as it is capable of detecting more errors. In terms of performance (the time and memory overheads), Smatus outperforms SoftBoundCETS and Valgrind in terms of both lower time and memory overheads incurred, and is on par with AddressSanitizer in terms of the time and memory overhead tradeoff made (with much lower memory overheads incurred).",
      "paperUrl": "https://doi.org/10.1145/3637227",
      "sourceUrl": "",
      "tags": [
        "Dynamic Analysis",
        "Embedded",
        "Performance",
        "Testing"
      ],
      "matchedAuthors": [
        "Jingling Xue",
        "Yulei Sui"
      ]
    },
    {
      "id": "openalex-w4321383480",
      "source": "openalex-discovery",
      "title": "datAFLow: Towards a Data-Flow-Guided Fuzzer",
      "authors": [
        {
          "name": "Adrian Herrera",
          "affiliation": ""
        },
        {
          "name": "Mathias Payer",
          "affiliation": ""
        },
        {
          "name": "Antony L. Hosking",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "program analyses to model program and input structure, and continuously gather dynamic information about the target.Leveraging dynamic information drives fuzzer efficiency.For example, coverage-guided greybox fuzzers-perhaps the most widely-used class of fuzzer-track code paths executed by the target. 1 This allows the fuzzer to focus its mutations on inputs reaching new code.Intuitively, a fuzzer cannot find bugs in code never executed, so maximizing the amount of code executed should maximize the number of bugs found.Code coverage serves as an approximation of program behavior, and expanding code coverage implies exploring program behaviors.Coverage-guided greybox fuzzers are now pervasive.Their success [2] can be attributed to one fuzzer in particular: American Fuzzy Lop (AFL) [3].AFL is a greybox fuzzer that uses lightweight instrumentation to track edges covered in the target's control-flow graph (CFG).A large body of research has built on AFL [4][5][6][7][8][9][10][11][12].While improvements have been made, most fuzzers still default to edge coverage as an approximation of program behavior.Is this the best we can do?In some targets, control flow offers only a coarse-grained approximation of program behavior.This includes targets whose control structure is decoupled from its semantics (e.g., LR parsers generated by yacc) [13].Such targets require data-flow coverage [13][14][15][16][17]. Whereas control flow focuses on the order of operations in a program (i.e., branch and loop structures), data flow instead focuses on how variables (i.e., data) are defined and used [14]: indeed, there may be no control dependence between variable definition and use sites (see §III for details).In fuzzing, data flow typically takes the form of dynamic taint analysis (DTA).Here, the target's input data is tainted at its definition site and tracked as it is accessed and used at runtime.Unfortunately, accurate DTA is difficult to achieve and expensive to compute (e.g., prior work has found DTA is expensive [18,19] and its accuracy highly variable across implementations [18,20]).Moreover, several real-world programs fail to compile under DTA, increasing deployability concerns.Thus, most widely-deployed greybox fuzzers (e.g., AFL [3], libFuzzer [21], and honggfuzz [22]) eschew DTA in favor of higher fuzzing throughput.While lightweight alternatives to DTA exist (e.g., REDQUEEN [23], GREYONE [19]), the full potential of control-vs.data-flow based fuzzer coverage metrics have not yet been thoroughly explored.To support this exploration, we 1 Miller et al.'s original fuzzer [1] is now known as a blackbox fuzzer, because it has no knowledge of the target's internals.",
      "paperUrl": "https://doi.org/10.14722/fuzzing.2022.23001",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w4220686429",
      "source": "openalex-discovery",
      "title": "Weaving Synchronous Reactions into the Fabric of SSA-form Compilers",
      "authors": [
        {
          "name": "Hugo Pompougnac",
          "affiliation": "Institut national de recherche en informatique et en automatique"
        },
        {
          "name": "Ulysse Beaugnon",
          "affiliation": ""
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        },
        {
          "name": "Dumitru Potop Butucaru",
          "affiliation": "Institut national de recherche en informatique et en automatique"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 19 (Issue 2)",
      "type": "research-paper",
      "abstract": "We investigate the programming of reactive systems combining closed-loop control with performance-intensive components such as Machine Learning (ML). Reactive control systems are often safety-critical and associated with real-time execution requirements, a domain of predilection for synchronous programming languages. Extending the high levels of assurance found in reactive control systems to computationally intensive code remains an open issue. We tackle it by unifying concepts and algorithms from synchronous languages with abstractions commonly found in general-purpose and ML compilers. This unification across embedded and high-performance computing enables a high degree of reuse of compiler abstractions and code. We first recall commonalities between dataflow synchronous languages and the static single assignment (SSA) form of general-purpose/ML compilers. We highlight the key mechanisms of synchronous languages that SSA does not cover—denotational concepts such as synchronizing computations with an external time base, cyclic and reactive I/O, as well as the operational notions of relaxing control flow dominance and the modeling of absent values. We discover that initialization-related static analyses and code generation aspects can be fully decoupled from other aspects of synchronous semantics such as memory management and causality analysis, the latter being covered by existing dominance-based algorithms of SSA-form compilers. We show how the SSA form can be seamlessly extended to enable all SSA-based transformations and optimizations on reactive programs with synchronous concurrency. We derive a compilation flow suitable for both high-performance and reactive aspects of a control application, by embedding the Lustre dataflow synchronous language into the SSA-based MLIR/LLVM compiler infrastructure. This allows the modeling of signal processing and deep neural network inference in the (closed) loop of feedback-directed control systems. With only minor efforts leveraging the MLIR infrastructure, the generated code matches or outperforms state-of-the-art synchronous language compilers on computationally intensive ML applications.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3506706",
      "sourceUrl": "https://doi.org/10.1145/3506706",
      "tags": [
        "Embedded",
        "Infrastructure",
        "ML",
        "MLIR",
        "Optimizations",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Albert Cohen"
      ]
    },
    {
      "id": "openalex-w4282978695",
      "source": "openalex-discovery",
      "title": "VICO",
      "authors": [
        {
          "name": "Sharjeel Khan",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Bodhisatwa Chatterjee",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Santosh Pande",
          "affiliation": "Georgia Institute of Technology"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In spite of tremendous advances in data dependence and dataflow analysis techniques, state-of-the-art optimizing compilers continue to suffer from imprecisions and miss potential optimization opportunities. These imprecisions result from statically unknown characteristics of variables that participate in the dependence systems, or aliases that affect key safety properties, which must be conservatively assumed. However, with the increased tractability of verification on modern systems, a demand-driven solution to this problem can be envisioned. In this work, we model loop optimization constraints as loop-invariants, with the goal of proving their runtime behaviour under all inputs. Our proposed framework VICO, first detects the unresolved constraints whose conservative assumption negatively affects specific compiler optimizations. These constraints are then modeled as potential invariants and are verified on a demand-driven basis. Finally, VICO incorporates the verified invariants in the analysis, which results in superior optimization. For this purpose, VICO converts conservative constraints identified by the LLVM compiler and parallelization tool PLuTo, to potential invariants, that are further verified by SMACK verification tool. Following such an approach enables us to target numerous optimizations at different compilation phases - automatic parallelization and loop transformations at the source-level, and register allocation, and global value numbering (GVN), at the IR-level. Our results show that VICO improves the precision of dependence analysis by 45% in real-world cases, leading to superior optimization in over 75 loops in different scenarios like mathematical simulations and solvers. The improvement in dependence precision led to an average speedup of 14.7x on Apple M1 Pro and 6.07x on Intel Xeon E5-2660 systems. In addition, VICO also enhances LLVM's alias analysis leading to improvements in LLVM backend optimizations and decreased code size by 4% alongside improved execution time by 2.2% in numerous linux programs and SPEC benchmarks with (mostly) low verification time.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3524059.3532393",
      "sourceUrl": "https://doi.org/10.1145/3524059.3532393",
      "tags": [
        "Backend",
        "IR",
        "Loop transformations",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Santosh Pande",
        "Sharjeel Khan"
      ]
    },
    {
      "id": "openalex-w4214617101",
      "source": "openalex-discovery",
      "title": "Understanding and exploiting optimal function inlining",
      "authors": [
        {
          "name": "Theodoros Theodoridis",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Zhendong Su",
          "affiliation": "ETH Zurich"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Inlining is a core transformation in optimizing compilers. It replaces a function call (call site) with the body of the called function (callee). It helps reduce function call overhead and binary size, and more importantly, enables other optimizations. The problem of inlining has been extensively studied, but it is far from being solved; predicting which inlining decisions are beneficial is nontrivial due to interactions with the rest of the compiler pipeline. Previous work has mainly focused on designing heuristics for better inlining decisions and has not investigated optimal inlining, i.e., exhaustively finding the optimal inlining decisions. Optimal inlining is necessary for identifying and exploiting missed opportunities and evaluating the state of the art. This paper fills this gap through an extensive empirical analysis of optimal inlining using the SPEC2017 benchmark suite. Our novel formulation drastically reduces the inlining search space size (from 2349 down to 225) and allows us to exhaustively evaluate all inlining choices on 1,135 SPEC2017 files. We show a significant gap between the state-of-the-art strategy in LLVM and optimal inlining when optimizing for binary size, an important, deterministic metric independent of workload (in contrast to performance, another important metric). Inspired by our analysis, we introduce a simple, effective autotuning strategy for inlining that outperforms the state of the art by 7% on average (and up to 28%) on SPEC2017, 15% on the source code of LLVM itself, and 10% on the source code of SQLite. This work highlights the importance of exploring optimal inlining by providing new, actionable insight and an effective autotuning strategy that is of practical utility.",
      "paperUrl": "https://www.research.ed.ac.uk/en/publications/cd4d1fd3-1c31-4ead-90ec-a56c346232e9",
      "sourceUrl": "https://doi.org/10.1145/3503222.3507744",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Theodoros Theodoridis",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4312573956",
      "source": "openalex-discovery",
      "title": "Tutorial: LLVM for Security Practitioners",
      "authors": [
        {
          "name": "John Criswell",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Ethan Johnson",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Colin Pronovost",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Many security researchers need to build tools that analyze and transform code. For example, researchers may want to build security hardening tools, tools that find vulnerabilities within software, or tools that prove that a program is invulnerable to attack. This tutorial will guide attendees through creating extensions to the LLVM compiler that perform simple analysis and transformation operations.",
      "paperUrl": "https://doi.org/10.1109/secdev53368.2022.00012",
      "sourceUrl": "",
      "tags": [
        "Security"
      ],
      "matchedAuthors": [
        "Ethan Johnson",
        "John Criswell"
      ]
    },
    {
      "id": "openalex-w4281681808",
      "source": "openalex-discovery",
      "title": "TinyIREE: An ML Execution Environment for Embedded Systems from Compilation to Deployment",
      "authors": [
        {
          "name": "Hsin-I Cindy Liu",
          "affiliation": ""
        },
        {
          "name": "Marius Brehler",
          "affiliation": ""
        },
        {
          "name": "M. Ravishankar",
          "affiliation": ""
        },
        {
          "name": "Nicolas Vasilache",
          "affiliation": ""
        },
        {
          "name": "Ben Vanik",
          "affiliation": ""
        },
        {
          "name": "Stella Laurenzo",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Machine learning model deployment for training and execution has been an important topic for industry and academic research in the last decade. Much of the attention has been focused on developing specific toolchains to support acceleration hardware. In this paper, we present IREE, a unified compiler and runtime stack with the explicit goal to scale down machine learning programs to the smallest footprints for mobile and edge devices, while maintaining the ability to scale up to larger deployment targets. IREE adopts a compiler-based approach and optimizes for heterogeneous hardware accelerators through the use of the MLIR compiler infrastructure which provides the means to quickly design and implement multi-level compiler intermediate representations (IR). More specifically, this paper is focused on TinyIREE, which is a set of deployment options in IREE that accommodate the limited memory and computation resources in embedded systems and bare-metal platforms, while also demonstrating IREE's intuitive workflow that generates workloads for different ISA extensions and ABIs through LLVM.",
      "paperUrl": "https://arxiv.org/pdf/2205.14479",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2205.14479",
      "tags": [
        "Embedded",
        "Infrastructure",
        "IR",
        "ML",
        "MLIR"
      ],
      "matchedAuthors": [
        "Marius Brehler",
        "Stella Laurenzo"
      ]
    },
    {
      "id": "openalex-w4226422324",
      "source": "openalex-discovery",
      "title": "The Taming of the Stack: Isolating Stack Data from Memory Errors",
      "authors": [
        {
          "name": "Kaiming Huang",
          "affiliation": "Pennsylvania State University"
        },
        {
          "name": "Yongzhe Huang",
          "affiliation": ""
        },
        {
          "name": "Mathias Payer",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Zhiyun Qian",
          "affiliation": "University of California, Riverside"
        },
        {
          "name": "Jack Sampson",
          "affiliation": "Pennsylvania State University"
        },
        {
          "name": "Gang Tan",
          "affiliation": ""
        },
        {
          "name": "Trent Jaeger",
          "affiliation": "Pennsylvania State University"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Despite vast research on defenses to protect stack objects from the exploitation of memory errors, much stack data remains at risk. Historically, stack defenses focus on the protection of code pointers, such as return addresses, but emerging techniques to exploit memory errors motivate the need for practical solutions to protect stack data objects as well. However, recent approaches provide an incomplete view of security by not accounting for memory errors comprehensively and by limiting the set of objects that can be protected unnecessarily. In this paper, we present the DATAGUARD system that identifies which stack objects are safe statically from spatial, type, and temporal memory errors to protect those objects efficiently. DATAGUARD improves security through a more comprehensive and accurate safety analysis that proves a larger number of stack objects are safe from memory errors, while ensuring that no unsafe stack objects are mistakenly classified as safe. DATAGUARD's analysis of server programs and the SPEC CPU2006 benchmark suite shows that DATAGUARD improves security by: (1) ensuring that no memory safety violations are possible for any stack objects classified as safe, removing 6.3% of the stack objects previously classified safe by the Safe Stack method, and (2) blocking exploit of all 118 stack vulnerabilities in the CGC Binaries. DATAGUARD extends the scope of stack protection by validating as safe over 70% of the stack objects classified as unsafe by the Safe Stack method, leading to an average of 91.45% of all stack objects that can only be referenced safely. By identifying more functions with only safe stack objects, DATAGUARD reduces the overhead of using Clang's Safe Stack defense for protection of the SPEC CPU2006 benchmarks from 11.3% to 4.3%. Thus, DATAGUARD shows that a comprehensive and accurate analysis can both increase the scope of stack data protection and reduce overheads.",
      "paperUrl": "https://doi.org/10.14722/ndss.2022.23060",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Security"
      ],
      "matchedAuthors": [
        "Gang Tan",
        "Jack Sampson",
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w4320067873",
      "source": "openalex-discovery",
      "title": "The Support of MLIR HLS Adaptor for LLVM IR",
      "authors": [
        {
          "name": "Geng-Ming Liang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chuan-Yue Yuan",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Meng-Shiun Yu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Tai-Liang Chen",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Kuan-Hsun Chen",
          "affiliation": ""
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Since the emergence of MLIR, High-level Synthesis (HLS) tools started to design in multi-level abstractions. Unlike the traditional HLS tools that are based on a single abstraction (e.g. LLVM), optimizations in different levels of abstraction could benefit from cross-layer optimizations to get better results. Although current HLS tools with MLIR can generate HLS C/C++ to do synthesis, we believe that a direct IR transformation from MLIR to LLVM will keep more expression details. In this paper, we propose an adaptor for LLVM IR, which can optimize the IR, generated from MLIR, into HLS readable IR. Without the gap of unsupported syntax between different versions, developers could focus on their specialization. Our preliminary results show that the MLIR flow via our adaptor can generate comparable performance results with the version by MLIR HLS tools generating HLS C++ codes. The experiment is performed with Xilinx Vitis and HLS tools.",
      "paperUrl": "https://doi.org/10.1145/3547276.3548515",
      "sourceUrl": "",
      "tags": [
        "C++",
        "IR",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4282601909",
      "source": "openalex-discovery",
      "title": "Summary of Model Checking C++ Programs",
      "authors": [
        {
          "name": "Felipe R. Monteiro",
          "affiliation": "Universidade Federal do Amazonas"
        },
        {
          "name": "Mikhail R. Gadelha",
          "affiliation": "Instituto Geográfico Nacional"
        },
        {
          "name": "Lucas C. Cordeiro",
          "affiliation": "University of Manchester"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This is an extended abstract of the article \"Model Checking C++ Programs\" by Felipe R. Monteiro, Mikhail R. Gadelha, and Lucas C. Cordeiro published at the journal of Software Testing, Verification and Reliability. We describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ programs. Our verification approach analyses bounded C++ programs by encoding into SMT various sophisticated features that the C++ programming language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard Template Libraries. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state-of-the-art verifiers to check C++ programs directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ programs, presenting a higher number of correct verification results. Additionally, ESBMC has been applied to a commercial C++ application in the telecommunication domain and successfully detected arithmetic-overflow errors, which could lead to security vulnerabilities.",
      "paperUrl": "https://doi.org/10.1109/icst53961.2022.00054",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Libraries",
        "Security",
        "Testing"
      ],
      "matchedAuthors": [
        "Mikhail R. Gadelha"
      ]
    },
    {
      "id": "openalex-w3211485653",
      "source": "openalex-discovery",
      "title": "ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation",
      "authors": [
        {
          "name": "Hanchen Ye",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Cong Hao",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Jianyi Cheng",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Hyunmin Jeong",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Jack Huang",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": "Xilinx (United States)"
        },
        {
          "name": "Deming Chen",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-level synthesis (HLS) has been widely adopted as it significantly improves the hardware design productivity and enables efficient design space exploration (DSE). Existing HLS tools are built using compiler infrastructures largely based on a single-level abstraction, such as LLVM. How-ever, as HLS designs typically come with intrinsic structural or functional hierarchies, different HLS optimization problems are often better solved with different levels of abstractions. This paper proposes ScaleHLS <sup>1</sup>, a new scalable and customizable HLS framework, on top of a multi-level compiler infrastructure called MLIR. ScaleHLS represents HLS designs at multiple representation levels and provides an HLS-dedicated analysis and transform library to solve the optimization problems at the suitable levels. Using this library, we provide a DSE engine to generate optimized HLS designs automatically. In addition, we develop an HLS C front-end and a C/C++ emission back-end to translate HLS designs into/from MLIR for enabling an end-to-end compilation flow. Experimental results show that, comparing to the baseline designs without manual directives insertion and code-rewriting, that are only optimized by Xilinx Vivado HLS, ScaleHLS improves the performances with amazing quality-of-results &#x2013; up to 768.1&#x00D7; better on computation kernel level programs and up to 3825.0&#x00D7; better on neural network models.",
      "paperUrl": "https://doi.org/10.1109/hpca53966.2022.00060",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Infrastructure",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w4321636603",
      "source": "openalex-discovery",
      "title": "Scalable Automatic Differentiation of Multiple Parallel Paradigms through Compiler Augmentation",
      "authors": [
        {
          "name": "William S. Moses",
          "affiliation": "IIT@MIT"
        },
        {
          "name": "Sri Hari Krishna Narayanan",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Ludger Paehler",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Valentin Churavy",
          "affiliation": "IIT@MIT"
        },
        {
          "name": "Michel Schanen",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Jan Hückelheim",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Paul Hovland",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Derivatives are key to numerous science, engineering, and machine learning applications. While existing tools generate derivatives of programs in a single language, modern parallel applications combine a set of frameworks and languages to leverage available performance and function in an evolving hardware landscape. We propose a scheme for differentiating arbitrary DAG-based parallelism that preserves scalability and efficiency, implemented into the LLVM-based Enzyme automatic differentiation framework. By integrating with a full-fledged compiler backend, Enzyme can differentiate numerous parallel frameworks and directly control code generation. Combined with its ability to differentiate any LLVM-based language, this flexibility permits Enzyme to leverage the compiler tool chain for parallel and differentiation-specitic optimizations. We differentiate nine distinct versions of the LULESH and miniBUDE applications, written in different programming languages (C++, Julia) and parallel frameworks (OpenMP, MPI, RAJA, Julia tasks, MPI.jl), demonstrating similar scalability to the original program. On benchmarks with 64 threads or nodes, we find a differentiation overhead of 3.4–6.8× on C++ and 5.4–12.5× on Julia.",
      "paperUrl": "https://doi.org/10.1109/sc41404.2022.00065",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "C++",
        "Optimizations",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Johannes Doerfert",
        "Ludger Paehler",
        "Valentin Churavy",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4225291709",
      "source": "openalex-discovery",
      "title": "SYCLops: A SYCL Specific LLVM to MLIR Converter",
      "authors": [
        {
          "name": "Alexandre Singer",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Frank Gao",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Kai-Ting Amy Wang",
          "affiliation": "Huawei Technologies (Canada)"
        }
      ],
      "year": "2022",
      "venue": "International Workshop on OpenCL | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "There is a growing need for higher level abstractions for device kernels in heterogeneous environments, and the multi-level nature of the MLIR infrastructure perfectly addresses this requirement. As SYCL begins to gain industry adoption for heterogeneous applications and MLIR continues to develop, we present SYCLops: a converter capable of translating SYCL specific LLVM IR to MLIR. This will allow for both target and application specific optimizations within the same framework to exploit opportunities for improvement present at different levels.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3529538.3529992",
      "sourceUrl": "https://doi.org/10.1145/3529538.3529992",
      "tags": [
        "Infrastructure",
        "IR",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Kai-Ting Amy Wang"
      ]
    },
    {
      "id": "openalex-w4289874447",
      "source": "openalex-discovery",
      "title": "SMT-Based Translation Validation for Machine Learning Compiler",
      "authors": [
        {
          "name": "Seongwon Bang",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Nam SeungHyeon",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Inwhan Chun",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Ho Young Jhoo",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Juneyoung Lee",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract Machine learning compilers are large software containing complex transformations for deep learning models, and any buggy transformation may cause a crash or silently bring a regression to the prediction accuracy and performance. This paper proposes an SMT-based translation validation framework for Multi-Level IR (MLIR), a compiler framework used by many deep learning compilers. It proposes an SMT encoding tailored for translation validation that is an over-approximation of the FP arithmetic and reduction operations. It performs abstraction refinement if validation fails. We also propose a new approach for encoding arithmetic properties of reductions in SMT. We found mismatches between the specification and implementation of MLIR, and validated high-level transformations for , , and with proper splitting.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-031-13188-2_19.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-031-13188-2_19",
      "tags": [
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Juneyoung Lee"
      ]
    },
    {
      "id": "openalex-w4308084052",
      "source": "openalex-discovery",
      "title": "RipTide: A Programmable, Energy-Minimal Dataflow Compiler and Architecture",
      "authors": [
        {
          "name": "Graham Gobieski",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Souradip Ghosh",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Marijn J. H. Heule",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Todd C. Mowry",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Tony Nowatzki",
          "affiliation": "University of California, Los Angeles"
        },
        {
          "name": "Nathan Beckmann",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Brandon Lucia",
          "affiliation": "Carnegie Mellon University"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Emerging sensing applications create an unprecedented need for energy efficiency in programmable processors. To achieve useful multi-year deployments on a small battery or energy harvester, these applications must avoid off-device communication and instead process most data locally. Recent work has proven coarse-grained reconfigurable arrays (CGRAs) as a promising architecture for this domain. Unfortunately, nearly all prior CGRAs support only computations with simple control flow and no memory aliasing (e.g., affine inner loops), causing an Amdahl efficiency bottleneck as non-trivial fractions of programs must run on an inefficient von Neumann core.RipTide is a co-designed compiler and CGRA architecture that achieves both high programmability and extreme energy efficiency, eliminating this bottleneck. RipTide provides a rich set of control-flow operators that support arbitrary control flow and memory access on the CGRA fabric. RipTide implements these primitives without tagged tokens to save energy; this requires careful ordering analysis in the compiler to guarantee correctness. RipTide further saves energy and area by offloading most control operations into its programmable on-chip network, where they can re-use existing network switches. RipTide's compiler is implemented in LLVM, and its hardware is synthesized in Intel 22FFL. RipTide compiles applications written in C while saving 25% energy v. the state-of-the-art energy-minimal CGRA and 6.6 × energy v. a von Neumann core.",
      "paperUrl": "https://doi.org/10.1109/micro56248.2022.00046",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Brandon Lucia"
      ]
    },
    {
      "id": "openalex-w4318603191",
      "source": "openalex-discovery",
      "title": "Reinforcement Learning assisted Loop Distribution for Locality and Vectorization",
      "authors": [
        {
          "name": "Shalini Jain",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "S. VenkataKeerthy",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Rohit Aggarwal",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Tharun Kumar Dangeti",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Dibyendu Das",
          "affiliation": "Intel (India)"
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": "Indian Institute of Technology Hyderabad"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Loop distribution (loop fission) is a well known compiler optimization that splits the loop into multiple loops. Loop distribution can be seen as an enabler of various other optimizations with different goals, like, the parallelizability of the loop, the vectorizability of the loop, its locality characteristics. In this work, we present a Reinforcement Learning (RL) based approach to efficiently perform loop-distribution with the twin goals of optimizing for both vectorization as well as locality. Broadly, we generate the SCC Dependence Graph (SDG) for each loop of the program. Our RL model learns to predict the distribution order of the loop by performing topological walk of the graph. The reward to our model is computed using the instruction cost and number of cache misses of the loop. As the RL models need data for training purposes, we also propose a novel strategy to extend the training set by generating new loops. We show our results on x86 architecture on various benchmarks: from TSVC, LLVM-Test-Suite, PolyBench and PolyBench-NN. Our framework achieves an average improvement of 3.63% on TSVC, 4.61% on LLVM-Test-Suite Microbenchmarks, 1.78% on PolyBench and 1.95% on PolyBench-NN benchmark suites for execution time. The baseline is O3 compiler option of LLVM. We also show the improvements of our method on other performance metrics like Instruction Per Cycle (IPC), Number of loops distributed & vectorized, and L1 cache performance.",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc56686.2022.00006",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Dibyendu Das",
        "Ramakrishna Upadrasta",
        "S. VenkataKeerthy",
        "Shalini Jain"
      ]
    },
    {
      "id": "openalex-w4210408109",
      "source": "openalex-discovery",
      "title": "Register-Pressure-Aware Instruction Scheduling Using Ant Colony Optimization",
      "authors": [
        {
          "name": "Ghassan Shobaki",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "V. Scott Gordon",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Paul McHugh",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Théodore Dubois",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Austin Kerbow",
          "affiliation": "California State University, Sacramento"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 19 (Issue 2)",
      "type": "research-paper",
      "abstract": "This paper describes a new approach to register-pressure-aware instruction scheduling, using Ant Colony Optimization (ACO) . ACO is a nature-inspired optimization technique that researchers have successfully applied to NP-hard sequencing problems like the Traveling Salesman Problem (TSP) and its derivatives. In this work, we describe an ACO algorithm for solving the long-standing compiler optimization problem of balancing Instruction-Level Parallelism (ILP) and Register Pressure (RP) in pre-allocation instruction scheduling. Three different cost functions are studied for estimating RP during instruction scheduling. The proposed ACO algorithm is implemented in the LLVM open-source compiler, and its performance is evaluated experimentally on three different machines with three different instruction-set architectures: Intel x86, ARM, and AMD GPU. The proposed ACO algorithm is compared to an exact Branch-and-Bound (B&amp;B) algorithm proposed in previous work. On x86 and ARM, both algorithms are evaluated relative to LLVM's generic scheduler, while on the AMD GPU, the algorithms are evaluated relative to AMD's production scheduler. The experimental results show that using SPECrate 2017 Floating Point, the proposed algorithm gives geometric-mean improvements of 1.13% and 1.25% in execution speed on x86 and ARM, respectively, relative to the LLVM scheduler. Using PlaidML on an AMD GPU, it gives a geometric-mean improvement of 7.14% in execution speed relative to the AMD scheduler. The proposed ACO algorithm gives approximately the same execution-time results as the B&amp;B algorithm, with each algorithm outperforming the other on a substantial number of hard scheduling regions. ACO gives better results than B&amp;B on many large instances that B&amp;B times out on. Both ACO and B&amp;B outperform the LLVM algorithm on the CPU and the AMD algorithm on the GPU.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3505558",
      "sourceUrl": "https://doi.org/10.1145/3505558",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Ghassan Shobaki"
      ]
    },
    {
      "id": "openalex-w4281389808",
      "source": "openalex-discovery",
      "title": "RecIPE: Revisiting the Evaluation of Memory Error Defenses",
      "authors": [
        {
          "name": "Yuancheng Jiang",
          "affiliation": "National University of Singapore"
        },
        {
          "name": "Roland H. C. Yap",
          "affiliation": "National University of Singapore"
        },
        {
          "name": "Zhenkai Liang",
          "affiliation": "National University of Singapore"
        },
        {
          "name": "Hubert Rosier",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM on Asia Conference on Computer and Communications Security | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Many detection and defense mechanisms have been proposed to prevent and mitigate memory errors. A good understanding of the pros and cons of memory defense mechanisms is essential before practical deployment. However, the environment, compilers, and defenses change over time. Thus, a benchmark providing comprehensive evaluations of a range of compilation option choices and defenses is helpful to give developers guidance. The most well-known test suite for evaluating spatial memory errors and exploits is RIPE. However, we show that it is no longer applicable, and a new benchmark is needed. Furthermore, a benchmark should be extensible to evolve over time easily. We propose RecIPE, a new extensible and comprehensive benchmark for evaluating memory error defenses. For extensibility and customisability, RecIPE consists of two components. The TestGen component generates vulnerable code from configurable templates across various vulnerable attributes allowing easy changes through the templates. Measuring the attack and exploitation is with the DefEval component, serving as an attacker at runtime and can be updated with new exploitations. We present a comprehensive evaluation of compiler defenses and sanitizers on RecIPE with gcc and clang, showing the strengths and weaknesses of the various defenses. The results show pros and cons which may not be well known, including gaps between the in-principle guarantee and practical implementation. Our results also point out directions for further improvement in defenses.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3488932.3524127",
      "sourceUrl": "https://doi.org/10.1145/3488932.3524127",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Zhenkai Liang"
      ]
    },
    {
      "id": "openalex-w4226439887",
      "source": "openalex-discovery",
      "title": "RISE & Shine: Language-Oriented Compiler Design",
      "authors": [
        {
          "name": "Michel Steuwer",
          "affiliation": ""
        },
        {
          "name": "Thomas Kœhler",
          "affiliation": ""
        },
        {
          "name": "Bastian Köpcke",
          "affiliation": ""
        },
        {
          "name": "Federico Pizzuti",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The trend towards specialization of software and hardware - fuelled by the end of Moore's law and the still accelerating interest in domain-specific computing, such as machine learning - forces us to radically rethink our compiler designs. The era of a universal compiler framework built around a single one-size-fits-all intermediate representation (IR) is over. This realization has sparked the creation of the MLIR compiler framework that empowers compiler engineers to design and integrate IRs capturing specific abstractions. MLIR provides a generic framework for SSA-based IRs, but it doesn't help us to decide how we should design IRs that are easy to develop, to work with and to combine into working compilers. To address the challenge of IR design, we advocate for a language-oriented compiler design that understands IRs as formal programming languages and enforces their correct use via an accompanying type system. We argue that programming language techniques directly guide extensible IR designs and provide a formal framework to reason about transforming between multiple IRs. In this paper, we discuss the design of the Shine compiler that compiles the high-level functional pattern-based data-parallel language RISE via a hybrid functional-imperative intermediate language to C, OpenCL, and OpenMP. We compare our work directly with the closely related pattern-based Lift IR and compiler. We demonstrate that our language-oriented compiler design results in a more robust and predictable compiler that is extensible at various abstraction levels. Our experimental evaluation shows that this compiler design is able to generate high-performance GPU code.",
      "paperUrl": "https://arxiv.org/pdf/2201.03611",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2201.03611",
      "tags": [
        "GPU",
        "IR",
        "MLIR",
        "OpenCL",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Michel Steuwer"
      ]
    },
    {
      "id": "openalex-w4291972620",
      "source": "openalex-discovery",
      "title": "Practical Software-Based Shadow Stacks on x86-64",
      "authors": [
        {
          "name": "Changwei Zou",
          "affiliation": "Macau University of Science and Technology"
        },
        {
          "name": "Yaoqing Gao",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 19 (Issue 4)",
      "type": "research-paper",
      "abstract": "Control-Flow Integrity (CFI) techniques focus often on protecting forward edges and assume that backward edges are protected by shadow stacks. However, software-based shadow stacks that can provide performance, security, and compatibility are still hard to obtain, leaving an important security gap on x86-64. In this article, we introduce a simple, efficient, and effective parallel shadow stack design (based on LLVM), FlashStack , for protecting return addresses in single- and multi-threaded programs running under 64-bit Linux on x86-64, with three distinctive features. First, we introduce a novel dual-prologue approach to enable a protected function to thwart the TOCTTOU attacks, which are constructed by Microsoft’s red team and lead to the deprecation of Microsoft’s RFG. Second, we design a new mapping mechanism, Segment+Rsp-S , to allow the parallel shadow stack to be accessed efficiently while satisfying the constraints of arch_prctl() and ASLR in 64-bit Linux. Finally, we introduce a lightweight inspection mechanism, SideChannel-K , to harden FlashStack further by detecting entropy-reduction attacks efficiently and protecting the parallel shadow stack effectively with a 10-ms shuffling policy. Our evaluation on SPEC CPU2006 , Nginx, and Firefox shows that FlashStack can provide high performance, meaningful security, and reasonable compatibility for server- and client-side programs on x86-64.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3556977",
      "sourceUrl": "https://doi.org/10.1145/3556977",
      "tags": [
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Jingling Xue"
      ]
    },
    {
      "id": "openalex-w4283662726",
      "source": "openalex-discovery",
      "title": "POSET-RL: Phase ordering for Optimizing Size and Execution Time using Reinforcement Learning",
      "authors": [
        {
          "name": "Shalini Jain",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Yashas Andaluri",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "S. VenkataKeerthy",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": "Indian Institute of Technology Hyderabad"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The ever increasing memory requirements of several applications has led to\\nincreased demands which might not be met by embedded devices. Constraining the\\nusage of memory in such cases is of paramount importance. It is important that\\nsuch code size improvements should not have a negative impact on the runtime.\\nImproving the execution time while optimizing for code size is a non-trivial\\nbut a significant task. The ordering of standard optimization sequences in\\nmodern compilers is fixed, and are heuristically created by the compiler domain\\nexperts based on their expertise. However, this ordering is sub-optimal, and\\ndoes not generalize well across all the cases. We present a reinforcement\\nlearning based solution to the phase ordering problem, where the ordering\\nimproves both the execution time and code size. We propose two different\\napproaches to model the sequences: one by manual ordering, and other based on a\\ngraph called Oz Dependence Graph (ODG). Our approach uses minimal data as\\ntraining set, and is integrated with LLVM. We show results on x86 and AArch64\\narchitectures on the benchmarks from SPEC-CPU 2006, SPEC-CPU 2017 and MiBench.\\nWe observe that the proposed model based on ODG outperforms the current Oz\\nsequence both in terms of size and execution time by 6.19% and 11.99% in SPEC\\n2017 benchmarks, on an average.\\n",
      "paperUrl": "https://arxiv.org/pdf/2208.04238",
      "sourceUrl": "https://doi.org/10.1109/ispass55109.2022.00012",
      "tags": [
        "Embedded"
      ],
      "matchedAuthors": [
        "Ramakrishna Upadrasta",
        "S. VenkataKeerthy",
        "Shalini Jain"
      ]
    },
    {
      "id": "openalex-w4309394501",
      "source": "openalex-discovery",
      "title": "Optimizing Function Layout for Mobile Applications",
      "authors": [
        {
          "name": "Ellis Hoag",
          "affiliation": ""
        },
        {
          "name": "Kyungwoo Lee",
          "affiliation": ""
        },
        {
          "name": "Julián Mestre",
          "affiliation": ""
        },
        {
          "name": "Sergey Pupyrev",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Function layout, also referred to as function reordering or function placement, is one of the most effective profile-guided compiler optimizations. By reordering functions in a binary, compilers are able to greatly improve the performance of large-scale applications or reduce the compressed size of mobile applications. Although the technique has been studied in the context of large-scale binaries, no recent study has investigated the impact of function layout on mobile applications. In this paper we develop the first principled solution for optimizing function layouts in the mobile space. To this end, we identify two important optimization goals, the compressed code size and the cold start-up time of a mobile application. Then we propose a formal model for the layout problem, whose objective closely matches the goals. Our novel algorithm to optimize the layout is inspired by the classic balanced graph partitioning problem. We carefully engineer and implement the algorithm in an open source compiler, LLVM. An extensive evaluation of the new method on large commercial mobile applications indicates up to 2% compressed size reduction and up to 3% start-up time improvement on top of the state-of-the-art approach.",
      "paperUrl": "https://arxiv.org/pdf/2211.09285",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2211.09285",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Ellis Hoag",
        "Kyungwoo Lee"
      ]
    },
    {
      "id": "openalex-w4220821662",
      "source": "openalex-discovery",
      "title": "Object Intersection Captures on Interactive Apps to Drive a Crowd-sourced Replay-based Compiler Optimization",
      "authors": [
        {
          "name": "Paschalis Mpeis",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Pavlos Petoumenos",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Kim Hazelwood",
          "affiliation": "Meta (United States)"
        },
        {
          "name": "Hugh Leather",
          "affiliation": "Menlo School"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 19 (Issue 3)",
      "type": "research-paper",
      "abstract": "Traditional offline optimization frameworks rely on representative hardware, software, and inputs to compare different optimizations on. With application-specific optimization for mobile systems though, the idea of a representative testbench is unrealistic while creating offline inputs is non-trivial. Online approaches partially overcome these problems but they might expose users to suboptimal or even erroneous code. Therefore, our mobile code is poorly optimized, resulting in wasted performance and energy and user frustration. In this article, we introduce a novel compiler optimization approach designed for mobile applications. It requires no developer effort, it tunes applications for the user’s device and usage patterns, and it has no negative impact on the user experience. It is based on a lightweight capture and replay mechanism. Our previous work [ 46 ] captures the state accessed by any targeted code region during its online stage. By repurposing existing OS capabilities, it keeps the overhead low. In its offline stage, it replays the code region but under different optimization decisions to enable sound comparisons of different optimizations under realistic conditions. In this article, we propose a technique that further decreases the storage sizes without any additional overhead. It captures only the intersection of reachable objects and accessed heap pages. We compare this with another new approach that has minimal runtime overheads at the cost of higher capture sizes. Coupled with a search heuristic for the compiler optimization space, our capture and replay mechanism allows us to discover optimization decisions that improve performance without testing these decisions directly on the user. Finally, with crowd-sourcing we split this offline evaluation effort between several users, allowing us to discover better code in less time. We implemented a prototype system in Android based on LLVM combined with a genetic search engine and a crowd-sourcing architecture. We evaluated it on both benchmarks and real Android applications. Online captures are infrequent and introduce ~5 ms or 15 ms on average, depending on the approach used. For this negligible effect on user experience, we achieve speedups of 44% on average over the Android compiler and 35% over LLVM -O3. Our collaborative search is just 5% short of that speedup, which is impressive given the acceleration gains. The user with the highest workload concluded the search 7 \\( \\times \\) faster.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3517338",
      "sourceUrl": "https://doi.org/10.1145/3517338",
      "tags": [
        "Optimizations",
        "Performance",
        "Rust",
        "Testing"
      ],
      "matchedAuthors": [
        "Hugh Leather",
        "Kim Hazelwood",
        "Paschalis Mpeis"
      ]
    },
    {
      "id": "openalex-w4308083924",
      "source": "openalex-discovery",
      "title": "OCOLOS: Online COde Layout OptimizationS",
      "authors": [
        {
          "name": "Yuxuan Zhang",
          "affiliation": "California University of Pennsylvania"
        },
        {
          "name": "Tanvir Ahmed Khan",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Gilles Pokam",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "Baris Kasikci",
          "affiliation": "California University of Pennsylvania"
        },
        {
          "name": "Heiner Litz",
          "affiliation": "University of California, Santa Cruz"
        },
        {
          "name": "Joseph Devietti",
          "affiliation": "California University of Pennsylvania"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The processor front-end has become an increasingly important bottleneck in recent years due to growing application code footprints, particularly in data centers. First-level instruction caches and branch prediction engines have not been able to keep up with this code growth, leading to more front-end stalls and lower Instructions Per Cycle (IPC). Profile-guided optimizations performed by compilers represent a promising approach, as they rearrange code to maximize instruction cache locality and branch prediction efficiency along a relatively small number of hot code paths. However, these optimizations require continuous profiling and rebuilding of applications to ensure that the code layout matches the collected profiles. If an application's code is frequently updated, it becomes challenging to map profiling data from a previous version onto the latest version, leading to ignored profiling data and missed optimization opportunities.In this paper, we propose OCOLOS, the first online code layout optimization system for unmodified applications written in unmanaged languages. OCOLOS allows profile-guided optimization to be performed on a running process, instead of being performed offline and requiring the application to be re-launched. By running online, profile data is always relevant to the current execution and always maps perfectly to the running code. OCOLOS demonstrates how to achieve robust online code replacement in complex multithreaded applications like MySQL and MongoDB, without requiring any application changes. Our experiments show that OCOLOS can accelerate MySQL by up to $1.41 \\times $, the Verilator hardware simulator by up to $2.20 \\times $, and a build of the Clang compiler by up to $1.14 \\times $.",
      "paperUrl": "https://doi.org/10.1109/micro56248.2022.00045",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Joseph Devietti"
      ]
    },
    {
      "id": "openalex-w3128680588",
      "source": "openalex-discovery",
      "title": "NOELLE Offers Empowering LLVM Extensions",
      "authors": [
        {
          "name": "Angelo Matni",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Enrico Armenio Deiana",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Yian Su",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Lukas Gross",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Souradip Ghosh",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Sotiris Apostolakis",
          "affiliation": "Princeton University"
        },
        {
          "name": "Ziyang Xu",
          "affiliation": "Princeton University"
        },
        {
          "name": "Zujun Tan",
          "affiliation": "Princeton University"
        },
        {
          "name": "Ishita Chaturvedi",
          "affiliation": "Princeton University"
        },
        {
          "name": "David I. August",
          "affiliation": "Northwestern University"
        },
        {
          "name": "Simone Campanoni",
          "affiliation": "Northwestern University"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern and emerging architectures demand increasingly complex compiler analyses and transformations. As the emphasis on compiler infrastructure moves beyond support for peephole optimizations and the extraction of instruction-level parallelism, they should support custom tools designed to meet these demands with higher-level analysis-powered abstractions of wider program scope. This paper introduces NOELLE, a robust open-source domain-independent compilation layer built upon LLVM providing this support. NOELLE is modular and demand-driven, making it easy-to-extend and adaptable to custom-tool-specific needs without unduly wasting compile time and memory. This paper shows the power of NOELLE by presenting a diverse set of ten custom tools built upon it, with a 33.2% to 99.2% reduction in code size (LoC) compared to their counterparts without NOELLE.",
      "paperUrl": "https://arxiv.org/pdf/2102.05081",
      "sourceUrl": "https://doi.org/10.1109/cgo53902.2022.9741276",
      "tags": [
        "Infrastructure",
        "Optimizations"
      ],
      "matchedAuthors": [
        "David I. August"
      ]
    },
    {
      "id": "openalex-w4293820470",
      "source": "openalex-discovery",
      "title": "MSWasm: Soundly Enforcing Memory-Safe Execution of Unsafe Code",
      "authors": [
        {
          "name": "Alexandra E. Michael",
          "affiliation": ""
        },
        {
          "name": "Anitha Gollamudi",
          "affiliation": ""
        },
        {
          "name": "Jay Bosamiya",
          "affiliation": ""
        },
        {
          "name": "Craig Disselkoen",
          "affiliation": ""
        },
        {
          "name": "Aidan Denlinger",
          "affiliation": ""
        },
        {
          "name": "Conrad Watt",
          "affiliation": ""
        },
        {
          "name": "Bryan Parno",
          "affiliation": ""
        },
        {
          "name": "Marco Patrignani",
          "affiliation": ""
        },
        {
          "name": "Marco Vassena",
          "affiliation": ""
        },
        {
          "name": "Deian Stefan",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Most programs compiled to WebAssembly (Wasm) today are written in unsafe languages like C and C++. Unfortunately, memory-unsafe C code remains unsafe when compiled to Wasm -- and attackers can exploit buffer overflows and use-after-frees in Wasm almost as easily as they can on native platforms. Memory-Safe WebAssembly (MSWasm) proposes to extend Wasm with language-level memory-safety abstractions to precisely address this problem. In this paper, we build on the original MSWasm position paper to realize this vision. We give a precise and formal semantics of MSWasm, and prove that well-typed MSWasm programs are, by construction, robustly memory safe. To this end, we develop a novel, language-independent memory-safety property based on colored memory locations and pointers. This property also lets us reason about the security guarantees of a formal C-to-MSWasm compiler -- and prove that it always produces memory-safe programs (and preserves the semantics of safe programs). We use these formal results to then guide several implementations: Two compilers of MSWasm to native code, and a C-to-MSWasm compiler (that extends Clang). Our MSWasm compilers support different enforcement mechanisms, allowing developers to make security-performance trade-offs according to their needs. Our evaluation shows that the overhead of enforcing memory safety in software ranges from 22% (enforcing spatial safety alone) to 198% (enforcing full memory safety) on the PolyBenchC suite. More importantly, MSWasm's design makes it easy to swap between enforcement mechanisms; as fast (especially hardware-based) enforcement techniques become available, MSWasm will be able to take advantage of these advances almost for free.",
      "paperUrl": "https://arxiv.org/pdf/2208.13583",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2208.13583",
      "tags": [
        "C++",
        "Clang",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Deian Stefan"
      ]
    },
    {
      "id": "openalex-w4292946825",
      "source": "openalex-discovery",
      "title": "MOM: Matrix Operations in MLIR",
      "authors": [
        {
          "name": "Lorenzo Chelini",
          "affiliation": ""
        },
        {
          "name": "Henrik Barthels",
          "affiliation": ""
        },
        {
          "name": "Paolo Bientinesi",
          "affiliation": ""
        },
        {
          "name": "Marcin Copik",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        },
        {
          "name": "Daniele G. Spampinato",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern research in code generators for dense linear algebra computations has shown the ability to produce optimized code with a performance which compares and often exceeds the one of state-of-the-art implementations by domain experts. However, the underlying infrastructure is often developed in isolation making the interconnection of logically combinable systems complicated if not impossible. In this paper, we propose to leverage MLIR as a unifying compiler infrastructure for the optimization of dense linear algebra operations. We propose a new MLIR dialect for expressing linear algebraic computations including matrix properties to enable high-level algorithmic transformations. The integration of this new dialect in MLIR enables end-to-end compilation of matrix computations via conversion to existing lower-level dialects already provided by the framework.",
      "paperUrl": "https://arxiv.org/pdf/2208.10391",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2208.10391",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4286231542",
      "source": "openalex-discovery",
      "title": "MLGOPerf: An ML Guided Inliner to Optimize Performance",
      "authors": [
        {
          "name": "Amir H. Ashouri",
          "affiliation": ""
        },
        {
          "name": "Mostafa Elhoushi",
          "affiliation": ""
        },
        {
          "name": "Yuzhe Hua",
          "affiliation": ""
        },
        {
          "name": "Xiang Wang",
          "affiliation": ""
        },
        {
          "name": "Muhammad Asif Manzoor",
          "affiliation": ""
        },
        {
          "name": "Bryan Chan",
          "affiliation": ""
        },
        {
          "name": "Yaoqing Gao",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "For the past 25 years, we have witnessed an extensive application of Machine Learning to the Compiler space; the selection and the phase-ordering problem. However, limited works have been upstreamed into the state-of-the-art compilers, i.e., LLVM, to seamlessly integrate the former into the optimization pipeline of a compiler to be readily deployed by the user. MLGO was among the first of such projects and it only strives to reduce the code size of a binary with an ML-based Inliner using Reinforcement Learning. This paper presents MLGOPerf; the first end-to-end framework capable of optimizing performance using LLVM's ML-Inliner. It employs a secondary ML model to generate rewards used for training a retargeted Reinforcement learning agent, previously used as the primary model by MLGO. It does so by predicting the post-inlining speedup of a function under analysis and it enables a fast training framework for the primary model which otherwise wouldn't be practical. The experimental results show MLGOPerf is able to gain up to 1.8% and 2.2% with respect to LLVM's optimization at O3 when trained for performance on SPEC CPU2006 and Cbench benchmarks, respectively. Furthermore, the proposed approach provides up to 26% increased opportunities to autotune code regions for our benchmarks which can be translated into an additional 3.7% speedup value.",
      "paperUrl": "https://arxiv.org/pdf/2207.08389",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2207.08389",
      "tags": [
        "ML",
        "Performance"
      ],
      "matchedAuthors": [
        "Bryan Chan",
        "Mostafa Elhoushi"
      ]
    },
    {
      "id": "openalex-w4291301639",
      "source": "openalex-discovery",
      "title": "MARTINI: The Little Match and Replace Tool forAutomatic Code Rewriting",
      "authors": [
        {
          "name": "Alister Johnson",
          "affiliation": ""
        },
        {
          "name": "Camille Coti",
          "affiliation": ""
        },
        {
          "name": "Allen D. Malony",
          "affiliation": ""
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "The Journal of Open Source Software | Vol. 7 (Issue 76)",
      "type": "research-paper",
      "abstract": "Rewriting code for cleanliness, API changes, and new programming models is a common, yet time-consuming task.Localized or syntax-based changes are often mechanical and can be automated with text-based tools, like Unix's sed.However, non-localized or semantic-based changes require specialized tools that usually come with complex, hard-coded rules that require expertise in compilers.This means existing techniques for code rewriting are either too simple for complex tasks or too complex to be customized by non-expert users; in either case, developers are often forced to manually update their code instead.This work presents MARTINI, a new approach to code rewriting built on the Clang compiler, which exposes complex and semantic-driven rewrite capabilities to users in a simple and natural way.Rewrite rules are expressed as a pair of parameterized \"before-and-after\" code snippets in the source language, one to describe what to match and one to describe what the replacement looks like.Through this novel and user-friendly interface, programmers can automate and customize complex code changes which require a deep understanding of the language without any knowledge of compiler internals.",
      "paperUrl": "https://joss.theoj.org/papers/10.21105/joss.04590.pdf",
      "sourceUrl": "https://doi.org/10.21105/joss.04590",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Alister Johnson",
        "Johannes Doerfert"
      ]
    },
    {
      "id": "openalex-w4225412165",
      "source": "openalex-discovery",
      "title": "LoopStack: a Lightweight Tensor Algebra Compiler Stack",
      "authors": [
        {
          "name": "Bram Wasti",
          "affiliation": ""
        },
        {
          "name": "José Cambronero",
          "affiliation": ""
        },
        {
          "name": "Benoit Steiner",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": ""
        },
        {
          "name": "Aleksandar Zlateski",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present LoopStack, a domain specific compiler stack for tensor operations, composed of a frontend, LoopTool, and an efficient optimizing code generator, LoopNest. This stack enables us to compile entire neural networks and generate code targeting the AVX2, AVX512, NEON, and NEONfp16 instruction sets while incorporating optimizations often missing from other machine learning compiler backends. We evaluate our stack on a collection of full neural networks and commonly used network blocks as well as individual operators, and show that LoopStack generates machine code that matches and frequently exceeds the performance of in state-of-the-art machine learning frameworks in both cases. We also show that for a large collection of schedules LoopNest's compilation is orders of magnitude faster than LLVM, while resulting in equal or improved run time performance. Additionally, LoopStack has a very small memory footprint - a binary size of 245KB, and under 30K lines of effective code makes it ideal for use on mobile and embedded devices.",
      "paperUrl": "https://arxiv.org/pdf/2205.00618",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2205.00618",
      "tags": [
        "Backend",
        "Embedded",
        "Frontend",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w4226420170",
      "source": "openalex-discovery",
      "title": "Learning to Combine Instructions in LLVM Compiler",
      "authors": [
        {
          "name": "Sandya Mannarswamy",
          "affiliation": ""
        },
        {
          "name": "Dibyendu Das",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Instruction combiner (IC) is a critical compiler optimization pass, which replaces a sequence of instructions with an equivalent and optimized instruction sequence at basic block level. There can be thousands of instruction-combining patterns which need to be frequently updated as new coding idioms/applications and novel hardware evolve over time. This results in frequent updates to the IC optimization pass thereby incurring considerable human effort and high software maintenance costs. To mitigate these challenges associated with the traditional IC, we design and implement a Neural Instruction Combiner (NIC) and demonstrate its feasibility by integrating it into the standard LLVM compiler optimization pipeline. NIC leverages neural sequence-to-sequence (Seq2Seq) models for generating optimized encoded IR sequence from the unoptimized encoded IR sequence. To the best of our knowledge, ours is the first work demonstrating the feasibility of a neural instruction combiner built into a full-fledged compiler pipeline. Given the novelty of this task, we built a new dataset for training our NIC neural model. We show that NIC achieves exact match results percentage of 72% for optimized sequences as compared to traditional IC and neural machine translation metric Bleu precision score of 0.94, demonstrating its feasibility in a production compiler pipeline.",
      "paperUrl": "https://arxiv.org/pdf/2202.12379",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2202.12379",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Dibyendu Das"
      ]
    },
    {
      "id": "openalex-w4221152812",
      "source": "openalex-discovery",
      "title": "Lambda the Ultimate SSA: Optimizing Functional Programs in SSA",
      "authors": [
        {
          "name": "S. Bhat",
          "affiliation": ""
        },
        {
          "name": "Tobias Grosser",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Static Single Assignment (SSA) is the workhorse of modern optimizing compilers for imperative programming languages. However, functional languages have been slow to adopt SSA and prefer to use intermediate representations based on minimal lambda calculi due to SSA's inability to express higher order constructs. We exploit a new SSA construct -- regions -- in order to express functional optimizations via classical SSA based reasoning. Region optimization currently relies on ad-hoc analyses and transformations on imperative programs. These ad-hoc transformations are sufficient for imperative languages as regions are used in a limited fashion. In contrast, we use regions pervasively to model sub-expressions in our functional IR. This motivates us to systematize region optimizations. We extend classical SSA reasoning to regions for functional-style analyses and transformations. We implement a new SSA+regions based backend for LEAN4, a theorem prover that implements a purely functional, dependently typed programming language. Our backend is feature-complete and handles all constructs of LEAN4's functional intermediate representation λrc within the SSA framework. We evaluate our proposed region optimizations by optimizing λrc within an SSA+regions based framework implemented in MLIR and demonstrating performance parity with the current LEAN4 backend. We believe our work will pave the way for a unified optimization framework capable of representing, analyzing, and optimizing both functional and imperative languages.",
      "paperUrl": "https://arxiv.org/pdf/2201.07272",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2201.07272",
      "tags": [
        "Backend",
        "IR",
        "MLIR",
        "Optimizations",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4313171097",
      "source": "openalex-discovery",
      "title": "LLTFI: Framework Agnostic Fault Injection for Machine Learning Applications (Tools and Artifact Track)",
      "authors": [
        {
          "name": "Udit Kumar Agarwal",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Abraham Chan",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Karthik Pattabiraman",
          "affiliation": "University of British Columbia"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As machine learning (ML) has become more preva-lent across many critical domains, so has the need to understand ML applications' resilience. While prior work like TensorFI [1], MindFI [2], and PyTorchFI [3] has focused on building ML fault injectors for specific ML frameworks, there has been little work on performing fault injection (FI) for ML applications written in multiple frameworks. We present LLTFI, a framework-agnostic fault injection tool for ML applications, allowing users to run FI experiments on ML applications at the LLVM IR level. LLTFI provides users with finer FI granularity at the level of instructions, and a better understanding of how faults manifest and propagate between different ML components. We evaluate LLTFI on six ML programs and compare it with TensorFI. We found significant differences in the Silent Data Corruption (SDC) rates for similar faults between the two tools. Finally, we use LLTFI to evaluate the efficacy of selective instruction duplication - an error mitigation technique - for ML programs.",
      "paperUrl": "https://doi.org/10.1109/issre55969.2022.00036",
      "sourceUrl": "",
      "tags": [
        "IR",
        "ML"
      ],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w4225919726",
      "source": "openalex-discovery",
      "title": "LART: Compiled Abstract Execution",
      "authors": [
        {
          "name": "Henrich Lauko",
          "affiliation": "Masaryk University"
        },
        {
          "name": "Petr Ročkai",
          "affiliation": "Masaryk University"
        }
      ],
      "year": "2022",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract lart – llvm abstraction and refinement tool – originates from the divine model-checker [5, 7], in which it was employed as an abstraction toolchain for the llvm interpreter. In this contribution, we present a stand-alone tool that does not need a verification backend but performs the verification natively. The core idea is to instrument abstract semantics directly into the program and compile it into a native binary that performs program analysis. This approach provides a performance gain of native execution over the interpreted analysis and allows compiler optimizations to be employed on abstracted code, further extending the analysis efficiency. Compilation-based abstraction introduces new challenges solved by lart , like domain interaction of concrete and abstract values simulation of nondeterministic runtime or constraint propagation.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-030-99527-0_31.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-030-99527-0_31",
      "tags": [
        "Backend",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Henrich Lauko"
      ]
    },
    {
      "id": "openalex-w4205586362",
      "source": "openalex-discovery",
      "title": "Isolation without taxation: near-zero-cost transitions for WebAssembly and SFI",
      "authors": [
        {
          "name": "Matthew Kolosick",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Shravan Narayan",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Evan Johnson",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Conrad Watt",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Michael LeMay",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Deepak Garg",
          "affiliation": "Max Planck Institute for Software Systems"
        },
        {
          "name": "Ranjit Jhala",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Deian Stefan",
          "affiliation": "University of California, San Diego"
        }
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 6 (Issue POPL)",
      "type": "research-paper",
      "abstract": "Software sandboxing or software-based fault isolation (SFI) is a lightweight approach to building secure systems out of untrusted components. Mozilla, for example, uses SFI to harden the Firefox browser by sandboxing third-party libraries, and companies like Fastly and Cloudflare use SFI to safely co-locate untrusted tenants on their edge clouds. While there have been significant efforts to optimize and verify SFI enforcement, context switching in SFI systems remains largely unexplored: almost all SFI systems use heavyweight transitions that are not only error-prone but incur significant performance overhead from saving, clearing, and restoring registers when context switching. We identify a set of zero-cost conditions that characterize when sandboxed code has sufficient structured to guarantee security via lightweight zero-cost transitions (simple function calls). We modify the Lucet Wasm compiler and its runtime to use zero-cost transitions, eliminating the undue performance tax on systems that rely on Lucet for sandboxing (e.g., we speed up image and font rendering in Firefox by up to 29.7% and 10% respectively). To remove the Lucet compiler and its correct implementation of the Wasm specification from the trusted computing base, we (1) develop a static binary verifier , VeriZero, which (in seconds) checks that binaries produced by Lucet satisfy our zero-cost conditions, and (2) prove the soundness of VeriZero by developing a logical relation that captures when a compiled Wasm function is semantically well-behaved with respect to our zero-cost conditions. Finally, we show that our model is useful beyond Wasm by describing a new, purpose-built SFI system, SegmentZero32, that uses x86 segmentation and LLVM with mostly off-the-shelf passes to enforce our zero-cost conditions; our prototype performs on-par with the state-of-the-art Native Client SFI system.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3498688",
      "sourceUrl": "https://doi.org/10.1145/3498688",
      "tags": [
        "Libraries",
        "Performance",
        "Rust",
        "Security"
      ],
      "matchedAuthors": [
        "Deian Stefan",
        "Ranjit Jhala"
      ]
    },
    {
      "id": "openalex-w4281950471",
      "source": "openalex-discovery",
      "title": "IRDL: an IR definition language for SSA compilers",
      "authors": [
        {
          "name": "Mathieu Fehr",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Jeff Niu",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "River Riddle",
          "affiliation": "Modular Genetics (United States)"
        },
        {
          "name": "Mehdi Amini",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Zhendong Su",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Designing compiler intermediate representations (IRs) is often a manual process that makes exploration and innovation in this space costly. Developers typically use general-purpose programming languages to design IRs. As a result, IR implementations are verbose, manual modifications are expensive, and designing tooling for the inspection or generation of IRs is impractical. While compilers relied historically on a few slowly evolving IRs, domain-specific optimizations and specialized hardware motivate compilers to use and evolve many IRs. We facilitate the implementation of SSA-based IRs by introducing IRDL, a domain-specific language to define IRs. We analyze all 28 domain-specific IRs developed as part of LLVM's MLIR project over the last two years and demonstrate how to express these IRs exclusively in IRDL while only rarely falling back to IRDL's support for generic C++ extensions. By enabling the concise and explicit specification of IRs, we provide foundations for developing effective tooling to automate the compiler construction process.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3519939.3523700",
      "sourceUrl": "https://doi.org/10.1145/3519939.3523700",
      "tags": [
        "C++",
        "IR",
        "MLIR",
        "Optimizations",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Jeff Niu",
        "Mathieu Fehr",
        "Mehdi Amini",
        "River Riddle",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4293102972",
      "source": "openalex-discovery",
      "title": "High‐coverage metamorphic testing of concurrency support in C compilers",
      "authors": [
        {
          "name": "Matt Windsor",
          "affiliation": "University of York"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        },
        {
          "name": "John Wickerson",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2022",
      "venue": "Software Testing Verification and Reliability | Vol. 32 (Issue 4)",
      "type": "research-paper",
      "abstract": "Summary We present a technique and automated toolbox for randomized testing of C compilers. Unlike prior compiler‐testing approaches, we generate concurrent test cases in which threads communicate using fine‐grained atomic operations, and we study actual compiler implementations rather than abstract mappings. Our approach is (1) to generate test cases with precise oracles directly from an axiomatization of the C concurrency model; (2) to apply metamorphic fuzzing to each test case, aiming to amplify the coverage they are likely to achieve on compiler codebases; and (3) to execute each fuzzed test case extensively on a range of real machines. Our tool, C4, benefits compiler developers in two ways. First, test cases generated by C4 can achieve line coverage of parts of the LLVM C compiler that are reached by neither the LLVM test suite nor an existing (sequential) C fuzzer. This information can be used to guide further development of the LLVM test suite and can also shed light on where and how concurrency‐related compiler optimizations are implemented. Second, C4 can be used to gain confidence that a compiler implements concurrency correctly. As evidence of this, we show that C4 achieves high strong mutation coverage with respect to a set of concurrency‐related mutants derived from a recent version of LLVM and that it can find historic concurrency‐related bugs in GCC. As a by‐product of concurrency‐focused testing, C4 also revealed two previously unknown sequential compiler bugs in recent versions of GCC and the IBM XL compiler.",
      "paperUrl": "https://doi.org/10.1002/stvr.1812",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Testing"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson"
      ]
    },
    {
      "id": "openalex-w4283820265",
      "source": "openalex-discovery",
      "title": "High-Performance GPU-to-CPU Transpilation and Optimization via High-Level Parallel Constructs",
      "authors": [
        {
          "name": "William S. Moses",
          "affiliation": ""
        },
        {
          "name": "Ivan R. Ivanov",
          "affiliation": ""
        },
        {
          "name": "Jens Domke",
          "affiliation": ""
        },
        {
          "name": "Toshio Endo",
          "affiliation": ""
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "While parallelism remains the main source of performance, architectural implementations and programming models change with each new hardware generation, often leading to costly application re-engineering. Most tools for performance portability require manual and costly application porting to yet another programming model. We propose an alternative approach that automatically translates programs written in one programming model (CUDA), into another (CPU threads) based on Polygeist/MLIR. Our approach includes a representation of parallel constructs that allows conventional compiler transformations to apply transparently and without modification and enables parallelism-specific optimizations. We evaluate our framework by transpiling and optimizing the CUDA Rodinia benchmark suite for a multi-core CPU and achieve a 76% geomean speedup over handwritten OpenMP code. Further, we show how CUDA kernels from PyTorch can efficiently run and scale on the CPU-only Supercomputer Fugaku without user intervention. Our PyTorch compatibility layer making use of transpiled CUDA PyTorch kernels outperforms the PyTorch CPU native backend by 2.7$\\times$.",
      "paperUrl": "https://arxiv.org/pdf/2207.00257",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2207.00257",
      "tags": [
        "Backend",
        "CUDA",
        "GPU",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Ivan R. Ivanov",
        "Jens Domke",
        "Johannes Doerfert",
        "Toshio Endo",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4220830361",
      "source": "openalex-discovery",
      "title": "HECATE: Performance-Aware Scale Optimization for Homomorphic Encryption Compiler",
      "authors": [
        {
          "name": "Yongwoo Lee",
          "affiliation": "Yonsei University"
        },
        {
          "name": "Seonyeong Heo",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Seonyoung Cheon",
          "affiliation": "Yonsei University"
        },
        {
          "name": "Shinnung Jeong",
          "affiliation": "Yonsei University"
        },
        {
          "name": "Changsu Kim",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Eunkyung Kim",
          "affiliation": "Samsung (South Korea)"
        },
        {
          "name": "Dongyoon Lee",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Hanjun Kim",
          "affiliation": "Yonsei University"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Despite the benefit of Fully Homomorphic Encryption (FHE) that supports encrypted computation, writing an efficient FHE application is challenging due to magnitude scale management. Each FHE operation increases scales of ciphertext and leaving the scales high harms performance of the following FHE operations. Thus, rescaling ciphertext is inevitable to optimize an FHE application, but since FHE requires programmers to match the rescaling levels of operands of each FHE operation, programmers should rescale ciphertext reflecting the entire FHE application. Although recently proposed FHE compilers reduce the programming burden by automatically manipulating ciphertext scales, they fail to fully optimize the FHE application because they greedily rescale the ciphertext without considering their performance impacts throughout the entire application. This work proposes HECATE, a new FHE compiler framework that optimizes scales of ciphertext reflecting their rescaling levels and performance impact. With a new type system that embeds the scale and rescaling level, and a new rescaling operation called downscale, HECATE makes various scale management plans, analyzes their expected performance, and finds the optimal rescaling points throughout the entire FHE application. This work implements HECATE on top of the MLIR framework with a Python frontend and shows that HECATE achieves 27% speedup over the state-of-the-art approach for various FHE applications.",
      "paperUrl": "https://doi.org/10.1109/cgo53902.2022.9741265",
      "sourceUrl": "",
      "tags": [
        "Frontend",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Hanjun Kim"
      ]
    },
    {
      "id": "openalex-w4221005776",
      "source": "openalex-discovery",
      "title": "Graph transformations for register-pressure-aware instruction scheduling",
      "authors": [
        {
          "name": "Ghassan Shobaki",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Justin Bassett",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Mark Heffernan",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Austin Kerbow",
          "affiliation": "California State University, Sacramento"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper presents graph transformation algorithms for register-pressure-aware instruction scheduling. The proposed transformations add edges to the data dependence graph (DDG) to eliminate solutions that are either redundant or sub-optimal. Register-pressure-aware instruction scheduling aims at balancing two conflicting objectives: maximizing instruction-level parallelism (ILP) and minimizing register pressure (RP). Graph transformations have been previously proposed for the problem of maximizing ILP without considering RP, which is a problem of limited practical value. In the current paper, we extend that work by proposing graph transformations for the RP minimization objective, which is an important objective in practice. Various cost functions are considered for representing RP, and we show that the proposed transformations preserve optimality with respect to each of them. The proposed transformations are used to reduce the size of the solution space before applying a Branch-and-Bound (B&B) algorithm that exhaustively searches for an optimal solution. The proposed transformations and the B&B algorithm were implemented in the LLVM compiler, and their performance was evaluated experimentally on a CPU target and a GPU target. The SPEC CPU2017 floating-point benchmarks were used on the CPU and the PlaidML benchmarks were used on the GPU. The results show that the proposed transformations significantly reduce the compile time while giving approximately the same execution-time performance.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3497776.3517771",
      "sourceUrl": "https://doi.org/10.1145/3497776.3517771",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Ghassan Shobaki",
        "Mark Heffernan"
      ]
    },
    {
      "id": "openalex-w4213287115",
      "source": "openalex-discovery",
      "title": "Finding missed optimizations through the lens of dead code elimination",
      "authors": [
        {
          "name": "Theodoros Theodoridis",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Manuel Rigger",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Zhendong Su",
          "affiliation": "ETH Zurich"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compilers are foundational software development tools and incorporate increasingly sophisticated optimizations. Due to their complexity, it is difficult to systematically identify opportunities for improving them. Indeed, the automatic discovery of missed optimizations has been an important and significant challenge. The few existing approaches either cannot accurately pinpoint missed optimizations or target only specific analyses. This paper tackles this challenge by introducing a novel, effective approach that --- in a simple and general manner --- automatically identifies a wide range of missed optimizations. Our core insight is to leverage dead code elimination (DCE) to both analyze how well compilers optimize code and identify missed optimizations: (1) insert \"optimization markers\" in the basic blocks of a given program, (2) compute the program's live/dead basic blocks using the \"optimization markers\", and (3) identify missed optimizations from how well compilers eliminate dead blocks. We essentially exploit that, since DCE heavily depends on the rest of the optimization pipeline, through the lens of DCE, one can systematically quantify how well compilers optimize code. We conduct an extensive analysis of GCC and LLVM using our approach, which (1) provides quantitative and qualitative insights regarding their optimization capabilities, and (2) uncovers a diverse set of missed optimizations. Our results also lead to 84 bug reports for GCC and LLVM, of which 62 have already been confirmed or fixed, demonstrating our work's strong practical utility. We expect that the simplicity and generality of our approach will make it widely applicable for understanding compiler performance and finding missed optimizations. This work opens and initiates this promising direction.",
      "paperUrl": "https://doi.org/10.1145/3503222.3507764",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Theodoros Theodoridis"
      ]
    },
    {
      "id": "openalex-w4311263204",
      "source": "openalex-discovery",
      "title": "FPChecker: Floating-Point Exception Detection Tool and Benchmark for Parallel and Distributed HPC",
      "authors": [
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Tanmay Tirpankar",
          "affiliation": "University of Utah"
        },
        {
          "name": "Xinyi Li",
          "affiliation": "University of Utah"
        },
        {
          "name": "Ganesh Gopalakrishnan",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Floating-point arithmetic is fundamental to many areas including high-performance computing and machine learning. In order to ensure the numerical integrity of the overall computation, numerical exceptions (such as NaNs) must be detected and suitably reported to the user. Unfortunately, today's best available methods and tools are not general-enough. Moreover, there are no comprehensive benchmarks today that can be compiled to intermediate representations such as LLVM and analyzed at that level. In this paper, we contribute the first such benchmark suite that spans five HPC proxy applications plus eight public benchmarks that run on CPU multicores under MPI, OpenMP, and performance portability layers. We also release our tool (also called LLFPX) that is up to 7.9× faster on many important benchmarks and overall more comprehensive with respect to exception variety coverage. This paper presents the tool, its design, and evaluation on our benchmarks, with the tool, benchmarks, and a facility to examine results on the web available for public use upon acceptance. Other result highlights include the effect of compiler optimizations on exceptions.",
      "paperUrl": "https://doi.org/10.1109/iiswc55918.2022.00014",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Ganesh Gopalakrishnan",
        "Ignacio Laguna"
      ]
    },
    {
      "id": "openalex-w4285595655",
      "source": "openalex-discovery",
      "title": "FFTc: An MLIR Dialect for Developing HPC Fast Fourier Transform Libraries",
      "authors": [
        {
          "name": "Yifei He",
          "affiliation": ""
        },
        {
          "name": "Artur Podobas",
          "affiliation": ""
        },
        {
          "name": "Måns I. Andersson",
          "affiliation": ""
        },
        {
          "name": "Stefano Markidis",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Discrete Fourier Transform (DFT) libraries are one of the most critical software components for scientific computing. Inspired by FFTW, a widely used library for DFT HPC calculations, we apply compiler technologies for the development of HPC Fourier transform libraries. In this work, we introduce FFTc, a domain-specific language, based on Multi-Level Intermediate Representation (MLIR), for expressing Fourier Transform algorithms. We present the initial design, implementation, and preliminary results of FFTc.",
      "paperUrl": "https://arxiv.org/pdf/2207.06803",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2207.06803",
      "tags": [
        "Libraries",
        "MLIR"
      ],
      "matchedAuthors": [
        "Yifei He"
      ]
    },
    {
      "id": "openalex-w4308090763",
      "source": "openalex-discovery",
      "title": "Enabling Transformers to Understand Low-Level Programs",
      "authors": [
        {
          "name": "Zifan Carl Guo",
          "affiliation": ""
        },
        {
          "name": "William S. Moses",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Unlike prior approaches to machine learning, Transformer models can first be trained on a large corpus of unlabeled data with a generic objective and then on a smaller task-specific dataset. This versatility has led to both larger models and datasets. Consequently, Transformers have led to breakthroughs in the field of natural language processing. Generic program optimization presently operates on low-level programs such as LLVM. Unlike the high-level languages (e.g. C, Python, Java), which have seen initial success in machine-learning analyses, lower-level languages tend to be more verbose and repetitive to precisely specify program behavior, provide more details about microarchitecture, and derive properties necessary for optimization, all of which makes it difficult for machine learning. In this work, we apply transfer learning to low-level (LLVM) programs and study how low-level programs can be made more amenable to Transformer models through various techniques, including preprocessing, infix/prefix operators, and information deduplication. We evaluate the effectiveness of these techniques through a series of ablation studies on the task of translating C to both unoptimized (-O0) and optimized (-01) LLVM IR. On the AnghaBench dataset, our model achieves a 49.57% verbatim match and BLEU score of 87.68 against Clang -O0 and 38.73% verbatim match and BLEU score of 77.03 against Clang -O1.",
      "paperUrl": "https://doi.org/10.1109/hpec55821.2022.9926313",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "IR"
      ],
      "matchedAuthors": [
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4285586681",
      "source": "openalex-discovery",
      "title": "ESBMC-CHERI: towards verification of C programs for CHERI platforms with ESBMC",
      "authors": [
        {
          "name": "Franz Brauße",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Fedor Shmarov",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Rafael Menezes",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Mikhail R. Gadelha",
          "affiliation": ""
        },
        {
          "name": "Konstantin Korovin",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Giles Reger",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Lucas C. Cordeiro",
          "affiliation": "University of Manchester"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper presents ESBMC-CHERI -- the first bounded model checker capable of formally verifying C programs for CHERI-enabled platforms. CHERI provides run-time protection for the memory-unsafe programming languages such as C/C++ at the hardware level. At the same time, it introduces new semantics to C programs, making some safe C programs cause hardware exceptions on CHERI-extended platforms. Hence, it is crucial to detect memory safety violations and compatibility issues ahead of compilation. However, there are no current verification tools for reasoning over CHERI-C programs. We demonstrate the work undertaken towards implementing support for CHERI-C in our state-of-the-art bounded model checker ESBMC and the plans for future work and extensive evaluation of ESBMC-CHERI. The ESBMC-CHERI demonstration and the source code are available at https://github.com/esbmc/esbmc/tree/cheri-clang.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3533767.3543289",
      "sourceUrl": "https://doi.org/10.1145/3533767.3543289",
      "tags": [
        "C++",
        "Clang",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Mikhail R. Gadelha"
      ]
    },
    {
      "id": "openalex-w4318603192",
      "source": "openalex-discovery",
      "title": "Direct GPU Compilation and Execution for Host Applications with OpenMP Parallelism",
      "authors": [
        {
          "name": "Shilei Tian",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Joseph Huber",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Konstantinos Parasyris",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Currently, offloading to accelerators requires users to identify which regions are to be executed on the device, what memory needs to be transferred, and how synchronization is to be resolved. On top of these manual tasks, many standard (C/ C++ library) functions, such as file I/O or memory manipulation, cannot be directly executed on the device and need to be worked around by the user explicitly. This makes it challenging to port programs in the first place and hinders developers from testing features on the GPU and within the GPU compilation pipeline. Existing tests and test suites for the host are effectively unusable for accelerators and need to be manually ported to provide the same benefits for the devices as they do on the host. In this paper, we propose a direct GPU compilation scheme that leverages the portable target offloading interface provided by LLVM/OpenMP. Utilizing this infrastructure allows us to compile an existing host application for the GPU and execute it there with only a minimal wrapper layer for the user code, command line arguments, and a compiler provided GPU implementation of C/ C++ standard library functions. The C/ C++ library functions are partially implemented for direct device execution and otherwise fallback to remote procedure call (RPC) to call host functions transparently. Our proposed prototype will allow users to quickly compile for, and test on, the GPU without explicitly handling kernel launches, data mapping, or host-device synchronization. We evaluate our implementation using three proxy applications with host OpenMP parallelism and three microbenchmarks to test the correctness of our prototype GPU compilation.",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc56686.2022.00010",
      "sourceUrl": "",
      "tags": [
        "C++",
        "GPU",
        "Infrastructure",
        "Testing"
      ],
      "matchedAuthors": [
        "Barbara Chapman",
        "Johannes Doerfert",
        "Joseph Huber",
        "Konstantinos Parasyris",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4297649502",
      "source": "openalex-discovery",
      "title": "DFI: An Interprocedural Value-Flow Analysis Framework that Scales to Large Codebases",
      "authors": [
        {
          "name": "Min-Yih Hsu",
          "affiliation": ""
        },
        {
          "name": "Felicitas Hetzelt",
          "affiliation": ""
        },
        {
          "name": "Michael Franz",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Context- and flow-sensitive value-flow information is an important building block for many static analysis tools. Unfortunately, current approaches to compute value-flows do not scale to large codebases, due to high memory and runtime requirements. This paper proposes a new scalable approach to compute value-flows via graph reachability. To this end, we develop a new graph structure as an extension of LLVM IR that contains two additional operations which significantly simplify the modeling of pointer aliasing. Further, by processing nodes in the opposite direction of SSA def-use chains, we are able to minimize the tree width of the resulting graph. This allows us to employ efficient tree traversal algorithms in order to resolve graph reachability. We present a value-flow analysis framework,DFI, implementing our approach. We compare DFI against two state-of-the-art value-flow analysis frameworks, Phasar and SVF, to extract value-flows from 4 real-world software projects. Given 32GB of memory, Phasar and SVF are unable to complete analysis of larger projects such as OpenSSL or FFmpeg, while DFI is able to complete all evaluations. For the subset of benchmarks that Phasar and SVF do handle, DFI requires significantly less memory (1.5% of Phasar's, 6.4% of SVF's memory footprint on average) and runs significantly faster (23x speedup over Phasar, 57x compared to SVF). Our analysis shows that, in contrast to previous approaches, DFI's memory and runtime requirements scale almost linearly with the number of analyzed instructions.",
      "paperUrl": "https://arxiv.org/pdf/2209.02638",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2209.02638",
      "tags": [
        "IR",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Michael Franz",
        "Min-Yih Hsu"
      ]
    },
    {
      "id": "openalex-w4214634252",
      "source": "openalex-discovery",
      "title": "Creating concise and efficient dynamic analyses with ALDA",
      "authors": [
        {
          "name": "Xiang Cheng",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "David Devecsery",
          "affiliation": "Georgia Institute of Technology"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Dynamic program analyses are essential to creating safe, reliable, and productive computing environments. However, these analyses are challenging and time-consuming to construct due to the low-level optimization required to achieve acceptable performance. Consequently, many analyses are often never realized, or have inefficient implementations. In this work we argue that many analyses can and should be constructed with a high-level description language, leaving the burden of low-level optimizations to the analysis instrumentation system itself. We propose a novel language for dynamic analysis called ALDA. ALDA leverages common structuring of dynamic analyses to provide a simple and high-level description for dynamic analyses. Through restricting the supported behaviors to only essential operations in dynamic analyses, an optimizing compiler for ALDA can create analysis implementations with performance on-par to hand-tuned analyses. To demonstrate ALDA's universality and efficiency, we create an optimizing compiler for ALDA targeting the LLVM instrumentation framework named ALDAcc. We use ALDAcc to construct 8 different dynamic analysis algorithms, including the popular MemorySanitizer analysis, and show their construction is succinct and simple. By comparing two of them (Eraser and MemorySanitizer) with their hand-tuned implementations, we show that ALDAcc's optimized analyses are comparable to hand-tuned implementations.",
      "paperUrl": "https://doi.org/10.1145/3503222.3507760",
      "sourceUrl": "",
      "tags": [
        "Dynamic Analysis",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "David Devecsery"
      ]
    },
    {
      "id": "openalex-w4313549812",
      "source": "openalex-discovery",
      "title": "Cornucopia : A Framework for Feedback Guided Generation of Binaries",
      "authors": [
        {
          "name": "Vidush Singhal",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Akul Abhilash Pillai",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Charitha Saumya",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Milind Kulkarni",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Aravind Machiry",
          "affiliation": "Purdue University West Lafayette"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Binary analysis is an important capability required for many security and software engineering applications. Consequently, there are many binary analysis techniques and tools with varied capabilities. However, testing these tools requires a large, varied binary dataset with corresponding source-level information. In this paper, we present Cornucopia, an architecture agnostic automated framework that can generate a plethora of binaries from corresponding program source by exploiting compiler optimizations and feedback-guided learning. Our evaluation shows that Cornucopia was able to generate 309K binaries across four architectures (x86, x64, ARM, MIPS) with an average of 403 binaries for each program and outperforms Bintuner, a similar technique. Our experiments revealed issues with the LLVM optimization scheduler resulting in compiler crashes ($\\sim$300). Our evaluation of four popular binary analysis tools Angr, Ghidra, Idapro, and Radare, using Cornucopia generated binaries, revealed various issues with these tools. Specifically, we found 263 crashes in Angr and one memory corruption issue in Idapro. Our differential testing on the analysis results revealed various semantic bugs in these tools. We also tested machine learning tools, Asmvec, Safe, and Debin, that claim to capture binary semantics and show that they perform poorly (For instance, Debin F1 score dropped to 12.9% from reported 63.1%) on Cornucopia generated binaries. In summary, our exhaustive evaluation shows that Cornucopia is an effective mechanism to generate binaries for testing binary analysis techniques effectively.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3551349.3561152",
      "sourceUrl": "https://doi.org/10.1145/3551349.3561152",
      "tags": [
        "Optimizations",
        "Security",
        "Testing"
      ],
      "matchedAuthors": [
        "Charitha Saumya"
      ]
    },
    {
      "id": "openalex-w4226383816",
      "source": "openalex-discovery",
      "title": "Compiler-Driven Simulation of Reconfigurable Hardware Accelerators",
      "authors": [
        {
          "name": "Zhijing Li",
          "affiliation": "Cornell University"
        },
        {
          "name": "Yuwei Ye",
          "affiliation": "Cornell University"
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": "Xilinx (United States)"
        },
        {
          "name": "Adrian Sampson",
          "affiliation": "Cornell University"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As customized accelerator design has become increasingly popular to keep up with the demand for high performance computing, it poses challenges for modern simulator design to adapt to such a large variety of accelerators. Existing simulators tend to two extremes: low-level and general approaches, such as RTL simulation, that can model any hardware but require substantial effort and long execution times; and higher-level application-specific models that can be much faster and easier to use but require one-off engineering effort.This work proposes a compiler-driven simulation workflow that can model configurable hardware accelerator. The key idea is to separate structure representation from simulation by developing an intermediate language that can flexibly represent a wide variety of hardware constructs. We design the Event Queue (EQueue) dialect of MLIR, a dialect that can model arbitrary hardware accelerators with explicit data movement and distributed event-based control; we also implement a generic simulation engine to model EQueue programs with hybrid MLIR dialects representing different abstraction levels. We demonstrate two case studies of EQueue-implemented accelerators: the systolic array of convolution and SIMD processors in a modern FPGA. In the former we show EQueue simulation is as accurate as a state-of-the-art simulator, while offering higher extensibility and lower iteration cost via compiler passes. In the latter we demonstrate our simulation flow can guide designer efficiently improve their design using visualizable simulation outputs.",
      "paperUrl": "https://doi.org/10.1109/hpca53966.2022.00052",
      "sourceUrl": "",
      "tags": [
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w4283372843",
      "source": "openalex-discovery",
      "title": "Compiler-Aided Type Correctness of Hybrid MPI-OpenMP Applications",
      "authors": [
        {
          "name": "Alexander Hück",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Sebastian Kreutzer",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Joachim Protze",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Jan-Patrick Lehr",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Christian Bischof",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Christian Terboven",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Matthias Müller",
          "affiliation": "RWTH Aachen University"
        }
      ],
      "year": "2022",
      "venue": "IT Professional | Vol. 24 (Issue 2)",
      "type": "research-paper",
      "abstract": "Hybrid MPI–OpenMP applications employ message-passing interface (MPI)-enabled process-level, distributed computations on many compute nodes in conjunction with OpenMP shared-memory, thread-level parallelism for the most efficient computation. This poses challenges on the dynamic MPI correctness tool MUST and TypeART, its memory allocation tracking sanitizer extension based on the LLVM compiler framework. In particular, at the thread-level granularity of a process, MPI calls and memory allocations, which are both tracked for our analysis, can occur concurrently. To correctly handle this situation, we: 1) extended our compiler extension to handle OpenMP and 2) introduced thread-safety mechanisms to our runtime libraries, thus keeping the tracking of data consistent and avoiding data races. Our approach exhibits acceptable runtime and memory overheads, both typically below 30%.",
      "paperUrl": "https://doi.org/10.1109/mitp.2021.3093949",
      "sourceUrl": "",
      "tags": [
        "Libraries"
      ],
      "matchedAuthors": [
        "Alexander Hück",
        "Joachim Protze",
        "Sebastian Kreutzer"
      ]
    },
    {
      "id": "openalex-w4313423325",
      "source": "openalex-discovery",
      "title": "Compiler Support for an AI-oriented SIMD Extension of a Space Processor",
      "authors": [
        {
          "name": "Marc Solé",
          "affiliation": "Barcelona Supercomputing Center"
        },
        {
          "name": "Leonidas Kosmidis",
          "affiliation": "Universitat Politècnica de Catalunya"
        }
      ],
      "year": "2022",
      "venue": "ACM SIGAda Ada Letters | Vol. 42 (Issue 1)",
      "type": "research-paper",
      "abstract": "In this on going research paper, we present our work on the compiler support for an AI-oriented SIMD Extension, called SPARROW. The SPARROW hardware design has been developed during a recently defended, awardwinning Master Thesis and is targeting Cobham Gaisler's space processors Leon3 and NOEL-V. We present the compiler support we have included in two compiler toolchains, gcc and llvm as well as a SIMD intrinsics library for easy programmability. Compiler modifications are kept to minimum in order to enable incremental qualification of the toolchains. We present our experience working with the two compilers and performance results for the two compilers on top an FPGA implementation of the target space processor.",
      "paperUrl": "http://hdl.handle.net/2117/389445",
      "sourceUrl": "https://doi.org/10.1145/3577949.3577968",
      "tags": [
        "AI",
        "Performance"
      ],
      "matchedAuthors": [
        "Leonidas Kosmidis"
      ]
    },
    {
      "id": "openalex-w4290648346",
      "source": "openalex-discovery",
      "title": "Compiler Support for Sparse Tensor Computations in MLIR",
      "authors": [
        {
          "name": "Aart J. C. Bik",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Penporn Koanantakool",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Tatiana Shpeisman",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Nicolas Vasilache",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Bixia Zheng",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Fredrik Kjølstad",
          "affiliation": "Stanford University"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 19 (Issue 4)",
      "type": "research-paper",
      "abstract": "Sparse tensors arise in problems in science, engineering, machine learning, and data analytics. Programs that operate on such tensors can exploit sparsity to reduce storage requirements and computational time. Developing and maintaining sparse software by hand, however, is a complex and error-prone task. Therefore, we propose treating sparsity as a property of tensors, not a tedious implementation task, and letting a sparse compiler generate sparse code automatically from a sparsity-agnostic definition of the computation. This article discusses integrating this idea into MLIR.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3544559",
      "sourceUrl": "https://doi.org/10.1145/3544559",
      "tags": [
        "MLIR"
      ],
      "matchedAuthors": [
        "Tatiana Shpeisman"
      ]
    },
    {
      "id": "openalex-w4285024991",
      "source": "openalex-discovery",
      "title": "Code Translation with Compiler Representations",
      "authors": [
        {
          "name": "Marc Szafraniec",
          "affiliation": ""
        },
        {
          "name": "Baptiste Rozière",
          "affiliation": ""
        },
        {
          "name": "Hugh Leather",
          "affiliation": ""
        },
        {
          "name": "François Charton",
          "affiliation": ""
        },
        {
          "name": "Patrick Labatut",
          "affiliation": ""
        },
        {
          "name": "Gabriel Synnaeve",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we leverage low-level compiler intermediate representations (IR) to improve code translation. Traditional transpilers rely on syntactic information and handcrafted rules, which limits their applicability and produces unnatural-looking code. Applying neural machine translation (NMT) approaches to code has successfully broadened the set of programs on which one can get a natural-looking translation. However, they treat the code as sequences of text tokens, and still do not differentiate well enough between similar pieces of code which have different semantics in different languages. The consequence is low quality translation, reducing the practicality of NMT, and stressing the need for an approach significantly increasing its accuracy. Here we propose to augment code translation with IRs, specifically LLVM IR, with results on the C++, Java, Rust, and Go languages. Our method improves upon the state of the art for unsupervised code translation, increasing the number of correct translations by 11% on average, and up to 79% for the Java -&gt; Rust pair with greedy decoding. We extend previous test sets for code translation, by adding hundreds of Go and Rust functions. Additionally, we train models with high performance on the problem of IR decompilation, generating programming source code from IR, and study using IRs as intermediary pivot for translation.",
      "paperUrl": "https://arxiv.org/pdf/2207.03578",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2207.03578",
      "tags": [
        "C++",
        "IR",
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w4285504014",
      "source": "openalex-discovery",
      "title": "Co-Designing an OpenMP GPU Runtime and Optimizations for Near-Zero Overhead Execution",
      "authors": [
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Atmn Patel",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "Joseph Huber",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Shilei Tian",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Jose M. Monsalve Diaz",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2022",
      "venue": "2022 IEEE International Parallel and Distributed Processing Symposium (IPDPS) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "GPU accelerators are ubiquitous in modern HPC systems. To program them, users have the choice between vendor-specific, native programming models, such as CUDA, which provide simple parallelism semantics with minimal runtime support, or portable alternatives, such as OpenMP, which offer rich parallel semantics and feature an extensive runtime library to support execution. While the operations of such a runtime can easily limit performance and drain resources, it was to some degree regarded an unavoidable overhead. In this work we present a co-design methodology for optimizing applications using a specifically crafted OpenMP GPU runtime such that most use cases induce near-zero overhead. Specifically, our approach exposes runtime semantics and state to the compiler such that optimization effectively eliminating abstractions and runtime state from the final binary. With the help of user provided assumptions we can further optimize common patterns that otherwise increase resource consumption. We evaluated our prototype build on top of the LLVM/OpenMP GPU offloading infrastructure with multiple HPC proxy applications and benchmarks. Comparison of CUDA, the original OpenMP runtime, and our co-designed alternative show that, by our approach, performance is significantly improved and resource consumption is significantly lowered. Oftentimes we can closely match the CUDA implementation without sacrificing the versatility and portability of OpenMP.",
      "paperUrl": "https://www.osti.gov/biblio/1890094",
      "sourceUrl": "https://doi.org/10.1109/ipdps53621.2022.00055",
      "tags": [
        "CUDA",
        "GPU",
        "Infrastructure",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Barbara Chapman",
        "Giorgis Georgakoudis",
        "Johannes Doerfert",
        "Jose M Monsalve Diaz",
        "Joseph Huber",
        "Shilei Tian"
      ]
    },
    {
      "id": "openalex-w4225270187",
      "source": "openalex-discovery",
      "title": "C++OpenCL4TVM: Support C++OpenCL Kernel for TVM NN Operators",
      "authors": [
        {
          "name": "Po-Yao Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Tai-Liang Chen",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yu-Tse Huang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Meng-Shiun Yu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2022",
      "venue": "International Workshop on OpenCL | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In an era of artificial intelligence (AI), OpenCL serves as one of the AI frameworks' back-ends, notably, the tensor virtual machine (TVM), which focuses on the inference side of neural networks. After optimizing a computational graph, TVM traverses the internal representations, Tensor-level IR (TIR), of each neural network (NN) operator generating OpenCL kernels for each one of them. In this work, we make TVM generate C++ for OpenCL, compile it to SPIR-V binary, and consume it with clCreateProgramWithIL inside TVM after we transform it by adding C[2]++ for_each and providing unseq as its argument. We also bumped into an llvm-spirv issue along the way. Finally, we found a workaround and proceeded to runnable TVM-generated C++ for OpenCL kernels.",
      "paperUrl": "https://doi.org/10.1145/3529538.3530001",
      "sourceUrl": "",
      "tags": [
        "AI",
        "C++",
        "IR",
        "OpenCL"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4292738089",
      "source": "openalex-discovery",
      "title": "BullsEye : Scalable and Accurate Approximation Framework for Cache Miss Calculation",
      "authors": [
        {
          "name": "Nilesh Rajendra Shah",
          "affiliation": "Centre National de la Recherche Scientifique"
        },
        {
          "name": "Ashitabh Misra",
          "affiliation": "Centre National de la Recherche Scientifique"
        },
        {
          "name": "Antoine Miné",
          "affiliation": "Centre National de la Recherche Scientifique"
        },
        {
          "name": "Rakesh Venkat",
          "affiliation": "Centre National de la Recherche Scientifique"
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": "Centre National de la Recherche Scientifique"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 20 (Issue 1)",
      "type": "research-paper",
      "abstract": "For Affine Control Programs or Static Control Programs (SCoP), symbolic counting of reuse distances could induce polynomials for each reuse pair. These polynomials along with cache capacity constraints lead to non-affine (semi-algebraic) sets; and counting these sets is considered to be a hard problem. The state-of-the-art methods use various exact enumeration techniques relying on existing cardinality algorithms that can efficiently count affine sets. We propose BullsEye , a novel, scalable, accurate, and problem-size independent approximation framework. It is an analytical cache model for fully associative caches with LRU replacement policy focusing on sampling and linearization of non-affine stack distance polynomials. First, we propose a simple domain sampling method that can improve the scalability of exact enumeration. Second, we propose linearization techniques relying on Handelman’s theorem and Bernstein’s representation . To improve the scalability of the Handelman’s theorem linearization technique, we propose template (Interval or Octagon) sub-polyhedral approximations. Our methods obtain significant compile-time improvements with high-accuracy when compared to HayStack on important polyhedral compilation kernels such as nussinov , cholesky , and adi from PolyBench , and harris , gaussianblur from LLVM -TestSuite. Overall, on PolyBench kernels, our methods show up to 3.31× (geomean) speedup with errors below ≈ 0.08% (geomean) for the octagon sub-polyhedral approximation.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3558003",
      "sourceUrl": "https://doi.org/10.1145/3558003",
      "tags": [],
      "matchedAuthors": [
        "Ramakrishna Upadrasta"
      ]
    },
    {
      "id": "openalex-w4214849268",
      "source": "openalex-discovery",
      "title": "Buddy Stacks: Protecting Return Addresses with Efficient Thread-Local Storage and Runtime Re-Randomization",
      "authors": [
        {
          "name": "Changwei Zou",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Xudong Wang",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Yaoqing Gao",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2022",
      "venue": "ACM Transactions on Software Engineering and Methodology | Vol. 31 (Issue 2)",
      "type": "research-paper",
      "abstract": "Shadow stacks play an important role in protecting return addresses to mitigate ROP attacks. Parallel shadow stacks, which shadow the call stack of each thread at the same constant offset for all threads, are known not to support multi-threading well. On the other hand, compact shadow stacks must maintain a separate shadow stack pointer in thread-local storage (TLS) , which can be implemented in terms of a register or the per-thread Thread-Control-Block (TCB) , suffering from poor compatibility in the former or high performance overhead in the latter. In addition, shadow stacks are vulnerable to information disclosure attacks. In this paper, we propose to mitigate ROP attacks for single- and multi-threaded server programs running on general-purpose computing systems by using a novel stack layout, called a buddy stack (referred to as Bustk ), that is highly performant, compatible with existing code, and provides meaningful security. These goals are met due to three novel design aspects in Bustk . First, Bustk places a parallel shadow stack just below a thread’s call stack (as each other’s buddies allocated together), avoiding the need to maintain a separate shadow stack pointer and making it now well-suited for multi-threading. Second, Bustk uses an efficient stack-based thread-local storage mechanism, denoted STK-TLS , to store thread-specific metadata in two TLS sections just below the shadow stack in dual redundancy (as each other’s buddies), so that both can be accessed and updated in a lightweight manner from the call stack pointer rsp alone. Finally, Bustk re-randomizes continuously (on the order of milliseconds) the return addresses on the shadow stack by using a new microsecond-level runtime re-randomization technique, denoted STK-MSR . This mechanism aims to obsolete leaked information, making it extremely unlikely for the attacker to hijack return addresses, particularly against a server program that sits often tens of milliseconds away from the attacker. Our evaluation using web servers, Nginx and Apache Httpd , shows that Bustk works well in terms of performance, compatibility, and security provided, with its parallel shadow stacks incurring acceptable memory overhead for real-world applications and its STK-TLS mechanism costing only two pages per thread. In particular, Bustk can protect the Nginx and Apache servers with an adaptive 1-ms re-randomization policy (without observable overheads when IO is intensive, with about 17,000 requests per second). In addition, we have also evaluated Bustk using other non-server applications, Firefox , Python , LLVM , JDK and SPEC CPU2006 , to demonstrate further the same degree of performance and compatibility provided, but the protection provided for, say, browsers, is weaker (since network-access delays can no longer be assumed).",
      "paperUrl": "http://hdl.handle.net/1959.4/unsworks_83696",
      "sourceUrl": "https://doi.org/10.1145/3494516",
      "tags": [
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Jingling Xue"
      ]
    },
    {
      "id": "openalex-w4313126055",
      "source": "openalex-discovery",
      "title": "Bring the BitCODE-Moving Compute and Data in Distributed Heterogeneous Systems",
      "authors": [
        {
          "name": "Wenbin Lu",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Luis E. Peña",
          "affiliation": "American Rock Mechanics Association"
        },
        {
          "name": "Pavel Shamis",
          "affiliation": "American Rock Mechanics Association"
        },
        {
          "name": "Valentin Churavy",
          "affiliation": "IIT@MIT"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Steve Poole",
          "affiliation": "Los Alamos National Laboratory"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we present a framework for moving compute and data between processing elements in a distributed heterogeneous system. The implementation of the framework is based on the LLVM compiler toolchain combined with the UCX communication framework. The framework can generate binary machine code or LLVM bitcode for multiple CPU architectures and move the code to remote machines while dynamically optimizing and linking the code on the target platform. The remotely injected code can recursively propagate itself to other remote machines or generate new code. The goal of this paper is threefold: (a) to present an architecture and implementation of the framework that provides essential infrastructure to program a new class of disaggregated systems wherein heterogeneous programming elements such as compute nodes and data processing units (DPUs) are distributed across the system, (b) to demonstrate how the framework can be integrated with modern, high-level programming languages such as Julia, and (c) to demonstrate and evaluate a new class of eXtended Remote Direct Memory Access (X-RDMA) communication operations that are enabled by this framework. To evaluate the capabilities of the framework, we used a cluster with Fujitsu CPUs and heterogeneous cluster with Intel CPUs and BlueField-2 DPUs interconnected using high-performance RDMA fabric. We demonstrated an X-RDMA pointer chase application that outperforms an RDMA GET-based implementation by 70% and is as fast as Active Messages, but does not require function predeployment on remote platforms.",
      "paperUrl": "https://arxiv.org/pdf/2208.01154",
      "sourceUrl": "https://doi.org/10.1109/cluster51413.2022.00017",
      "tags": [
        "Infrastructure",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Barbara Chapman",
        "Valentin Churavy"
      ]
    },
    {
      "id": "openalex-w4220795190",
      "source": "openalex-discovery",
      "title": "BUILDING BRIDGES BETWEEN ACADEMY AND INDUSTRY: IMPROVING CRUCIAL DEVELOPMENT SKILLS THROUGH PARTICIPATING IN OPEN SOURCE PROJECTS",
      "authors": [
        {
          "name": "Anett Fekete",
          "affiliation": ""
        },
        {
          "name": "Zoltán Gera",
          "affiliation": ""
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "INTED proceedings | Vol. 1 (Issue None)",
      "type": "research-paper",
      "abstract": "Computer science trainings in Hungarian universities often get criticized for providing too many theoretical courses, and not enough practice to make students suitable for immediately producing value in a workplace after they graduate. This is partly legitimate, since universities intend to equip students with lots of theoretical knowledge which will later help them easily understand and engineer large software systems. However, long-standing practical skills should also be instilled. In this paper, we describe the workings of the Model-C++ software technology lab in the Faculty of Informatics, Eötvös Loránd University (ELTE), and how taking part in open source software development prepares students for industrial work. Most students want to join real-life software development projects as soon as possible, in order to gain useful experience which they can easily utilize when they enter the world of work. We expect such inquiring students in our software technology lab.Software development in the lab is built on inevitable skills, such as confident usage of at least one widely used programming language (C, C++, Python, Java, C# etc.), basic knowledge in version control systems (git, SVN) and compilers, and, since our students mostly come from trainings that are held in Hungarian, at least intermediate knowledge of English.The two main projects in the lab that students can join are CodeChecker and CodeCompass. Both are open source, multi-language projects which are developed by ELTE and Ericsson. CodeChecker consists of more than 1000 files, and is mainly developed in Python. Students are first invited to join the development of Clang-Tidy, then they can also contribute to the more complex Clang Static Analyzer. Developers from major companies like Google and Apple take part in the review process of the submitted checkers. CodeCompass consists of more than 400 files. It is first and foremost developed in C++, but the pluginable framework allows a plugin to be written in any language. CodeCompass is used by multiple multinational companies, e.g. Ericsson, Intel, and Graphisoft. Both projects are developed in cooperation with the listed companies, which entails immediate feedback of the new functionalities through industrial usage. This way it is also guaranteed that no useless or unnecessary additions are made to the projects.The students in the lab can learn about various version control strategies that are common practice in large software projects. They have to actively participate in the review process of each other’s code, thus they are introduced to the usual team work of software development, and learn to argue for their coding decisions and solutions. The projects they work on are also rather large and long-running, throughout which we teach architectural skills to students.In the last 6 to 8 years, 10-15 students took part in either project through the lab a year on average. All of them got employed shortly after graduation at various workplaces, including multinational companies and smaller startups. In addition to their jobs, some of the students keep contributing to the projects even after graduation, which greatly helps the open source community.",
      "paperUrl": "https://doi.org/10.21125/inted.2022.0515",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4320061944",
      "source": "openalex-discovery",
      "title": "Automatic Asynchronous Execution of Synchronously Offloaded OpenMP Target Regions",
      "authors": [
        {
          "name": "Rafael A. Herrera Guaitero",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Jose M. Monsalve Diaz",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Thomas Applencourt",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Xiaoming Li",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2022",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Use of heterogeneous architectures has steadily increased during the past decade. However, non-homogeneous systems present a challenge to the programming model as the execution models between CPU and accelerator might differ considerably. OpenMP, since version 4.0, has been trying to bridge this gap by allowing to offload a code block to a target device. Among the additions to the OpenMP offloading API since, the most notably probably is asynchronous execution between device and host. By default, offloaded regions are executed synchronously, thus the host thread blocks until their completion. The nowait clause allows work to overlap between the host and target device. However, nowait must be manually added by the user, along with the tasks data dependencies and appropriate synchronization to avoid race conditions, increasing the program complexity and developer burden. In this work, we present automatic asynchronous execution for OpenMP offloaded regions. By taking advantage of the distinct host and target data environments, we discover opportunities that allow them to overlap execution without any need for user intervention. We also describe the necessary changes in the LLVM/OpenMP runtime. We evaluate our implementation through multiple HPC proxy applications and well known parallel benchmarks executed on GPUs. The measured performance can double for an ideal test case while real application exhibit speedups between 5% and 34%.",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc56686.2022.00008",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Johannes Doerfert",
        "Jose M Monsalve Diaz",
        "Rafael A Herrera Guaitero",
        "Xiaoming Li"
      ]
    },
    {
      "id": "openalex-w4307886774",
      "source": "openalex-discovery",
      "title": "AnICA: analyzing inconsistencies in microarchitectural code analyzers",
      "authors": [
        {
          "name": "Fabian Ritter",
          "affiliation": "Saarland University"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2022",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 6 (Issue OOPSLA2)",
      "type": "research-paper",
      "abstract": "Microarchitectural code analyzers, i.e., tools that estimate the throughput of machine code basic blocks, are important utensils in the tool belt of performance engineers. Recent tools like llvm-mca, uiCA, and Ithemal use a variety of techniques and different models for their throughput predictions. When put to the test, it is common to see these state-of-the-art tools give very different results. These inconsistencies are either errors, or they point to different and rarely documented assumptions made by the tool designers. In this paper, we present AnICA, a tool taking inspiration from differential testing and abstract interpretation to systematically analyze inconsistencies among these code analyzers. Our evaluation shows that AnICA can summarize thousands of inconsistencies in a few dozen descriptions that directly lead to high-level insights into the different behavior of the tools. In several case studies, we further demonstrate how AnICA automatically finds and characterizes known and unknown bugs in llvm-mca, as well as a quirk in AMD's Zen microarchitectures.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3563288",
      "sourceUrl": "https://doi.org/10.1145/3563288",
      "tags": [
        "Performance",
        "Testing"
      ],
      "matchedAuthors": [
        "Sebastian Hack"
      ]
    },
    {
      "id": "openalex-w4312121047",
      "source": "openalex-discovery",
      "title": "An MLIR-based Compiler Flow for System-Level Design and Hardware Acceleration",
      "authors": [
        {
          "name": "Nícolas Bohm Agostini",
          "affiliation": ""
        },
        {
          "name": "Serena Curzel",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Vinay Amatya",
          "affiliation": ""
        },
        {
          "name": "Cheng Tan",
          "affiliation": ""
        },
        {
          "name": "Marco Minutoli",
          "affiliation": ""
        },
        {
          "name": "Vito Giovanni Castellana",
          "affiliation": ""
        },
        {
          "name": "Joseph Manzano",
          "affiliation": ""
        },
        {
          "name": "David Kaeli",
          "affiliation": "Universidad del Noreste"
        },
        {
          "name": "Antonino Tumeo",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "Proceedings of the 41st IEEE/ACM International Conference on Computer-Aided Design | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The generation of custom hardware accelerators for applications implemented within high-level productive programming frameworks requires considerable manual effort. To automate this process, we introduce SODA-OPT, a compiler tool that extends the MLIR infrastructure. SODA-OPT automatically searches, outlines, tiles, and pre-optimizes relevant code regions to generate high-quality accelerators through high-level synthesis. SODA-OPT can support any high-level programming framework and domain-specific language that interface with the MLIR infrastructure. By leveraging MLIR, SODA-OPT solves compiler optimization problems with specialized abstractions. Backend synthesis tools connect to SODA-OPT through progressive intermediate representation lowerings. SODAOPT interfaces to a design space exploration engine to identify the combination of compiler optimization passes and options that provides high-performance generated designs for different backends and targets. We demonstrate the practical applicability of the compilation flow by exploring the automatic generation of accelerators for deep neural networks operators outlined at arbitrary granularity and by combining outlining with tiling on large convolution layers. Experimental results with kernels from the PolyBench benchmark show that our high-level optimizations improve execution delays of synthesized accelerators up to 60x. We also show that for the selected kernels, our solution outperforms the current of state-ofthe art in more than 70% of the benchmarks and provides better average speedup in 55% of them. SODA-OPT is an open source project available at https://gitlab.pnnl.gov/sodalite/soda-opt.",
      "paperUrl": "https://hdl.handle.net/11311/1229389",
      "sourceUrl": "https://doi.org/10.1145/3508352.3549424",
      "tags": [
        "Backend",
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "David Kaeli"
      ]
    },
    {
      "id": "openalex-w4312770919",
      "source": "openalex-discovery",
      "title": "A Source-Level Instrumentation Framework for the Dynamic Analysis of Memory Safety",
      "authors": [
        {
          "name": "Zhe Chen",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Qi Zhang",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Jun Wu",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Yan Jun-qi",
          "affiliation": "Nanjing University of Aeronautics and Astronautics"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Software Engineering | Vol. 49 (Issue 4)",
      "type": "research-paper",
      "abstract": "Low-level control makes C unsafe, resulting in memory errors that can lead to data corruption, security vulnerabilities or program crashes. Dynamic analysis tools, which have been widely used for detecting memory errors at runtime, usually perform instrumentation at the IR or binary level. However, these non-source-level instrumentation frameworks and tools suffer from two inherent drawbacks: optimization sensitivity and platform dependence. Due to optimization sensitivity, the user of these tools must trade either performance for effectiveness by compiling the program at <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-O0</monospace> or effectiveness for performance by compiling the program at a higher optimization level, say, <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-O3</monospace> . In this paper, we propose a new source-level instrumentation framework to overcome these two drawbacks, and implement it in a new dynamic analysis tool, called <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Movec</small> , that adopts a pointer-based monitoring algorithm. We have evaluated <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Movec</small> comprehensively by using the NIST's SARD benchmark suite (1152 programs), a set of 126 microbenchmarks (with ground truth), a set of 20 MiBench benchmarks and 5 pure-C SPEC CPU 2017 benchmarks. In terms of effectiveness, <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Movec</small> outperforms three state-of-the-art dynamic analysis tools, AddressSanitizer, SoftBoundCETS and Valgrind, for all the standard optimization levels (from <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-O0</monospace> to <monospace xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-O3</monospace> ). In terms of performance, <sc xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">Movec</small> outperforms SoftBoundCETS and Valgrind, and is slower than AddressSanitizer but consumes less memory.",
      "paperUrl": "https://doi.org/10.1109/tse.2022.3210580",
      "sourceUrl": "",
      "tags": [
        "Dynamic Analysis",
        "IR",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Jingling Xue"
      ]
    },
    {
      "id": "openalex-w4221154427",
      "source": "openalex-discovery",
      "title": "A Highly Scalable, Hybrid, Cross-Platform Timing Analysis Framework Providing Accurate Differential Throughput Estimation via Instruction-Level Tracing",
      "authors": [
        {
          "name": "Min-Yih Hsu",
          "affiliation": ""
        },
        {
          "name": "David Gen�s",
          "affiliation": ""
        },
        {
          "name": "Michael Franz",
          "affiliation": ""
        },
        {
          "name": "Maitland, Michael",
          "affiliation": ""
        },
        {
          "name": "Franz, Michael",
          "affiliation": ""
        }
      ],
      "year": "2022",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Estimating instruction-level throughput is critical for many applications: multimedia, low-latency networking, medical, automotive, avionic, and industrial control systems all rely on tightly calculable and accurate timing bounds of their software. Unfortunately, how long a program may run - or if it may indeed stop at all - cannot be answered in the general case. This is why state-of-the-art throughput estimation tools usually focus on a subset of operations and make several simplifying assumptions. Correctly identifying these sets of constraints and regions of interest in the program typically requires source code, specialized tools, and dedicated expert knowledge. Whenever a single instruction is modified, this process must be repeated, incurring high costs when iteratively developing timing sensitive code in practice. In this paper, we present MCAD, a novel and lightweight timing analysis framework that can identify the effects of code changes on the microarchitectural level for binary programs. MCAD provides accurate differential throughput estimates by emulating whole program execution using QEMU and forwarding traces to LLVM for instruction-level analysis. This allows developers to iterate quickly, with low overhead, using common tools: identifying execution paths that are less sensitive to changes over timing-critical paths only takes minutes within MCAD. To the best of our knowledge this represents an entirely new capability that reduces turnaround times for differential throughput estimation by several orders of magnitude compared to state-of-the-art tools. Our detailed evaluation shows that MCAD scales to real-world applications like FFmpeg and Clang with millions of instructions, achieving &lt; 3% geo mean error compared to ground truth timings from hardware-performance counters on x86 and ARM machines.",
      "paperUrl": "https://arxiv.org/pdf/2201.04804",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2201.04804",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Michael Franz",
        "Min-Yih Hsu"
      ]
    },
    {
      "id": "openalex-w3155032774",
      "source": "openalex-discovery",
      "title": "Who’s debugging the debuggers? exposing debug information bugs in optimized binaries",
      "authors": [
        {
          "name": "Giuseppe Antonio Di Luna",
          "affiliation": "Sapienza University of Rome"
        },
        {
          "name": "Davide Italiano",
          "affiliation": "Apple (United States)"
        },
        {
          "name": "Luca Massarelli",
          "affiliation": "Sapienza University of Rome"
        },
        {
          "name": "Sebastian Österlund",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Cristiano Giuffrida",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Leonardo Querzoni",
          "affiliation": "Sapienza University of Rome"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Despite the advancements in software testing, bugs still plague deployed software and result in crashes in production. When debugging issues-sometimes caused by \"heisenbugs\"-there is the need to interpret core dumps and reproduce the issue offline on the same binary deployed. This requires the entire toolchain (compiler, linker, debugger) to correctly generate and use debug information. Little attention has been devoted to checking that such information is correctly preserved by modern toolchains' optimization stages. This is particularly important as managing debug information in optimized production binaries is non-trivial, often leading to toolchain bugs that may hinder post-deployment debugging efforts. In this paper, we present Debug2, a framework to find debug information bugs in modern toolchains. Our framework feeds random source programs to the target toolchain and surgically compares the debugging behavior of their optimized/unoptimized binary variants. Such differential analysis allows Debug2 to check invariants at each debugging step and detect bugs from invariant violations. Our invariants are based on the (in)consistency of common debug entities, such as source lines, stack frames, and function arguments. We show that, while simple, this strategy yields powerful cross-toolchain and cross-language invariants, which can pinpoint several bugs in modern toolchains. We have used Debug2 to find 23 bugs in the LLVM toolchain (clang/lldb), 8 bugs in the GNU toolchain (GCC/gdb), and 3 in the Rust toolchain (rustc/lldb)-with 14 bugs already fixed by the developers.",
      "paperUrl": "https://hdl.handle.net/11573/1555591",
      "sourceUrl": "https://doi.org/10.1145/3445814.3446695",
      "tags": [
        "Clang",
        "Debug Information",
        "LLDB",
        "Rust",
        "Testing"
      ],
      "matchedAuthors": [
        "Cristiano Giuffrida"
      ]
    },
    {
      "id": "openalex-w4205104372",
      "source": "openalex-discovery",
      "title": "Tutorial: LLVM for Security Practitioners",
      "authors": [
        {
          "name": "John Criswell",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Ethan Johnson",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Colin Pronovost",
          "affiliation": "University of Rochester"
        }
      ],
      "year": "2021",
      "venue": "Vol. 8559 (Issue None)",
      "type": "research-paper",
      "abstract": "Many security researchers need to build tools that analyze and transform code. For example, researchers may want to build security hardening tools, tools that find vulnerabilities within software, or tools that prove that a program is invulnerable to attack. This tutorial will guide attendees through creating extensions to the LLVM compiler that perform simple analysis and transformation operations.",
      "paperUrl": "https://doi.org/10.1109/secdev51306.2021.00016",
      "sourceUrl": "",
      "tags": [
        "Security"
      ],
      "matchedAuthors": [
        "Ethan Johnson",
        "John Criswell"
      ]
    },
    {
      "id": "openalex-w3189579529",
      "source": "openalex-discovery",
      "title": "Thread-Level Speculation Execution Model Based on LLVM Compiler",
      "authors": [
        {
          "name": "Deshuo Zhao",
          "affiliation": "Northwest A&F University"
        },
        {
          "name": "Bin Liu",
          "affiliation": "Northwest A&F University"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "With the trend of growing number of processing cores on Chip Multiprocessors, researchers have made a lot of efforts to make full use of core resources through extracting programs' parallelism. Thread-Level Speculation (TLS) can speculatively parallelized sequential dependences-carried programs in an aggressive manner by predicting dependent values in advance. However, the existing TLS execution model is error-prone and extra overhead will be introduced. To solve above problems, in this paper, a pure software-TLS execution model based on LLVM compiler is proposed with low-misspeculation and low-overhead. Firstly, an adaptive forking model is presented. This adaptive forking model can not only exploit higher degree of parallelism, but also reduce rollback rate. Secondly, an efficient memory management strategy is designed. With this strategy, the speculative status is separated from the non-speculative status, accelerating both communications among speculative threads and non-speculative threads, and its validation process with less time-consuming overhead. Experiments show that the presented speculative execution model can effectively extract sequential programs' parallelism, and it can achieve a maximum speedup of 3.76, average 2.59 times speedup on 8-cores processor. Compared with the traditional TLS execution model, the improved model delivers a 9% performance improvement on average.",
      "paperUrl": "https://doi.org/10.1145/3468691.3468707",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Bin Liu"
      ]
    },
    {
      "id": "openalex-w3186613262",
      "source": "openalex-discovery",
      "title": "ScaleHLS: Scalable High-Level Synthesis through MLIR",
      "authors": [
        {
          "name": "Hanchen Ye",
          "affiliation": ""
        },
        {
          "name": "Cong Hao",
          "affiliation": ""
        },
        {
          "name": "Jianyi Cheng",
          "affiliation": ""
        },
        {
          "name": "Hyunmin Jeong",
          "affiliation": ""
        },
        {
          "name": "Jack Huang",
          "affiliation": ""
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": ""
        },
        {
          "name": "Deming Chen",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-level Synthesis (HLS) has been widely adopted as it significantly improves the hardware design productivity and enables efficient design space exploration (DSE). HLS tools can be used to deliver solutions for many different kinds of design problems, which are often better solved with different levels of abstraction. While existing HLS tools are built using compiler infrastructures largely based on a single-level abstraction (e.g., LLVM), we propose ScaleHLS, a next-generation HLS compilation flow, on top of a multi-level compiler infrastructure called MLIR, for the first time. By using an intermediate representation (IR) that can be better tuned to particular algorithms at different representation levels, we are able to build this new HLS tool that is more scalable and customizable towards various applications coming with intrinsic structural or functional hierarchies. ScaleHLS is able to represent and optimize HLS designs at multiple levels of abstraction and provides an HLS-dedicated transform and analysis library to solve the optimization problems at the suitable representation levels. On top of the library, we also build an automated DSE engine to explore the multi-dimensional design space efficiently. In addition, we develop an HLS C front-end and a C/C++ emission back-end to translate HLS designs into/from MLIR for enabling the end-to-end ScaleHLS flow. Experimental results show that, comparing to the baseline designs only optimized by Xilinx Vivado HLS, ScaleHLS improves the performances with amazing quality-of-results -- up to 768.1x better on computation kernel level programs and up to 3825.0x better on neural network models.",
      "paperUrl": "https://arxiv.org/pdf/2107.11673",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Infrastructure",
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w4309500223",
      "source": "openalex-discovery",
      "title": "ScaleHLS: A New Scalable High-Level Synthesis Framework on Multi-Level Intermediate Representation",
      "authors": [
        {
          "name": "Hanchen Ye",
          "affiliation": ""
        },
        {
          "name": "Cong Hao",
          "affiliation": ""
        },
        {
          "name": "Jianyi Cheng",
          "affiliation": ""
        },
        {
          "name": "Hyunmin Jeong",
          "affiliation": ""
        },
        {
          "name": "Jack Huang",
          "affiliation": ""
        },
        {
          "name": "Stephen Neuendorffer",
          "affiliation": ""
        },
        {
          "name": "Deming Chen",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "High-level synthesis (HLS) has been widely adopted as it significantly improves the hardware design productivity and enables efficient design space exploration (DSE). Existing HLS tools are built using compiler infrastructures largely based on a single-level abstraction, such as LLVM. However, as HLS designs typically come with intrinsic structural or functional hierarchies, different HLS optimization problems are often better solved with different levels of abstractions. This paper proposes ScaleHLS, a new scalable and customizable HLS framework, on top of a multi-level compiler infrastructure called MLIR. ScaleHLS represents HLS designs at multiple representation levels and provides an HLS-dedicated analysis and transform library to solve the optimization problems at the suitable levels. Using this library, we provide a DSE engine to generate optimized HLS designs automatically. In addition, we develop an HLS C front-end and a C/C++ emission back-end to translate HLS designs into/from MLIR for enabling an end-to-end compilation flow. Experimental results show that, comparing to the baseline designs without manual directives insertion and code-rewriting, that are only optimized by Xilinx Vivado HLS, ScaleHLS improves the performances with amazing quality-of-results -- up to 768.1x better on computation kernel level programs and up to 3825.0x better on neural network models.",
      "paperUrl": "https://arxiv.org/pdf/2107.11673",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2107.11673",
      "tags": [
        "C++",
        "Infrastructure",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Stephen Neuendorffer"
      ]
    },
    {
      "id": "openalex-w3216765584",
      "source": "openalex-discovery",
      "title": "Robustness between Weak Memory Models",
      "authors": [
        {
          "name": "Soham Chakraborty",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "reposiTUm (TU Wien) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Robustness of a concurrent program ensures that its behaviors on a weak concurrency model are indistinguishable from those on a stronger model. Enforcing robustness is particularly useful when porting or migrating applications between architectures. Existing tools mostly focus on ensuring sequential consistency (SC) robustness which is a stronger condition and may result in unnecessary fences. To address this gap, we analyze and enforce robustness between weak memory models, more specifically for two mainstream architectures: x86 and ARM (versions 7 and 8). We identify robustness conditions and develop analysis techniques that facilitate porting an application between these architectures. To the best of our knowledge, this is the first approach that addresses robustness between the hardware weak memory models. We implement our robustness checking and enforcement procedure as a compiler pass in LLVM and experiment on a number of standard concurrent benchmarks. In almost all cases, our procedure terminates instantaneously and insert significantly less fences than the naive schemes that enforce SC-robustness.",
      "paperUrl": "https://doi.org/10.34727/2021/isbn.978-3-85448-046-4_26",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Soham Chakraborty"
      ]
    },
    {
      "id": "openalex-w3210510236",
      "source": "openalex-discovery",
      "title": "Reverse-mode automatic differentiation and optimization of GPU kernels via enzyme",
      "authors": [
        {
          "name": "William S. Moses",
          "affiliation": ""
        },
        {
          "name": "Valentin Churavy",
          "affiliation": ""
        },
        {
          "name": "Ludger Paehler",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Jan Hückelheim",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Sri Hari Krishna Narayanan",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Michel Schanen",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Computing derivatives is key to many algorithms in scientific computing and machine learning such as optimization, uncertainty quantification, and stability analysis. Enzyme is a LLVM compiler plugin that performs reverse-mode automatic differentiation (AD) and thus generates high performance gradients of programs in languages including C/C++, Fortran, Julia, and Rust. Prior to this work, Enzyme and other AD tools were not capable of generating gradients of GPU kernels. Our paper presents a combination of novel techniques that make Enzyme the first fully automatic reversemode AD tool to generate gradients of GPU kernels. Since unlike other tools Enzyme performs automatic differentiation within a general-purpose compiler, we are able to introduce several novel GPU and AD-specific optimizations. To show the generality and efficiency of our approach, we compute gradients of five GPU-based HPC applications, executed on NVIDIA and AMD GPUs. All benchmarks run within an order of magnitude of the original program's execution time. Without GPU and AD-specific optimizations, gradients of GPU kernels either fail to run from a lack of resources or have infeasible overhead. Finally, we demonstrate that increasing the problem size by either increasing the number of threads or increasing the work per thread, does not substantially impact the overhead from differentiation.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3458817.3476165",
      "sourceUrl": "https://doi.org/10.1145/3458817.3476165",
      "tags": [
        "C++",
        "GPU",
        "Optimizations",
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Johannes Doerfert",
        "Ludger Paehler",
        "Valentin Churavy",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w4200435136",
      "source": "openalex-discovery",
      "title": "PyOMP: Multithreaded Parallel Programming in Python",
      "authors": [
        {
          "name": "Timothy G. Mattson",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Todd A. Anderson",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Giorgis Georgakoudis",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2021",
      "venue": "Computing in Science & Engineering | Vol. 23 (Issue 6)",
      "type": "research-paper",
      "abstract": "We know that Python is a widely used language in scientific computing. When the goal is high performance, however, Python lags far behind low-level languages such as C and Fortran. To support applications that stress performance, Python needs to access the full capabilities of modern CPUs. That means support for parallel multithreading. In this paper, we describe PyOMP, a system that enables OpenMP in Python. Programmers write code in Python with OpenMP, Numba generates code that compiles to LLVM, and the resulting programs run with performance that approaches that from code written with C and OpenMP. In this paper we provide an update on the PyOMP project and explain how to install it and use it to write parallel multithreaded code in Python.",
      "paperUrl": "https://www.osti.gov/biblio/1843562",
      "sourceUrl": "https://doi.org/10.1109/mcse.2021.3128806",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Giorgis Georgakoudis"
      ]
    },
    {
      "id": "openalex-w3121932930",
      "source": "openalex-discovery",
      "title": "Progressive Raising in Multi-level IR",
      "authors": [
        {
          "name": "Lorenzo Chelini",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Andi Drebes",
          "affiliation": ""
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        },
        {
          "name": "Nicolas Vasilache",
          "affiliation": "Google (Switzerland)"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Henk Corporaal",
          "affiliation": "Eindhoven University of Technology"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Multi-level intermediate representations (IR) show great promise for lowering the design costs for domain-specific compilers by providing a reusable, extensible, and non-opini-onated framework for expressing domain-specific and high-level abstractions directly in the IR. But, while such frameworks support the progressive lowering of high-level representations to low-level IR, they do not raise in the opposite direction. Thus, the entry point into the compilation pipeline defines the highest level of abstraction for all subsequent transformations, limiting the set of applicable optimizations, in particular for general-purpose languages that are not semantically rich enough to model the required abstractions. We propose Progressive Raising, a complementary approach to the progressive lowering in multi-level IRs that raises from lower to higher-level abstractions to leverage domain-specific transformations for low-level representations. We further introduce Multilevel Tactics, our declarative approach for progressive raising, implemented on top of the MLIR framework, and demonstrate the progressive raising from affine loop nests specified in a general-purpose language to high-level linear algebra operations. Our raising paths leverage subsequent high-level domain-specific transformations with significant performance improvements.",
      "paperUrl": "https://doi.org/10.1109/cgo51591.2021.9370332",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Lorenzo Chelini",
        "Oleksandr Zinenko",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4226373143",
      "source": "openalex-discovery",
      "title": "Profile Guided Optimization without Profiles: A Machine Learning Approach",
      "authors": [
        {
          "name": "Nadav Rotem",
          "affiliation": ""
        },
        {
          "name": "Chris Cummins",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Profile guided optimization is an effective technique for improving the optimization ability of compilers based on dynamic behavior, but collecting profile data is expensive, cumbersome, and requires regular updating to remain fresh. We present a novel statistical approach to inferring branch probabilities that improves the performance of programs that are compiled without profile guided optimizations. We perform offline training using information that is collected from a large corpus of binaries that have branch probabilities information. The learned model is used by the compiler to predict the branch probabilities of regular uninstrumented programs, which the compiler can then use to inform optimization decisions. We integrate our technique directly in LLVM, supplementing the existing human-engineered compiler heuristics. We evaluate our technique on a suite of benchmarks, demonstrating some gains over compiling without profile information. In deployment, our technique requires no profiling runs and has negligible effect on compilation time.",
      "paperUrl": "https://arxiv.org/pdf/2112.14679",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2112.14679",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Nadav Rotem"
      ]
    },
    {
      "id": "openalex-w3205717712",
      "source": "openalex-discovery",
      "title": "Polygeist: Raising C to Polyhedral MLIR",
      "authors": [
        {
          "name": "William S. Moses",
          "affiliation": ""
        },
        {
          "name": "Lorenzo Chelini",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Rongxuan Zhao",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": "Google (United States)"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present Polygeist, a new compilation flow that connects the MLIR compiler infrastructure to cutting edge polyhedral optimization tools. It consists of a C and C++ frontend capable of converting a broad range of existing codes into MLIR suitable for polyhedral transformation and a bi-directional conversion between MLIR and OpenScop exchange format. The Polygeist/MLIR intermediate representation featuring high-level (affine) loop constructs and n-D arrays embedded into a single static assignment (SSA) substrate enables an unprecedented combination of SSA-based and polyhedral optimizations. We illustrate this by proposing and implementing two extra transformations: statement splitting and reduction parallelization. Our evaluation demonstrates that Polygeist outperforms on average both an LLVM IR-level optimizer (Polly) and a source-to-source state-of-the-art polyhedral compiler (Pluto) when exercised on the Polybench/C benchmark suite in sequential (2.53x vs 1.41x, 2.34x) and parallel mode (9.47x vs 3.26x, 7.54x) thanks to the new representation and transformations.",
      "paperUrl": "https://doi.org/10.1109/pact52795.2021.00011",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Embedded",
        "Frontend",
        "Infrastructure",
        "IR",
        "MLIR",
        "Optimizations",
        "Polly"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini",
        "Oleksandr Zinenko",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w3207198460",
      "source": "openalex-discovery",
      "title": "PolyGym: Polyhedral Optimizations as an Environment for Reinforcement Learning",
      "authors": [
        {
          "name": "Alexander Brauckmann",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Andrés Goens",
          "affiliation": "Barkhausen Institute"
        },
        {
          "name": "Jerónimo Castrillón",
          "affiliation": "TU Dresden"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing search space formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found the search space to contain transformations that lead to a speedup of 3.39x over LLVM O3, which is 1.34x better than the best transformations found in the search space of isl, and 1.83x better than the speedup achieved by the default heuristics of isl. Our generic MDP formulation enables future work to use reinforcement learning to learn optimization heuristics over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.",
      "paperUrl": "https://doi.org/10.1109/pact52795.2021.00009",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexander Brauckmann"
      ]
    },
    {
      "id": "openalex-w3206908800",
      "source": "openalex-discovery",
      "title": "Pointer-Based Divergence Analysis for OpenCL 2.0 Programs",
      "authors": [
        {
          "name": "Shao-Chung Wang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Lin-Ya Yu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Li-An Her",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Shin Hwang",
          "affiliation": "National Taiwan University of Science and Technology"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2021",
      "venue": "ACM Transactions on Parallel Computing | Vol. 8 (Issue 4)",
      "type": "research-paper",
      "abstract": "A modern GPU is designed with many large thread groups to achieve a high throughput and performance. Within these groups, the threads are grouped into fixed-size SIMD batches in which the same instruction is applied to vectors of data in a lockstep. This GPU architecture is suitable for applications with a high degree of data parallelism, but its performance degrades seriously when divergence occurs. Many optimizations for divergence have been proposed, and they vary with the divergence information about variables and branches. A previous analysis scheme viewed pointers and return values from functions as divergence directly, and only focused on OpenCL 1.x. In this article, we present a novel scheme that reports the divergence information for pointer-intensive OpenCL programs. The approach is based on extended static single assignment (SSA) and adds some special functions and annotations from memory SSA and gated SSA. The proposed scheme first constructs extended SSA, which is then used to build a divergence relation graph that includes all of the possible points-to relationships of the pointers and initialized divergence states. The divergence state of the pointers can be determined by propagating the divergence state of the divergence relation graph. The scheme is further extended for interprocedural cases by considering function-related statements. The proposed scheme was implemented in an LLVM compiler and can be applied to OpenCL programs. We analyzed 10 programs with 24 kernels, with a total analyzed program size of 1,306 instructions in an LLVM intermediate representation, with 885 variables, 108 branches, and 313 pointer-related statements. The total number of divergent pointers detected was 146 for the proposed scheme, 200 for the scheme in which the pointer was always divergent, and 155 for the current LLVM default scheme; the total numbers of divergent variables detected were 458, 519, and 482, respectively, with 31, 34, and 32 divergent branches. These experimental results indicate that the proposed scheme is more precise than both a scheme in which a pointer is always divergent and the current LLVM default scheme.",
      "paperUrl": "https://doi.org/10.1145/3470644",
      "sourceUrl": "",
      "tags": [
        "GPU",
        "OpenCL",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee",
        "Li-An Her",
        "Lin-Ya Yu"
      ]
    },
    {
      "id": "openalex-w3185662757",
      "source": "openalex-discovery",
      "title": "PICO",
      "authors": [
        {
          "name": "Tina Jung",
          "affiliation": "Saarland University"
        },
        {
          "name": "Fabian Ritter",
          "affiliation": "Saarland University"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2021",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 18 (Issue 4)",
      "type": "research-paper",
      "abstract": "Memory safety violations such as buffer overflows are a threat to security to this day. A common solution to ensure memory safety for C is code instrumentation. However, this often causes high execution-time overhead and is therefore rarely used in production. Static analyses can reduce this overhead by proving some memory accesses in bounds at compile time. In practice, however, static analyses may fail to verify in-bounds accesses due to over-approximation. Therefore, it is important to additionally optimize the checks that reside in the program. In this article, we present PICO, an approach to eliminate and replace in-bounds checks. PICO exactly captures the spatial memory safety of accesses using Presburger formulas to either verify them statically or substitute existing checks with more efficient ones. Thereby, PICO can generate checks of which each covers multiple accesses and place them at infrequently executed locations. We evaluate our LLVM-based PICO prototype with the well-known SoftBound instrumentation on SPEC benchmarks commonly used in related work. PICO reduces the execution-time overhead introduced by SoftBound by 36% on average (and the code-size overhead by 24%). Our evaluation shows that the impact of substituting checks dominates that of removing provably redundant checks.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3460434",
      "sourceUrl": "https://doi.org/10.1145/3460434",
      "tags": [
        "Security"
      ],
      "matchedAuthors": [
        "Sebastian Hack"
      ]
    },
    {
      "id": "openalex-w4392268042",
      "source": "openalex-discovery",
      "title": "Optimizing Property-Preserving Compilation",
      "authors": [
        {
          "name": "Son Tuan Vu",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "theses.fr (ABES) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In order to ensure security guarantees of binary applications, program analyses and verifications have to be performed at the binary level. These analyses and verifications require various security or functional properties about the program being analyzed. It is thus necessary to propagate these properties,usually expressed in the source level, down to binary code. However, preserving these properties throughout the optimizing compilation flow is hard due to code optimizations which reorder computations or eliminate unused variables. This thesis presents different approaches to preserve and propagate program properties throughout the optimizing compilation flow with minimal changes to individual transformation passes. In the implementations in LLVM, properties are emitted into executable binaries as DWARF debug information, which can next beused by binary analysis tools. Our mechanisms can be applied to address the problem of preserving security protections inserted at the source level, compiled with optimizations enabled.",
      "paperUrl": "http://www.theses.fr/2021SORUS435/document",
      "sourceUrl": "",
      "tags": [
        "Debug Information",
        "Optimizations",
        "Security"
      ],
      "matchedAuthors": [
        "Son Tuan Vu"
      ]
    },
    {
      "id": "openalex-w3206992429",
      "source": "openalex-discovery",
      "title": "Optimizing Barrier Synchronization on ARMv8 Many-Core Architectures",
      "authors": [
        {
          "name": "Wanrong Gao",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Jianbin Fang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Chun Huang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Chuanfu Xu",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Zheng Wang",
          "affiliation": "University of Leeds"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Synchronization operations are commonly seen in OpenMP programs where a parallel construct often works with an explicit or implicit barrier operation. While OpenMP synchronization has been extensively studied on the traditional x86 CPU architectures, there is little work on understanding OpenMP barrier synchronization operations on ARMv8 high-performance many-cores. This paper presents the first comprehensive performance study on OpenMP barrier implementations on emerging ARMvS-based many-cores. We evaluate seven representative barrier algorithms on three distinct ARMv8 architectures: Phytium 2000+, ThunderX2, and Kunpeng920. We empirically show that the existing synchronization implementations exhibit poor scalability on ARMv8 architectures compared to the x86 counterpart. We then propose various optimization strategies for improving these widely used synchronization algorithms on each platform. We showcase that our optimizations yield 12.6x performance improvement over the GCC implementation and 4.7x improvement over the LLVM implementation, translating to 1.6x improvement over the state-of-the-art best-performing algorithm. We share our experience and practical insights on optimizing OpenMP synchronization operations on emerging ARMv8 multi-core CPU architectures.",
      "paperUrl": "https://doi.org/10.1109/cluster48925.2021.00044",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Jianbin Fang",
        "Zheng Wang"
      ]
    },
    {
      "id": "openalex-w3198818607",
      "source": "openalex-discovery",
      "title": "OpenSHMEM Checker - A Clang Based Static Checker for OpenSHMEM",
      "authors": [
        {
          "name": "Md Abdullah Shahneous Bari",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Ujjwal Arora",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Varun Hegde",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Tony Curtis",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Stony Brook University"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compilers are generally not aware of the semantics of library-based parallel programming models such as MPI and OpenSHMEM, and hence are unable to detect programming errors related to their use. To alleviate this issue, we developed a custom static checker for OpenSHMEM programs based on LLVM's Clang Static Analyzer framework (CSA). We leverage the Symbolic Execution engine of the core Static Analyzer framework and its path-sensitive analysis to check for bugs on all OpenSHMEM program paths. We have identified common programming mistakes in OpenSHMEM programs that are detectable at compile-time and provided checks for them in the analyzer. They cover: utilization of the right type of memory (private vs. symmetric memory); safe/synchronized access to program data in the presence of asynchronous, one-sided communication; and double-free of memories allocated using OpenSHMEM memory allocation routines. Our experimental analysis showed that the static checker successfully detects bugs in OpenSHMEM code.",
      "paperUrl": "https://doi.org/10.1109/ispdc52870.2021.9521645",
      "sourceUrl": "",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Barbara Chapman"
      ]
    },
    {
      "id": "openalex-w4286859125",
      "source": "openalex-discovery",
      "title": "OpenMP aware MHP Analysis for Improved Static Data-Race Detection",
      "authors": [
        {
          "name": "Utpal Bora",
          "affiliation": ""
        },
        {
          "name": "Shraiysh Vaishay",
          "affiliation": ""
        },
        {
          "name": "Saurabh Joshi",
          "affiliation": ""
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Data races, a major source of bugs in concurrent programs, can result in loss of manpower and time as well as data loss due to system failures. OpenMP, the de facto shared memory parallelism framework used in the HPC community, also suffers from data races. To detect race conditions in OpenMP programs and improve turnaround time and/or developer productivity, we present a data flow analysis based, fast, static data race checker in the LLVM compiler framework. Our tool can detect races in the presence or absence of explicit barriers, with implicit or explicit synchronization. In addition, our tool effectively works for the OpenMP target offloading constructs and also supports the frequently used OpenMP constructs. We formalize and provide a data flow analysis framework to perform Phase Interval Analysis (PIA) of OpenMP programs. Phase intervals are then used to compute the MHP (and its complement NHP) sets for the programs, which, in turn, are used to detect data races statically. We evaluate our work using multiple OpenMP race detection benchmarks and real world applications. Our experiments show that the checker is comparable to the state-of-the-art in various performance metrics with around 90% accuracy, almost perfect recall, and significantly lower runtime and memory footprint.",
      "paperUrl": "https://arxiv.org/pdf/2111.04259",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2111.04259",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Ramakrishna Upadrasta",
        "Saurabh Joshi",
        "Utpal Bora"
      ]
    },
    {
      "id": "openalex-w3025058513",
      "source": "openalex-discovery",
      "title": "Not so fast: understanding and mitigating negative impacts of compiler optimizations on code reuse gadget sets",
      "authors": [
        {
          "name": "Michael D. Brown",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Matthew Pruett",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Robert Bigelow",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Girish Mururu",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Santosh Pande",
          "affiliation": "Georgia Institute of Technology"
        }
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 5 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Despite extensive testing and correctness certification of their functional semantics, a number of compiler optimizations have been shown to violate security guarantees implemented in source code. While prior work has shed light on how such optimizations may introduce semantic security weaknesses into programs, there remains a significant knowledge gap concerning the impacts of compiler optimizations on non-semantic properties with security implications. In particular, little is currently known about how code generation and optimization decisions made by the compiler affect the availability and utility of reusable code segments called gadgets required for implementing code reuse attack methods such as return-oriented programming. In this paper, we bridge this gap through a study of the impacts of compiler optimization on code reuse gadget sets. We analyze and compare 1,187 variants of 20 different benchmark programs built with two production compilers (GCC and Clang) to determine how their optimization behaviors affect the code reuse gadget sets present in program variants with respect to both quantitative and qualitative metrics. Our study exposes an important and unexpected problem; compiler optimizations introduce new gadgets at a high rate and produce code containing gadget sets that are generally more useful to an attacker than those in unoptimized code. Using differential binary analysis, we identify several undesirable behaviors at the root of this phenomenon. In turn, we propose and evaluate several strategies to mitigate these behaviors. In particular, we show that post-production binary recompilation can effectively mitigate these behaviors with negligible performance impacts, resulting in optimized code with significantly smaller and less useful gadget sets.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3485531",
      "sourceUrl": "https://doi.org/10.1145/3485531",
      "tags": [
        "Clang",
        "Optimizations",
        "Performance",
        "Security",
        "Testing"
      ],
      "matchedAuthors": [
        "Girish Mururu",
        "Santosh Pande"
      ]
    },
    {
      "id": "openalex-w3196027996",
      "source": "openalex-discovery",
      "title": "Modular, compositional, and executable formal semantics for LLVM IR",
      "authors": [
        {
          "name": "Yannick Zakowski",
          "affiliation": "Institut national de recherche en informatique et en automatique"
        },
        {
          "name": "Calvin Beck",
          "affiliation": "California University of Pennsylvania"
        },
        {
          "name": "Irene Yoon",
          "affiliation": "California University of Pennsylvania"
        },
        {
          "name": "Ilia Zaichuk",
          "affiliation": "Taras Shevchenko National University of Kyiv"
        },
        {
          "name": "Vadim Zaliva",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Steve Zdancewic",
          "affiliation": "California University of Pennsylvania"
        }
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 5 (Issue ICFP)",
      "type": "research-paper",
      "abstract": "This paper presents a novel formal semantics, mechanized in Coq, for a large, sequential subset of the LLVM IR. In contrast to previous approaches, which use relationally-specified operational semantics, this new semantics is based on monadic interpretation of interaction trees, a structure that provides a more compositional approach to defining language semantics while retaining the ability to extract an executable interpreter. Our semantics handles many of the LLVM IR's non-trivial language features and is constructed modularly in terms of event handlers, including those that deal with nondeterminism in the specification. We show how this semantics admits compositional reasoning principles derived from the interaction trees equational theory of weak bisimulation, which we extend here to better deal with nondeterminism, and we use them to prove that the extracted reference interpreter faithfully refines the semantic model. We validate the correctness of the semantics by evaluating it on unit tests and LLVM IR programs generated by HELIX.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3473572",
      "sourceUrl": "https://doi.org/10.1145/3473572",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Steve Zdancewic"
      ]
    },
    {
      "id": "openalex-w3177871936",
      "source": "openalex-discovery",
      "title": "Model checking C++ programs",
      "authors": [
        {
          "name": "Felipe R. Monteiro",
          "affiliation": "Universidade Federal do Amazonas"
        },
        {
          "name": "Mikhail R. Gadelha",
          "affiliation": "Universidade da Coruña"
        },
        {
          "name": "Lucas C. Cordeiro",
          "affiliation": "University of Manchester"
        }
      ],
      "year": "2021",
      "venue": "Software Testing Verification and Reliability | Vol. 32 (Issue 1)",
      "type": "research-paper",
      "abstract": "Summary In the last three decades, memory safety issues in system programming languages such as C or C++ have been one of the most significant sources of security vulnerabilities. However, there exist only a few attempts with limited success to cope with the complexity of C++ program verification. We describe and evaluate a novel verification approach based on bounded model checking (BMC) and satisfiability modulo theories (SMT) to verify C++ programs. Our verification approach analyses bounded C++ programs by encoding into SMT various sophisticated features that the C++ programming language offers, such as templates, inheritance, polymorphism, exception handling, and the Standard Template Libraries. We formalize these features within our formal verification framework using a decidable fragment of first‐order logic and then show how state‐of‐the‐art SMT solvers can efficiently handle that. We implemented our verification approach on top of ESBMC. We compare ESBMC to LLBMC and DIVINE, which are state‐of‐the‐art verifiers to check C++ programs directly from the LLVM bitcode. Experimental results show that ESBMC can handle a wide range of C++ programs, presenting a higher number of correct verification results. Additionally, ESBMC has been applied to a commercial C++ application in the telecommunication domain and successfully detected arithmetic‐overflow errors, which could potentially lead to security vulnerabilities.",
      "paperUrl": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/stvr.1793",
      "sourceUrl": "https://doi.org/10.1002/stvr.1793",
      "tags": [
        "C++",
        "Libraries",
        "Programming Languages",
        "Security"
      ],
      "matchedAuthors": [
        "Mikhail R. Gadelha"
      ]
    },
    {
      "id": "openalex-w3122286897",
      "source": "openalex-discovery",
      "title": "MLIR: Scaling Compiler Infrastructure for Domain Specific Computation",
      "authors": [
        {
          "name": "Chris Lattner",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Mehdi Amini",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Uday Bondhugula",
          "affiliation": "Indian Institute of Science Bangalore"
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        },
        {
          "name": "Andy Davis",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Jacques A. Pienaar",
          "affiliation": "Google (United States)"
        },
        {
          "name": "River Riddle",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Tatiana Shpeisman",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Nicolas Vasilache",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR addresses software fragmentation, compilation for heterogeneous hardware, significantly reducing the cost of building domain specific compilers, and connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, while identifying the challenges and opportunities posed by this novel design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.",
      "paperUrl": "https://doi.org/10.1109/cgo51591.2021.9370308",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Chris Lattner",
        "Mehdi Amini",
        "Oleksandr Zinenko",
        "River Riddle",
        "Tatiana Shpeisman"
      ]
    },
    {
      "id": "openalex-w3120378074",
      "source": "openalex-discovery",
      "title": "MLGO: a Machine Learning Guided Compiler Optimizations Framework",
      "authors": [
        {
          "name": "Mircea Trofin",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Yundi Qian",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Eugene Brevdo",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Zinan Lin",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Krzysztof Choromański",
          "affiliation": "Google (United States)"
        },
        {
          "name": "David Li",
          "affiliation": "Google (United States)"
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Leveraging machine-learning (ML) techniques for compiler optimizations has been widely studied and explored in academia. However, the adoption of ML in general-purpose, industry strength compilers has yet to happen. We propose MLGO, a framework for integrating ML techniques systematically in an industrial compiler -- LLVM. As a case study, we present the details and results of replacing the heuristics-based inlining-for-size optimization in LLVM with machine learned models. To the best of our knowledge, this work is the first full integration of ML in a complex compiler pass in a real-world setting. It is available in the main LLVM repository. We use two different ML algorithms: Policy Gradient and Evolution Strategies, to train the inlining-for-size model, and achieve up to 7\\% size reduction, when compared to state of the art LLVM -Oz. The same model, trained on one corpus, generalizes well to a diversity of real-world targets, as well as to the same set of targets after months of active development. This property of the trained models is beneficial to deploy ML techniques in real-world settings.",
      "paperUrl": "https://arxiv.org/pdf/2101.04808",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2101.04808",
      "tags": [
        "ML",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Eugene Brevdo",
        "Mircea Trofin"
      ]
    },
    {
      "id": "openalex-w3186925585",
      "source": "openalex-discovery",
      "title": "Loop Transformations using Clang’s Abstract Syntax Tree",
      "authors": [
        {
          "name": "Michael Kruse",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "OpenMP 5.1 introduced the first loop nest transformation directives unroll and tile, and more are expected to be included in OpenMP 6.0. We discuss the two Abstract Syntax Tree (AST) representations used by Clang's implementation that is currently under development. The first representation is designed for compatibility with the existing implementation and stores the transformed loop nest in a shadow AST next to the syntactical AST. The second representation introduces a new meta AST-node OMPCanonicalLoop that guarantees that the semantic requirements of an OpenMP loop are met, and a CanonicalLoopInfo type that the OpenMPIRBuilder uses to represent literal and transformed loops. This second approach provides a better abstraction of loop semantics, removes the need for shadow AST nodes that are only relevant for code generation, allows sharing the implementation with other front-ends such as flang, but depends on the OpenMPIRBuilder which is currently under development.",
      "paperUrl": "https://arxiv.org/pdf/2107.08132",
      "sourceUrl": "https://doi.org/10.1145/3458744.3473359",
      "tags": [
        "Clang",
        "Flang",
        "Loop transformations"
      ],
      "matchedAuthors": [
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w4240301832",
      "source": "openalex-discovery",
      "title": "Listings",
      "authors": [
        {
          "name": "Michael Klemm",
          "affiliation": ""
        },
        {
          "name": "Jim Cownie",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Listing 1.1 Simple array example using Intel Threading Building Blocks.| 5 Listing 1.2 Simple array example using Fortran array syntax.| 7 Listing 2.1 Classic \"Hello World\" example using the POSIX thread API.| 14 Listing 2.2 Classic \"Hello World\" example with C++ threads.| 15 Listing 2.3 Classic \"Hello World\" example using the OpenMP API.| 18 Listing 2.4 Worksharing loop in the OpenMP API.| 20 Listing 2.5 The OpenMP master, masked, single, and sections constructs.| 21 Listing 2.6 Using OpenMP tasks to print \"Hello World\".| 24 Listing 2.7 Classic \"Hello World\" example using Intel Threading Building Blocks.| 25 Listing 2.8 OpenMP taskloop Matrix Multiplication.| 26 Listing 2.9 Mutual exclusion features in the OpenMP API.| 29 Listing 2.10 Implicit and explicit barriers in the OpenMP API.| 32 Listing 2.11 Calculation of Fibonacci numbers to illustrate waiting for child tasks.| 34 Listing 2.12 Traversal of a linked list with task barrier.| 35 Listing 2.13 OpenMP code with task dependences.| 36 Listing 2.14 Matrix-matrix multiplication using task dependences.| 38 Listing 3.1 Broken point-to-point channel.| 74 Listing 3.2 Broken point to point channel, x86_64 assembly code.| 74 Listing 3.3 Correct point-to-point channel.| 75 Listing 3.4 Fixed point-to-point channel x86_64 assembly code.| 76 Listing 4.1 TBB example code before outlining for parallel execution.| 96 Listing 4.2 TBB example code after outlining for parallel execution.| 97 Listing 4.3 TBB code fragments to store a task in the task pool.| 98 Listing 4.4 Source code before outlining for parallel execution.| 104 Listing 4.5 Code of Listing 4.4 after outlining the code of the parallel region.| 104 Listing 4.6 Code of Listing 4.4 after outlining a nested thunk function.| 105 Listing 4.7 Element-wise array summation with a worksharing construct.| 106 Listing 4.8 Compiler-generated loop-scheduling code for Listing 4.7.| 107 Listing 4.9 Element-wise array summation using a task-loop construct.| 108 Listing 4.10 Code for element-wise array summation targeting a SIMD machine.| 110 Listing 4.11 Assembly code of the array code of Listing 4.10.| 111 Listing 4.12 Sorted assembly code of the unrolled code of Listing 4.11.| 112 Listing 4.13 Jammed SIMD code of the unrolled code of Listing 4.12.| 113 Listing 4.14 Implementing the master construct.| 114 Listing 4.15 Implementing the single construct.| 114 Listing 4.16 OpenMP static tasks and corresponding code generation pattern.| 116 Listing 4.17 Transforming a task construct into a lambda expression.| 117 Listing 4.18 Transforming a task construct into low-level runtime entry points.| 118 Listing 4.19 GCC high-level intermediate code for Listing 4.4.| 121 Listing 4.20 GCC low-level intermediate code for Listing 4.4.| 122 Listing 4.21 The clang compiler's simplified AST for the code in Listing 4.4.",
      "paperUrl": "https://www.degruyter.com/document/doi/10.1515/9783110632729-204/pdf",
      "sourceUrl": "https://doi.org/10.1515/9783110632729-204",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Michael Klemm"
      ]
    },
    {
      "id": "openalex-w3154599330",
      "source": "openalex-discovery",
      "title": "Language-parametric compiler validation with application to LLVM",
      "authors": [
        {
          "name": "Theodoros Kasampalis",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Daejun Park",
          "affiliation": "Runtime Verification (United States)"
        },
        {
          "name": "Zhengyao Lin",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Vikram Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Grigore Roşu",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We propose a new design for a Translation Validation (TV) system geared towards practical use with modern optimizing compilers, such as LLVM. Unlike existing TV systems, which are custom-tailored for a particular sequence of transformations and a specific, common language for input and output programs, our design clearly separates the transformation-specific components from the rest of the system, and generalizes the transformation-independent components. Specifically, we present Keq, the first program equivalence checker that is parametric to the input and output language semantics and has no dependence on the transformation between the input and output programs. The Keq algorithm is based on a rigorous formalization, namely cut-bisimulation, and is proven correct. We have prototyped a TV system for the Instruction Selection pass of LLVM, being able to automatically prove equivalence for translations from LLVM IR to the MachineIR used in compiling to x86-64. This transformation uses different input and output languages, and as such has not been previously addressed by the state of the art. An experimental evaluation shows that Keq successfully proves correct the translation of over 90% of 4732 supported functions in GCC from SPEC 2006.",
      "paperUrl": "https://doi.org/10.1145/3445814.3446751",
      "sourceUrl": "",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Vikram Adve"
      ]
    },
    {
      "id": "openalex-w3197331411",
      "source": "openalex-discovery",
      "title": "LaForge: Always-Correct and Fast Incremental Builds from Simple Specifications",
      "authors": [
        {
          "name": "Charlie Curtsinger",
          "affiliation": "Grinnell College"
        },
        {
          "name": "Daniel W. Barowy",
          "affiliation": "Williams College"
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Developers rely on build systems to generate software from code. At a minimum, a build system should produce build targets from a clean copy of the code. However, developers rarely work from clean checkouts. Instead, they rebuild software repeatedly, sometimes hundreds of times a day. To keep rebuilds fast, build systems run incrementally, executing commands only when built state cannot be reused. Existing tools like make present users with a tradeoff. Simple build specifications are easy to write, but limit incremental work. More complex build specifications produce faster incremental builds, but writing them is labor-intensive and error-prone. This work shows that no such tradeoff is necessary; build specifications can be both simple and fast. We introduce LaForge, a novel build tool that eliminates the need to specify dependencies or incremental build steps. LaForge builds are easy to specify; developers write a simple script that runs a full build. Even a single command like gcc src/*.c will suffice. LaForge traces the execution of the build and generates a transcript in the TraceIR language. On later builds, LaForge evaluates the TraceIR transcript to detect changes and perform an efficient incremental rebuild that automatically captures all build dependencies. We evaluate LaForge by building 14 software packages, including LLVM and memcached. Our results show that LaForge automatically generates efficient builds from simple build specifications. Full builds with LaForge have a median overhead of 16.1% compared to a project's default full build. LaForge's incremental builds consistently run fewer commands, and most take less than 3.08s longer than manually-specified incremental builds. Finally, LaForge is always correct.",
      "paperUrl": "https://arxiv.org/pdf/2108.12469",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2108.12469",
      "tags": [],
      "matchedAuthors": [
        "Charlie Curtsinger"
      ]
    },
    {
      "id": "openalex-w3158108678",
      "source": "openalex-discovery",
      "title": "Isolation Without Taxation: Near Zero Cost Transitions for SFI",
      "authors": [
        {
          "name": "Matthew Kolosick",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Shravan Narayan",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Conrad Watt",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Michael LeMay",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Deepak Garg",
          "affiliation": "Max Planck Institute for Software Systems"
        },
        {
          "name": "Ranjit Jhala",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Deian Stefan",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Stefan, Deian",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Software sandboxing or software-based fault isolation (SFI) is a lightweight approach to building secure systems out of untrusted components. Mozilla, for example, uses SFI to harden the Firefox browser by sandboxing third-party libraries, and companies like Fastly and Cloudflare use SFI to safely co-locate untrusted tenants on their edge clouds. While there have been significant efforts to optimize and verify SFI enforcement, context switching in SFI systems remains largely unexplored: almost all SFI systems use \\emph{heavyweight transitions} that are not only error-prone but incur significant performance overhead from saving, clearing, and restoring registers when context switching. We identify a set of \\emph{zero-cost conditions} that characterize when sandboxed code has sufficient structured to guarantee security via lightweight \\emph{zero-cost} transitions (simple function calls). We modify the Lucet Wasm compiler and its runtime to use zero-cost transitions, eliminating the undue performance tax on systems that rely on Lucet for sandboxing (e.g., we speed up image and font rendering in Firefox by up to 29.7\\% and 10\\% respectively). To remove the Lucet compiler and its correct implementation of the Wasm specification from the trusted computing base, we (1) develop a \\emph{static binary verifier}, VeriZero, which (in seconds) checks that binaries produced by Lucet satisfy our zero-cost conditions, and (2) prove the soundness of VeriZero by developing a logical relation that captures when a compiled Wasm function is semantically well-behaved with respect to our zero-cost conditions. Finally, we show that our model is useful beyond Wasm by describing a new, purpose-built SFI system, SegmentZero32, that uses x86 segmentation and LLVM with mostly off-the-shelf passes to enforce our zero-cost conditions; our prototype performs on-par with the state-of-the-art Native Client SFI system.",
      "paperUrl": "https://arxiv.org/pdf/2105.00033",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2105.00033",
      "tags": [
        "Libraries",
        "Performance",
        "Rust",
        "Security"
      ],
      "matchedAuthors": [
        "Deian Stefan",
        "Ranjit Jhala"
      ]
    },
    {
      "id": "openalex-w3203966350",
      "source": "openalex-discovery",
      "title": "Inlining for Code Size Reduction",
      "authors": [
        {
          "name": "Thaís Regina Damásio",
          "affiliation": "Pontifícia Universidade Católica de Minas Gerais"
        },
        {
          "name": "Vinícius Pacheco",
          "affiliation": "Pontifícia Universidade Católica de Minas Gerais"
        },
        {
          "name": "Luís F. W. Góes",
          "affiliation": "University of Leicester"
        },
        {
          "name": "Fernando Magno Quintão Pereira",
          "affiliation": "Hospital das Clínicas da Universidade Federal de Minas Gerais"
        },
        {
          "name": "Rodrigo C. O. Rocha",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Function inlining is a compiler optimization that replaces the call of a function with its body. Inlining is typically seen as an optimization that improves performance at the expenses of increasing code size. This paper goes against this intuition, and shows that inlining can be employed, in specific situations, as a way to reduce code size. Towards this end, we bring forward two results. First, we gauge the benefits of a trivial heuristic for code-size reduction: the inlining of functions that are invoked at only one call site in the program, followed by the elimination of the original callee. Second, we present and evaluate an analysis that identifies call sites where inlining enables context-sensitive optimizations that reduce code. We have implemented all these techniques in the LLVM compilation infrastructure. When applied onto MiBench, our inlining heuristics yield an average code size reduction of 2.96%, reaching 11% in the best case, over clang -Os. Moreover, our techniques preserve the performance gains of LLVM's standard inlining decisions on MiBench: there is no statistically significant difference in the running time of code produced by these different approaches.",
      "paperUrl": "https://doi.org/10.1145/3475061.3475081",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Infrastructure",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Luís F. W. Góes",
        "Rodrigo C. O. Rocha"
      ]
    },
    {
      "id": "openalex-w3174413148",
      "source": "openalex-discovery",
      "title": "HyFM: function merging for free",
      "authors": [
        {
          "name": "Rodrigo C. O. Rocha",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Pavlos Petoumenos",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Zheng Wang",
          "affiliation": "University of Leeds"
        },
        {
          "name": "Murray Cole",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Kim Hazelwood",
          "affiliation": "Meta (United States)"
        },
        {
          "name": "Hugh Leather",
          "affiliation": "Meta (United States)"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Description of how to download and use the artifact of the LCTES paper, titled \"HyFM: Function Merging for Free\". It also includes the LLVM source code with the SalSSA and HyFM implementation.",
      "paperUrl": "https://research.manchester.ac.uk/en/publications/c8af3c99-0892-472d-aa5d-01f8a1d422ff",
      "sourceUrl": "https://doi.org/10.1145/3461648.3463852",
      "tags": [],
      "matchedAuthors": [
        "Hugh Leather",
        "Kim Hazelwood",
        "Rodrigo C. O. Rocha",
        "Zheng Wang"
      ]
    },
    {
      "id": "openalex-w3186074728",
      "source": "openalex-discovery",
      "title": "GenMC: A Model Checker for Weak Memory Models",
      "authors": [
        {
          "name": "Michalis Kokologiannakis",
          "affiliation": "Max Planck Institute for Software Systems"
        },
        {
          "name": "Viktor Vafeiadis",
          "affiliation": "Max Planck Institute for Software Systems"
        }
      ],
      "year": "2021",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract GenMC is an LLVM-based state-of-the-art stateless model checker for concurrent C/C++ programs. Its modular infrastructure allows it to support complex memory models, such as RC11 and IMM, and makes it easy to extend to support further axiomatic memory models. In this paper, we discuss the overall architecture of the tool and how it can be extended to support additional memory models, programming languages, and/or synchronization primitives. To demonstrate the point, we have extended the tool with support for the Linux kernel memory model (LKMM), synchronization barriers, POSIX I/O system calls, and better error detection capabilities.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-030-81685-8_20.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-030-81685-8_20",
      "tags": [
        "C++",
        "Infrastructure",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Viktor Vafeiadis"
      ]
    },
    {
      "id": "openalex-w4200487671",
      "source": "openalex-discovery",
      "title": "Flacc: Towards OpenACC support for Fortran in the LLVM Ecosystem",
      "authors": [
        {
          "name": "Valentin Clément",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "OpenACC is a directive-based programming model for heterogeneous accelerators initially launched in 2010 to provide a portable solution at a level of abstraction above OpenCL, CUDA, and other lower-level programming models. Various implementations of OpenACC for C, C++, and Fortran exist; however, only one open-source, production implementation of OpenACC for Fortran does exist. Moreover, most contemporary compiler tool chains for heterogeneous computing are based on LLVM. This lack of support poses a serious risk for high-performance computing application developers targeting GPUs and other accelerators, and it limits the ability of the community to experiment with, extend, and contribute to the OpenACC specification and open-source implementation itself. To address this gap, we have designed and begun implementing Flacc: an effort funded by the US Exascale Computing Project to develop production OpenACC compiler support for Fortran based on Flang within the LLVM ecosystem. In this paper, we describe the Flacc goals, initial design and prototype, and challenges that we have encountered so far in our prototyping efforts. Flacc is implemented as a MLIR dialect in the Flang Fortran front end in LLVM. The Flacc front end currently supports OpenACC version 3.1, and the Flacc run time is currently under development and relies on contributions from the Clacc project. Current contributions to Flacc are available in the main ${\\color{Green}{\\mathbf{LLVM}}\\;{\\mathbf{repository}}}$. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>",
      "paperUrl": "https://doi.org/10.1109/llvmhpc54804.2021.00007",
      "sourceUrl": "",
      "tags": [
        "C++",
        "CUDA",
        "Flang",
        "MLIR",
        "OpenCL",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w3181766882",
      "source": "openalex-discovery",
      "title": "Extending C++ for Heterogeneous Quantum-Classical Computing",
      "authors": [
        {
          "name": "Alexander McCaskey",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Thien Nguyen",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Anthony Santana",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Daniel Claudino",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Tyler Kharazi",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Hal Finkel",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2021",
      "venue": "ACM Transactions on Quantum Computing | Vol. 2 (Issue 2)",
      "type": "research-paper",
      "abstract": "We present qcor—a language extension to C++ and compiler implementation that enables heterogeneous quantum-classical programming, compilation, and execution in a single-source context. Our work provides a first-of-its-kind C++ compiler enabling high-level quantum kernel (function) expression in a quantum-language agnostic manner, as well as a hardware-agnostic, retargetable compiler workflow targeting a number of physical and virtual quantum computing backends. qcor leverages novel Clang plugin interfaces and builds upon the XACC system-level quantum programming framework to provide a state-of-the-art integration mechanism for quantum-classical compilation that leverages the best from the community at-large. qcor translates quantum kernels ultimately to the XACC intermediate representation, and provides user-extensible hooks for quantum compilation routines like circuit optimization, analysis, and placement. This work details the overall architecture and compiler workflow for qcor, and provides a number of illuminating programming examples demonstrating its utility for near-term variational tasks, quantum algorithm expression, and feed-forward error correction schemes.",
      "paperUrl": "https://www.osti.gov/biblio/1846549",
      "sourceUrl": "https://doi.org/10.1145/3462670",
      "tags": [
        "Backend",
        "C++",
        "Clang",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Hal Finkel"
      ]
    },
    {
      "id": "openalex-w4249668372",
      "source": "openalex-discovery",
      "title": "Evaluating the Performance of OpenMP Offloading on the NEC SX-Aurora TSUBASA Vector Engine",
      "authors": [
        {
          "name": "Tim Cramer",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Boris Kosmynin",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Simon Moll",
          "affiliation": ""
        },
        {
          "name": "Manoel Römmer",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Erich Focht",
          "affiliation": ""
        },
        {
          "name": "Matthias S. Müller",
          "affiliation": "RWTH Aachen University"
        }
      ],
      "year": "2021",
      "venue": "Supercomputing Frontiers and Innovations | Vol. 8 (Issue 2)",
      "type": "research-paper",
      "abstract": "The NEC SX-Aurora TSUBASA vector engine (VE) follows the tradition of long vector processors for high-performance computing (HPC). The technology combines the vector computing capabilities with the popularity of standard x86 architecture by integrating it as an accelerator. To decrease the burden of code porting for different accelerator types, the OpenMP specification is designed to be single parallel programming model for all of them. Besides the availability of compiler and runtime implementations, the functionality as well as the performance is important for the usability and acceptance of this paradigm. In this work, we present LLVM-based solutions for OpenMP target device offloading from the host to the vector engine and vice versa (reverse offloading). Therefore, we use our source-to-source transformation tool sotoc as well as the native LLVM-VE code path. We assess the functionality and present the first performance numbers of real-world HPC kernels. We discuss the advantages and disadvantage of the different approaches and show that our implementation is competitive to other GPU OpenMP runtime implementations. Our work gives scientific programmers new opportunities and flexibilities for the development of scalable OpenMP offloading applications for SX-Aurora TSUBASA.",
      "paperUrl": "https://superfri.org/index.php/superfri/article/download/385/389",
      "sourceUrl": "https://doi.org/10.14529/jsfi210204",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Erich Focht",
        "Simon Moll"
      ]
    },
    {
      "id": "openalex-w3159746323",
      "source": "openalex-discovery",
      "title": "Enabling the Use of C++20 Unseq Execution Policy for OpenCL",
      "authors": [
        {
          "name": "Po-Yao Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Tai-Liang Chen",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2021",
      "venue": "International Workshop on OpenCL | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This work facilitates the usage of unsequenced execution policy as seen in C++20 standard library with the newly introduced OpenCL kernel language, C++ for OpenCL. By passing unseq, a global object of type unsequenced_policy, as an argument to selected C++ parallel algorithms, the function would then be vectorized with the help of clang and LLVM. This work complements the introduction of C++ for OpenCL, which brings the core language part of C++17 to OpenCL while leaving out the standard library part. In the best case, we see a whopping 6.9 time speedup.",
      "paperUrl": "https://doi.org/10.1145/3456669.3456674",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "OpenCL"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w3154816604",
      "source": "openalex-discovery",
      "title": "Efficient LLVM-based dynamic binary translation",
      "authors": [
        {
          "name": "Alexis Engelke",
          "affiliation": ""
        },
        {
          "name": "Dominik Okwieka",
          "affiliation": ""
        },
        {
          "name": "Martin Schulz",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Emulation of other or newer processor architectures is necessary for a wide variety of use cases, from ensuring compatibility to offering a vehicle for computer architecture research. This problem is usually approached using dynamic binary translation, where machine code is translated, on the fly, to the host architecture during program execution. Existing systems, like QEMU, usually focus on translation performance rather than the overall program execution, and extensions, like HQEMU, are limited by their underlying implementation. Conversely, performance-focused systems are typically used for binary instrumentation. E.g., DynamoRIO reuses original instructions where possible, while Instrew utilizes the LLVM compiler infrastructure, but only supports same-architecture code generation.",
      "paperUrl": "https://doi.org/10.1145/3453933.3454022",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexis Engelke",
        "Martin Schulz"
      ]
    },
    {
      "id": "openalex-w3196320218",
      "source": "openalex-discovery",
      "title": "Domain-Specific Multi-Level IR Rewriting for GPU: The Open Earth Compiler for GPU-accelerated Climate Simulation",
      "authors": [
        {
          "name": "Tobias Gysi",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Christoph Müller",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        },
        {
          "name": "Stephan Herhut",
          "affiliation": ""
        },
        {
          "name": "Eddie C. Davis",
          "affiliation": "Vulcan (United States)"
        },
        {
          "name": "Tobias Wicky",
          "affiliation": "Vulcan (United States)"
        },
        {
          "name": "Oliver Fuhrer",
          "affiliation": "Vulcan (United States)"
        },
        {
          "name": "Torsten Hoefler",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2021",
      "venue": "Repository for Publications and Research Data (ETH Zurich) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Most compilers have a single core intermediate representation (IR) (e.g., LLVM) sometimes complemented with vaguely defined IR-like data structures. This IR is commonly low-level and close to machine instructions. As a result, optimizations relying on domain-specific information are either not possible or require complex analysis to recover the missing information. In contrast, multi-level rewriting instantiates a hierarchy of dialects (IRs), lowers programs level-by-level, and performs code transformations at the most suitable level. We demonstrate the effectiveness of this approach for the weather and climate domain. In particular, we develop a prototype compiler and design stencil- and GPU-specific dialects based on a set of newly introduced design principles. We find that two domain-specific optimizations (500 lines of code) realized on top of LLVM's extensible MLIR compiler infrastructure suffice to outperform state-of-the-art solutions. In essence, multilevel rewriting promises to herald the age of specialized compilers composed from domain- and target-specific dialects implemented on top of a shared infrastructure.",
      "paperUrl": "http://hdl.handle.net/20.500.11850/509880",
      "sourceUrl": "https://doi.org/10.3929/ethz-b-000509880",
      "tags": [
        "GPU",
        "Infrastructure",
        "IR",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Oleksandr Zinenko",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w3031192821",
      "source": "openalex-discovery",
      "title": "Domain-Specific Multi-Level IR Rewriting for GPU",
      "authors": [
        {
          "name": "Tobias Gysi",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Christoph Müller",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        },
        {
          "name": "Stephan Herhut",
          "affiliation": ""
        },
        {
          "name": "Eddie C. Davis",
          "affiliation": "Vulcan (United States)"
        },
        {
          "name": "Tobias Wicky",
          "affiliation": "Vulcan (United States)"
        },
        {
          "name": "Oliver Fuhrer",
          "affiliation": "Vulcan (United States)"
        },
        {
          "name": "Torsten Hoefler",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2021",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 18 (Issue 4)",
      "type": "research-paper",
      "abstract": "Most compilers have a single core intermediate representation (IR) (e.g., LLVM) sometimes complemented with vaguely defined IR-like data structures. This IR is commonly low-level and close to machine instructions. As a result, optimizations relying on domain-specific information are either not possible or require complex analysis to recover the missing information. In contrast, multi-level rewriting instantiates a hierarchy of dialects (IRs), lowers programs level-by-level, and performs code transformations at the most suitable level. We demonstrate the effectiveness of this approach for the weather and climate domain. In particular, we develop a prototype compiler and design stencil- and GPU-specific dialects based on a set of newly introduced design principles. We find that two domain-specific optimizations (500 lines of code) realized on top of LLVM’s extensible MLIR compiler infrastructure suffice to outperform state-of-the-art solutions. In essence, multi-level rewriting promises to herald the age of specialized compilers composed from domain- and target-specific dialects implemented on top of a shared infrastructure.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3469030",
      "sourceUrl": "https://doi.org/10.1145/3469030",
      "tags": [
        "GPU",
        "Infrastructure",
        "IR",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Oleksandr Zinenko",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w3176023707",
      "source": "openalex-discovery",
      "title": "Developer and user-transparent compiler optimization for interactive applications",
      "authors": [
        {
          "name": "Paschalis Mpeis",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Pavlos Petoumenos",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Kim Hazelwood",
          "affiliation": "Meta (United States)"
        },
        {
          "name": "Hugh Leather",
          "affiliation": "Meta (United States)"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Traditional offline optimization frameworks rely on representative hardware, software, and inputs to compare different optimization decisions on. With application-specific optimization for mobile systems though, the idea of a representative testbench is unrealistic while creating offline inputs<br/>is non-trivial. Online approaches partially overcome these problems but they might expose users to suboptimal or even erroneously optimized code. As a result, our mobile code is poorly optimized and this results in wasted performance, wasted energy, and user frustration. In this paper, we introduce a novel compiler optimization approach designed for mobile applications. It requires no developer effort, it tunes applications for the user’s device and usage patterns, and has no negative impact on the user experience. It is based on a lightweight capture and replay mechanism. In its online stage, it captures the state accessed by any targeted code region. By re-purposing existing OS capabilities, it keeps the overhead low. In its offline stage, it replays the code region but under different optimization decisions to enable sound comparisons of different optimizations<br/>under realistic conditions. Coupled with a search heuristic for the compiler optimization space, it allows us to discover optimization decisions that improve performance without testing these decisions directly on the user. We implemented a prototype system in Android based on LLVM combined with a genetic search engine. We evaluated it on both benchmarks and real Android applications. Online captures are infrequent and each one introduces an overhead of less than 15ms on average. For this negligible effect on user experience, we achieve speedups of 44% on average over the Android compiler and 35% over LLVM -O3.",
      "paperUrl": "https://research.manchester.ac.uk/en/publications/212271cc-8446-4c0f-9463-2354d6d81fb4",
      "sourceUrl": "https://doi.org/10.1145/3453483.3454043",
      "tags": [
        "Optimizations",
        "Performance",
        "Rust",
        "Testing"
      ],
      "matchedAuthors": [
        "Hugh Leather",
        "Kim Hazelwood",
        "Paschalis Mpeis"
      ]
    },
    {
      "id": "openalex-w3160539805",
      "source": "openalex-discovery",
      "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop Optimization Transformations",
      "authors": [
        {
          "name": "Jaehoon Koo",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Prasanna Balaprakash",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Michael Kruse",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Xingfu Wu",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Paul Hovland",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Mary Hall",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Polly is the LLVM project's polyhedral loop optimizer. Recent user-directed loop transformation pragmas were proposed based on LLVM/Clang and Polly. The search space exposed by the transformation pragmas is a tree, wherein each node represents a specific combination of loop transformations that can be applied to the code resulting from the parent node's loop transformations. To find the best combination of these loop transformations, we have developed a search algorithm based on Monte Carlo tree search (MCTS). The algorithm consists of two phases: exploring loop transformations at different depths of the tree to identify promising regions in the tree search space and exploiting those regions by performing a local search. Moreover, a restart mechanism is used to avoid the MCTS getting trapped in a local solution. The best and worst solutions are transferred from the previous phases of the restarts to leverage the search history. We compare our approach with breadth-first, beam, global greedy, and random search methods using PolyBench benchmarks and ECP proxy applications. Experimental results show that our MCTS algorithm finds pragma combinations with a speedup of 2.3x over Polly's heuristic optimizations on average.",
      "paperUrl": "https://doi.org/10.1109/pmbs54543.2021.00015",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Loop transformations",
        "Optimizations",
        "Polly"
      ],
      "matchedAuthors": [
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w4226382273",
      "source": "openalex-discovery",
      "title": "Crux: Symbolic Execution Meets SMT-based Verification (Competition Contribution)",
      "authors": [
        {
          "name": "Ryan P. Scott",
          "affiliation": "Galois (United States)"
        },
        {
          "name": "Robert Dockins",
          "affiliation": "Galois (United States)"
        },
        {
          "name": "Tristan Ravitch",
          "affiliation": "Galois (United States)"
        },
        {
          "name": "Aaron Tomb",
          "affiliation": "Galois (United States)"
        }
      ],
      "year": "2021",
      "venue": "Zenodo (CERN European Organization for Nuclear Research) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Crux is a software verification tool that proves properties of imperative programs containing inline specifications. Crux translates programs into functional models that can be reasoned about in a symbolic execution engine, which ultimately uses SMT solvers to discharge proof obligations. There are several frontends to Crux, but the C frontend, Crux-LLVM, is the most mature at present.",
      "paperUrl": "https://doi.org/10.5281/zenodo.6147218",
      "sourceUrl": "",
      "tags": [
        "Frontend"
      ],
      "matchedAuthors": [
        "Tristan Ravitch"
      ]
    },
    {
      "id": "openalex-w3206147709",
      "source": "openalex-discovery",
      "title": "Cross-domain compilation: exploiting synergies across the CS community (keynote)",
      "authors": [
        {
          "name": "Tobias Grosser",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Developing a new programming language, constructing a new domain-specific compiler, writing a new verification tool, optimizing a large application, designing a microprocessor, or verifying some of its components, all of these tasks require today a multi-year project. While most of the underlying problems are inherently hard and cannot be accelerated magically, we are additionally slowed down by a lack of well-defined interfaces that prevent us to exploit synergies between CS sub-communities. In this presentation, I raise the question of how we can accelerate the innovation speed of our CS technology stack to levels recently seen in deep learning, battery electric vehicles, or rocket launches. While I won't provide an answer, I will share the latest developments from the LLVM compiler community where the recent introduction of MLIR initiated the design of numerous IR abstractions that can be freely composed to build hybrid tools crossing community boundaries, that can be analyzed to gain a deep understanding of the various IR abstractions, and which may be the seed of a new abstraction sharing economy in our community. I will share some of my very own steps in this space on analyzing and understanding the various IR abstractions already in existence and will point out new cross-community collaboration opportunities. This talk concludes by raising the question of how we as researchers can build impactful and lasting open-source communities to move from interfacing software to towards building bridges between communities.",
      "paperUrl": "https://doi.org/10.1145/3486606.3488074",
      "sourceUrl": "",
      "tags": [
        "IR",
        "MLIR"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w3201219443",
      "source": "openalex-discovery",
      "title": "Copy-and-patch compilation: a fast compilation algorithm for high-level languages and bytecode",
      "authors": [
        {
          "name": "Haoran Xu",
          "affiliation": "Stanford University"
        },
        {
          "name": "Fredrik Kjølstad",
          "affiliation": "Stanford University"
        }
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 5 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Fast compilation is important when compilation occurs at runtime, such as query compilers in modern database systems and WebAssembly virtual machines in modern browsers. We present copy-and-patch, an extremely fast compilation technique that also produces good quality code. It is capable of lowering both high-level languages and low-level bytecode programs to binary code, by stitching together code from a large library of binary implementation variants. We call these binary implementations stencils because they have holes where missing values must be inserted during code generation. We show how to construct a stencil library and describe the copy-and-patch algorithm that generates optimized binary code. We demonstrate two use cases of copy-and-patch: a compiler for a high-level C-like language intended for metaprogramming and a compiler for WebAssembly. Our high-level language compiler has negligible compilation cost: it produces code from an AST in less time than it takes to construct the AST. We have implemented an SQL database query compiler on top of this metaprogramming system and show that on TPC-H database benchmarks, copy-and-patch generates code two orders of magnitude faster than LLVM -O0 and three orders of magnitude faster than higher optimization levels. The generated code runs an order of magnitude faster than interpretation and 14% faster than LLVM -O0. Our WebAssembly compiler generates code 4.9X-6.5X faster than Liftoff, the WebAssembly baseline compiler in Google Chrome. The generated code also outperforms Liftoff's by 39%-63% on the Coremark and PolyBenchC WebAssembly benchmarks.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3485513",
      "sourceUrl": "https://doi.org/10.1145/3485513",
      "tags": [],
      "matchedAuthors": [
        "Haoran Xu"
      ]
    },
    {
      "id": "openalex-w3205875080",
      "source": "openalex-discovery",
      "title": "Compacting points-to sets through object clustering",
      "authors": [
        {
          "name": "Mohamad Barbar",
          "affiliation": "Commonwealth Scientific and Industrial Research Organisation"
        },
        {
          "name": "Yulei Sui",
          "affiliation": "University of Technology Sydney"
        }
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 5 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Inclusion-based set constraint solving is the most popular technique for whole-program points-to analysis whereby an analysis is typically formulated as repeatedly resolving constraints between points-to sets of program variables. The set union operation is central to this process. The number of points-to sets can grow as analyses become more precise and input programs become larger, resulting in more time spent performing unions and more space used storing these points-to sets. Most existing approaches focus on improving scalability of precise points-to analyses from an algorithmic perspective and there has been less research into improving the data structures behind the analyses. Bit-vectors as one of the more popular data structures have been used in several mainstream analysis frameworks to represent points-to sets. To store memory objects in bit-vectors, objects need to mapped to integral identifiers. We observe that this object-to-identifier mapping is critical for a compact points-to set representation and the set union operation. If objects in the same points-to sets (co-pointees) are not given numerically close identifiers, points-to resolution can cost significantly more space and time. Without data on the unpredictable points-to relations which would be discovered by the analysis, an ideal mapping is extremely challenging. In this paper, we present a new approach to inclusion-based analysis by compacting points-to sets through object clustering. Inspired by recent staged analysis where an auxiliary analysis produces results approximating a more precise main analysis, we formulate points-to set compaction as an optimisation problem solved by integer programming using constraints generated from the auxiliary analysis’s results in order to produce an effective mapping. We then develop a more approximate mapping, yet much more efficiently, using hierarchical clustering to compact bit-vectors. We also develop an improved representation of bit-vectors (called core bit-vectors) to fully take advantage of the newly produced mapping. Our approach requires no algorithmic change to the points-to analysis. We evaluate our object clustering on flow sensitive points-to analysis using 8 open-source programs (&gt;3.1 million lines of LLVM instructions) and our results show that our approach can successfully improve the analysis with an up to 1.83× speed up and an up to 4.05× reduction in memory usage.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3485547",
      "sourceUrl": "https://doi.org/10.1145/3485547",
      "tags": [],
      "matchedAuthors": [
        "Yulei Sui"
      ]
    },
    {
      "id": "openalex-w3203231867",
      "source": "openalex-discovery",
      "title": "Code Polymorphism Meets Code Encryption: Confidentiality and Side-channel Protection of Software Components",
      "authors": [
        {
          "name": "Lionel Morel",
          "affiliation": "Université Grenoble Alpes"
        },
        {
          "name": "Damien Couroussé",
          "affiliation": "CEA Grenoble"
        },
        {
          "name": "Thomas Hiscock",
          "affiliation": "Université Grenoble Alpes"
        }
      ],
      "year": "2021",
      "venue": "Digital Threats Research and Practice | Vol. 4 (Issue 2)",
      "type": "research-paper",
      "abstract": "In this article, we consider that, in practice, attack scenarios involving side-channel analysis combine two successive phases: an analysis phase, targeting the extraction of information about the target and the identification of possible vulnerabilities, and an exploitation phase, applying attack techniques on candidate vulnerabilities. We advocate that protections need to cover these two phases to be effective against real-life attacks. We present PolEn , a toolchain and a processor architecture that combine countermeasures to provide an effective mitigation of side-channel attacks: As a countermeasure against the analysis phase, our approach considers the use of code encryption; as a countermeasure against the exploitation phase, our approach considers the use of code polymorphism, because it relies on runtime code generation, and its combination with code encryption is particularly challenging. Code encryption is supported by a processor extension such that machine instructions are only decrypted inside the CPU, which effectively prevents reverse engineering or any extraction of useful information from memory dumps. Code polymorphism is implemented by software means. It regularly changes the observable behaviour of the program, making it unpredictable for an attacker, hence reducing the possibility to exploit side-channel leakages. We present a prototype implementation, based on the RISC-V Spike simulator and a modified LLVM toolchain. In our experimental evaluation, we illustrate that PolEn effectively reduces side-channel leakages. For the protected functions evaluated, static memory use increases by a factor of 5 to 22, corresponding to the joint application of code encryption and code polymorphism. The overhead, in terms of execution time, ranges between a factor of 1.8 and 4.6.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3487058",
      "sourceUrl": "https://doi.org/10.1145/3487058",
      "tags": [],
      "matchedAuthors": [
        "Damien Couroussé"
      ]
    },
    {
      "id": "openalex-w2515612765",
      "source": "openalex-discovery",
      "title": "Clean application compartmentalization with SOAAP (extended version)",
      "authors": [
        {
          "name": "Khilan Gudka",
          "affiliation": ""
        },
        {
          "name": "Robert N. M. Watson",
          "affiliation": ""
        },
        {
          "name": "Jonathan Anderson",
          "affiliation": ""
        },
        {
          "name": "David Chisnall",
          "affiliation": ""
        },
        {
          "name": "Brooks Davis",
          "affiliation": ""
        },
        {
          "name": "Ben Laurie",
          "affiliation": ""
        },
        {
          "name": "Ilias Marinos",
          "affiliation": ""
        },
        {
          "name": "Peter G. Neumann",
          "affiliation": ""
        },
        {
          "name": "A.J. Richardson",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "CL Technical Reports | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Application compartmentalization, a vulnerability mitigation technique employed in programs such as OpenSSH and the Chrome web browser, decomposes software into sandboxed components to limit privileges leaked or otherwise available to attackers. However, compartmentalizing applications – and maintaining that compartmentalization – is hindered by ad hoc methodologies and significantly increased programming effort. In practice, programmers stumble through (rather than overtly reason about) compartmentalization spaces of possible decompositions, unknowingly trading off correctness, security, complexity, and performance. We present a new conceptual framework embodied in an LLVM-based tool: the Security-Oriented Analysis of Application Programs (SOAAP) that allows programmers to reason about compartmentalization using source-code annotations (compartmentalization hypotheses). We demonstrate considerable benefit when creating new compartmentalizations for complex applications, and analyze existing compartmentalized applications to discover design faults and maintenance issues arising from application evolution. This technical report is an extended version of the similarly named conference paper presented at the 2015 ACM Conference on Computer and Communications Security (CCS).",
      "paperUrl": "https://doi.org/10.48456/tr-873",
      "sourceUrl": "",
      "tags": [
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Brooks Davis",
        "David Chisnall",
        "Jonathan Anderson",
        "Khilan Gudka",
        "Peter G. Neumann",
        "Robert N. M. Watson"
      ]
    },
    {
      "id": "openalex-w3212544850",
      "source": "openalex-discovery",
      "title": "Characterizing OpenMP Synchronization Implementations on ARMv8 Multi-Cores",
      "authors": [
        {
          "name": "Pengyu Wang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Wanrong Gao",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Jianbin Fang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Chun Huang",
          "affiliation": "National University of Defense Technology"
        },
        {
          "name": "Zheng Wang",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Synchronization operations like barriers are fre-quently seen in parallel OpenMP programs, where an inefficient implementation can severely limit the application performance. While synchronization optimization has been heavily studied on traditional x86 architectures, there is no consensus on how synchronization can be best implemented on the ARMv8 multi-core CPUs. This paper presents a study of OpenMP synchronization implementation on two representative ARMv8 multi-core architectures, Phytium 2000+ and ThunderX2, by considering various OpenMP synchronization mechanisms offered by two mainstreamed OpenMP compilers, GCC and LLVM. Our evalu-ation compares the performance, overhead and scalability of both compiler implementations. We show that there is no \"one-fits-for-all\" synchronization mechanism, and the efficiency of a scheme varies across hardware architectures and thread parallelism. We then share our insights and discuss how OpenMP synchronization operations can be better optimized on emerging ARMv8 multi-cores, offering quantified results for future research directions.",
      "paperUrl": "https://doi.org/10.1109/hpcc-dss-smartcity-dependsys53884.2021.00111",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Jianbin Fang",
        "Zheng Wang"
      ]
    },
    {
      "id": "openalex-w2516191413",
      "source": "openalex-discovery",
      "title": "Capability Hardware Enhanced RISC Instructions: CHERI User’s guide",
      "authors": [
        {
          "name": "Robert N. M. Watson",
          "affiliation": ""
        },
        {
          "name": "David Chisnall",
          "affiliation": ""
        },
        {
          "name": "Brooks Davis",
          "affiliation": ""
        },
        {
          "name": "Wojciech Koszek",
          "affiliation": ""
        },
        {
          "name": "Simon W. Moore",
          "affiliation": ""
        },
        {
          "name": "Steven J. Murdoch",
          "affiliation": ""
        },
        {
          "name": "Peter G. Neumann",
          "affiliation": ""
        },
        {
          "name": "Jonathan Woodruff",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "CL Technical Reports | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The CHERI User’s Guide documents the software environment for the Capability Hardware Enhanced RISC Instructions (CHERI) prototype developed by SRI International and the University of Cambridge. The User’s Guide is targeted at hardware and software developers working with capability-enhanced software. It describes the CheriBSD operating system, a version of the FreeBSD operating system that has been adapted to support userspace capability systems via the CHERI ISA, and the CHERI Clang/LLVM compiler suite. It also describes the earlier Deimos demonstration microkernel.",
      "paperUrl": "https://doi.org/10.48456/tr-851",
      "sourceUrl": "",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Brooks Davis",
        "David Chisnall",
        "Jonathan Woodruff",
        "Peter G. Neumann",
        "Robert N. M. Watson",
        "Simon W. Moore"
      ]
    },
    {
      "id": "openalex-w4226225561",
      "source": "openalex-discovery",
      "title": "COX: CUDA on X86 by Exposing Warp-Level Functions to CPUs",
      "authors": [
        {
          "name": "Ruobing Han",
          "affiliation": ""
        },
        {
          "name": "Jaewon Lee",
          "affiliation": ""
        },
        {
          "name": "Jaewoong Sim",
          "affiliation": ""
        },
        {
          "name": "Hyesoon Kim",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As CUDA programs become the de facto program among data parallel applications such as high-performance computing or machine learning applications, running CUDA on other platforms has been a compelling option. Although several efforts have attempted to support CUDA on other than NVIDIA GPU devices, due to extra steps in the translation, the support is always behind a few years from supporting CUDA's latest features. The examples are DPC, Hipfy, where CUDA source code have to be translated to their native supporting language and then they are supported. In particular, the new CUDA programming model exposes the warp concept in the programming language, which greatly changes the way the CUDA code should be mapped to CPU programs. In this paper, hierarchical collapsing that \\emph{correctly} supports CUDA warp-level functions on CPUs is proposed. Based on hierarchical collapsing, a framework, COX, is developed that allows CUDA programs with the latest features to be executed efficiently on CPU platforms. COX consists of a compiler IR transformation (new LLVM pass) and a runtime system to execute the transformed programs on CPU devices. COX can support the most recent CUDA features, and the application coverage is much higher (90%) than for previous frameworks (68%) with comparable performance. We also show that the warp-level functions in CUDA can be efficiently executed by utilizing CPU SIMD (AVX) instructions.",
      "paperUrl": "https://arxiv.org/pdf/2112.10034",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2112.10034",
      "tags": [
        "CUDA",
        "GPU",
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Ruobing Han"
      ]
    },
    {
      "id": "openalex-w3037883157",
      "source": "openalex-discovery",
      "title": "CHERI C/C++ Programming Guide",
      "authors": [
        {
          "name": "Robert N. M. Watson",
          "affiliation": ""
        },
        {
          "name": "Alexander Richardson",
          "affiliation": ""
        },
        {
          "name": "Brooks Davis",
          "affiliation": ""
        },
        {
          "name": "John Baldwin",
          "affiliation": ""
        },
        {
          "name": "David Chisnall",
          "affiliation": ""
        },
        {
          "name": "Jessica Clarke",
          "affiliation": ""
        },
        {
          "name": "Nathaniel Wesley Filardo",
          "affiliation": ""
        },
        {
          "name": "Simon W. Moore",
          "affiliation": ""
        },
        {
          "name": "Edward Napierala",
          "affiliation": ""
        },
        {
          "name": "Peter Sewell",
          "affiliation": ""
        },
        {
          "name": "Peter G. Neumann",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "CL Technical Reports | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This document is a brief introduction to the CHERI C/C++ programming languages. We explain the principles underlying these language variants, and their grounding in CHERI’s multiple architectural instantiations: CHERI-MIPS, CHERI-RISC-V, and Arm’s Morello. We describe the most commonly encountered differences between these dialects and C/C++ on conventional architectures, and where existing software may require minor changes. We document new compiler warnings and errors that may be experienced compiling code with the CHERI Clang/LLVM compiler, and suggest how they may be addressed through typically minor source-code changes. We explain how modest language extensions allow selected software, such as memory allocators, to further refine permissions and bounds on pointers. This guidance is based on our experience adapting the FreeBSD operating-system userspace, and applications such as PostgreSQL and WebKit, to run in a CHERI C/C++ capability-based programming environment. We conclude by recommending further reading.",
      "paperUrl": "https://doi.org/10.48456/tr-947",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Alexander Richardson",
        "Brooks Davis",
        "David Chisnall",
        "Peter G. Neumann",
        "Robert N. M. Watson",
        "Simon W. Moore"
      ]
    },
    {
      "id": "openalex-w3178385707",
      "source": "openalex-discovery",
      "title": "C4: the C compiler concurrency checker",
      "authors": [
        {
          "name": "Matt Windsor",
          "affiliation": "University of York"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        },
        {
          "name": "John Wickerson",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The correct compilation of atomic-action concurrency is vital now that multicore processors are ubiquitous. Despite much recent work on automated compiler testing, little existing tooling can test how real-world compilers handle compilation of atomic-action code. We demonstrate C4, a tool for exploring the concurrency behaviour of real-world C compilers such as GCC and LLVM. C4 automates a workflow based on generating, fuzzing, and executing litmus tests. So far, C4 has found two new control-flow bugs in GCC and IBM XL, and reproduced two historic concurrency bugs in GCC 4.",
      "paperUrl": "https://doi.org/10.1145/3460319.3469079",
      "sourceUrl": "",
      "tags": [
        "Testing"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson"
      ]
    },
    {
      "id": "openalex-w3214154787",
      "source": "openalex-discovery",
      "title": "Autotuning PolyBench benchmarks with LLVM Clang/Polly loop optimization pragmas using Bayesian optimization",
      "authors": [
        {
          "name": "Xingfu Wu",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Michael Kruse",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Prasanna Balaprakash",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Hal Finkel",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Paul Hovland",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Valerie Taylor",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Mary Hall",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2021",
      "venue": "Concurrency and Computation Practice and Experience | Vol. 34 (Issue 20)",
      "type": "research-paper",
      "abstract": "Abstract We develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat‐3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd–Warshall benchmark did not benefit from autotuning. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd–Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.",
      "paperUrl": "https://www.osti.gov/servlets/purl/1883233",
      "sourceUrl": "https://doi.org/10.1002/cpe.6683",
      "tags": [
        "Clang",
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Hal Finkel",
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w3157174609",
      "source": "openalex-discovery",
      "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization Pragmas Using Bayesian Optimization (extended version)",
      "authors": [
        {
          "name": "Xingfu Wu",
          "affiliation": ""
        },
        {
          "name": "Michael Kruse",
          "affiliation": ""
        },
        {
          "name": "Prasanna Balaprakash",
          "affiliation": ""
        },
        {
          "name": "Hal Finkel",
          "affiliation": ""
        },
        {
          "name": "Paul Hovland",
          "affiliation": ""
        },
        {
          "name": "Valerie Taylor",
          "affiliation": ""
        },
        {
          "name": "Mary Hall",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we develop a ytopt autotuning framework that leverages Bayesian optimization to explore the parameter space search and compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We select six of the most complex PolyBench benchmarks and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We find that the Floyd-Warshall benchmark did not benefit from autotuning because Polly uses heuristics to optimize the benchmark to make it run much slower. To cope with this issue, we provide some compiler option solutions to improve the performance. Then we present loop autotuning without a user's knowledge using a simple mctree autotuning framework to further improve the performance of the Floyd-Warshall benchmark. We also extend the ytopt autotuning framework to tune a deep learning application.",
      "paperUrl": "https://arxiv.org/pdf/2104.13242",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2104.13242",
      "tags": [
        "Clang",
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Hal Finkel",
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w3159646613",
      "source": "openalex-discovery",
      "title": "Automatically enforcing fresh and consistent inputs in intermittent systems",
      "authors": [
        {
          "name": "Milijana Surbatovich",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Limin Jia",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Brandon Lucia",
          "affiliation": "Carnegie Mellon University"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Intermittently powered energy-harvesting devices enable new applications in\\ninaccessible environments. Program executions must be robust to unpredictable\\npower failures, introducing new challenges in programmability and correctness.\\nOne hard problem is that input operations have implicit constraints, embedded\\nin the behavior of continuously powered executions, on when input values can be\\ncollected and used. This paper aims to develop a formal framework for enforcing\\nthese constraints. We identify two key properties -- freshness (i.e., uses of\\ninputs must satisfy the same time constraints as in continuous executions) and\\ntemporal consistency (i.e., the collection of a set of inputs must satisfy the\\nsame time constraints as in continuous executions). We formalize these\\nproperties and show that they can be enforced using atomic regions. We develop\\nOcelot, an LLVM-based analysis and transformation tool targeting Rust, to\\nenforce these properties automatically. Ocelot provides the programmer with\\nannotations to express these constraints and infers atomic region placement in\\na program to satisfy them. We then formalize Ocelot's design and show that\\nOcelot generates correct programs with little performance cost or code changes.\\n",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3453483.3454081",
      "sourceUrl": "https://doi.org/10.1145/3453483.3454081",
      "tags": [
        "Embedded",
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Brandon Lucia"
      ]
    },
    {
      "id": "openalex-w3116487210",
      "source": "openalex-discovery",
      "title": "An abstract interpretation for SPMD divergence on reducible control flow graphs",
      "authors": [
        {
          "name": "Julian Rosemann",
          "affiliation": "Saarland University"
        },
        {
          "name": "Simon Moll",
          "affiliation": "NEC (Germany)"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 5 (Issue POPL)",
      "type": "research-paper",
      "abstract": "Vectorizing compilers employ divergence analysis to detect at which program point a specific variable is uniform, i.e. has the same value on all SPMD threads that execute this program point. They exploit uniformity to retain branching to counter branch divergence and defer computations to scalar processor units. Divergence is a hyper-property and is closely related to non-interference and binding time. There exist several divergence, binding time, and non-interference analyses already but they either sacrifice precision or make significant restrictions to the syntactical structure of the program in order to achieve soundness. In this paper, we present the first abstract interpretation for uniformity that is general enough to be applicable to reducible CFGs and, at the same time, more precise than other analyses that achieve at least the same generality. Our analysis comes with a correctness proof that is to a large part mechanized in Coq. Our experimental evaluation shows that the compile time and the precision of our analysis is on par with LLVM's default divergence analysis that is only sound on more restricted CFGs. At the same time, our analysis is faster and achieves better precision than a state-of-the-art non-interference analysis that is sound and at least as general as our analysis.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3434312",
      "sourceUrl": "https://doi.org/10.1145/3434312",
      "tags": [],
      "matchedAuthors": [
        "Sebastian Hack",
        "Simon Moll"
      ]
    },
    {
      "id": "openalex-w3184692337",
      "source": "openalex-discovery",
      "title": "An SMT Encoding of LLVM’s Memory Model for Bounded Translation Validation",
      "authors": [
        {
          "name": "Juneyoung Lee",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Dongjoo Kim",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Chung-Kil Hur",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Nuno P. Lopes",
          "affiliation": "Microsoft Research (United Kingdom)"
        }
      ],
      "year": "2021",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract Several automatic verification tools have been recently developed to verify subsets of LLVM’s optimizations. However, none of these tools has robust support to verify memory optimizations. In this paper, we present the first SMT encoding of LLVM’s memory model that 1) is sufficiently precise to validate all of LLVM’s intra-procedural memory optimizations, and 2) enables bounded translation validation of programs with up to hundreds of thousands of lines of code. We implemented our new encoding in Alive2, a bounded translation validation tool, and used it to uncover 21 new bugs in LLVM memory optimizations, 10 of which have been already fixed. We also found several inconsistencies in LLVM IR’s official specification document (LangRef) and fixed LLVM’s code and the document so they are in agreement.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-030-81688-9_35.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-030-81688-9_35",
      "tags": [
        "IR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Chung-Kil Hur",
        "Juneyoung Lee",
        "Nuno P. Lopes"
      ]
    },
    {
      "id": "openalex-w2978621079",
      "source": "openalex-discovery",
      "title": "An Introduction to CHERI",
      "authors": [
        {
          "name": "Robert N. M. Watson",
          "affiliation": ""
        },
        {
          "name": "Simon W. Moore",
          "affiliation": ""
        },
        {
          "name": "Peter Sewell",
          "affiliation": ""
        },
        {
          "name": "Peter G. Neumann",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "CL Technical Reports | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "CHERI (Capability Hardware Enhanced RISC Instructions) extends conventional processor Instruction-Set Architectures (ISAs) with architectural capabilities to enable fine-grained memory protection and highly scalable software compartmentalization. CHERI’s hybrid capability-system approach allows architectural capabilities to be integrated cleanly with contemporary RISC architectures and microarchitectures, as well as with MMU-based C/C++-language software stacks. CHERI’s capabilities are unforgeable tokens of authority, which can be used to implement both explicit pointers (those declared in the language) and implied pointers (those used by the runtime and generated code) in C and C++. When used for C/C++ memory protection, CHERI directly mitigates a broad range of known vulnerability types and exploit techniques. Support for more scalable software compartmentalization facilitates software mitigation techniques such as sandboxing, which also defend against future (currently unknown) vulnerability classes and exploit techniques. We have developed, evaluated, and demonstrated this approach through hardware-software prototypes, including multiple CPU prototypes, and a full software stack. This stack includes an adapted version of the Clang/LLVM compiler suite with support for capability-based C/C++, and a full UNIX-style OS (CheriBSD, based on FreeBSD) implementing spatial, referential, and (currently for userspace) non-stack temporal memory safety. Formal modeling and verification allow us to make strong claims about the security properties of CHERI-enabled architectures. This report is a high-level introduction to CHERI. The report describes our architectural approach, CHERI’s key microarchitectural implications, our approach to formal modeling and proof, the CHERI software model, our software-stack prototypes, further reading, and potential areas of future research.",
      "paperUrl": "https://doi.org/10.48456/tr-941",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Security"
      ],
      "matchedAuthors": [
        "Peter G. Neumann",
        "Robert N. M. Watson",
        "Simon W. Moore"
      ]
    },
    {
      "id": "openalex-w3164004581",
      "source": "openalex-discovery",
      "title": "Alive2: bounded translation validation for LLVM",
      "authors": [
        {
          "name": "Nuno P. Lopes",
          "affiliation": "Microsoft Research (United Kingdom)"
        },
        {
          "name": "Juneyoung Lee",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Chung-Kil Hur",
          "affiliation": "Seoul National University"
        },
        {
          "name": "Zhengyang Liu",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "© 2021 ACM.We designed, implemented, and deployed Alive2: a bounded translation validation tool for the LLVM compiler&amp;apos;s intermediate representation (IR). It limits resource consumption by, for example, unrolling loops up to some bound, which means there are circumstances in which it misses bugs. Alive2 is designed to avoid false alarms, is fully automatic through the use of an SMT solver, and requires no changes to LLVM. By running Alive2 over LLVM&amp;apos;s unit test suite, we discovered and reported 47 new bugs, 28 of which have been fixed already. Moreover, our work has led to eight patches to the LLVM Language Reference-the definitive description of the semantics of its IR-and we have participated in numerous discussions with the goal of clarifying ambiguities and fixing errors in these semantics. Alive2 is open source and we also made it available on the web, where it has active users from the LLVM community.",
      "paperUrl": "https://doi.org/10.1145/3453483.3454030",
      "sourceUrl": "",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Chung-Kil Hur",
        "John Regehr",
        "Juneyoung Lee",
        "Nuno P. Lopes",
        "Zhengyang Liu"
      ]
    },
    {
      "id": "openalex-w3203635996",
      "source": "openalex-discovery",
      "title": "Advancing OpenMP Offload Debugging Capabilities in LLVM",
      "authors": [
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Joseph Huber",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Melanie Cornelius",
          "affiliation": "Illinois Institute of Technology"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Debugging an application is famously twice as hard as writing the application in the first place. While this sentiment predates modern GPU programming by decades, it is all the more true when the application has to manage computation and memory across different architectures, memory spaces, and execution modes. Any subtle error, whether in the application, the compiler, or runtime system, can lead to unexpected behavior that is hard to understand from the program output alone. While some tooling solutions for GPU debugging exist, their maturity and usefulness varies gravely between vendors. Furthermore, as OpenMP offloading puts an abstraction layer between the programmer and the underlying hardware, the information from a native GPU driver (debugging tool) is not always transferable to the OpenMP programming model. As the OpenMP Tooling [12] (OMPT) and Debug [4] (OMPD) interfaces are still not ready to debug OpenMP offloading code in production, developers have a hard time to comprehend the implementation state, error sources, and interplay of the OpenMP world with the foreign device runtimes, e.g., CUDA.",
      "paperUrl": "https://doi.org/10.1145/3458744.3473358",
      "sourceUrl": "",
      "tags": [
        "CUDA",
        "GPU"
      ],
      "matchedAuthors": [
        "Johannes Doerfert",
        "Joseph Huber"
      ]
    },
    {
      "id": "openalex-w3203599871",
      "source": "openalex-discovery",
      "title": "Adapting SYCL’s SIMT Programming Paradigm for Accelerators via Program Reconstruction",
      "authors": [
        {
          "name": "Jiashu Wang",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Xun Deng",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "Kai-Ting Amy Wang",
          "affiliation": "Huawei Technologies (Canada)"
        },
        {
          "name": "ZiChun Ye",
          "affiliation": "Huawei Technologies (Canada)"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present an IR-to-IR Converter that is capable of converting from LLVM IR to Halide IR and MLIR's Affine Dialect to support generation of high performance SYCL kernel code [10] targeting accelerators with explicit memory hierarchy design. The converter performs program reconstruction to raise abstraction of the IR. Once the IR is raised to the level of Halide IR, AKG [2] can be leveraged to generate performant DaVinci code [2]. Alternatively, when the IR is raised to MLIR's Affine Dialect, existing MLIR Affine passes with minor modifications can be readily used. Subsequently, the IR is lowered back to LLVM IR through progressive lowering. LLVM's LLC is used to create the final binary for both cases. We extend upon SYCL's buffer, accessor and parallel_for abstractions to facilitate the IR raising process.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3458744.3473354",
      "sourceUrl": "https://doi.org/10.1145/3458744.3473354",
      "tags": [
        "IR",
        "MLIR",
        "Performance"
      ],
      "matchedAuthors": [
        "Kai-Ting Amy Wang"
      ]
    },
    {
      "id": "openalex-w3176080641",
      "source": "openalex-discovery",
      "title": "ARBALEST: Dynamic Detection of Data Mapping Issues in Heterogeneous OpenMP Applications",
      "authors": [
        {
          "name": "Lechen Yu",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Joachim Protze",
          "affiliation": "RWTH Aachen University"
        },
        {
          "name": "Óscar Hernández",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Vivek Sarkar",
          "affiliation": "Georgia Institute of Technology"
        }
      ],
      "year": "2021",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "From OpenMP 4.0 onwards, programmers can offload code regions to accelerators by using the target offloading feature. However, incorrect usage of target offloading constructs may incur data mapping issues. A data mapping issue occurs when the host fails to observe updates on the accelerator or vice versa. It may further lead to multiple memory issues such as use of uninitialized memory, use of stale data, and data race. To the best of our knowledge, currently there is no prior work on dynamic detection of data mapping issues in heterogeneous OpenMP applications.In this paper, we identify possible root causes of data mapping issues in OpenMP's standard memory model and the unified memory model. We find that data mapping issues primarily result from incorrect settings of map and nowait clauses in target offloading constructs. Further, the novel unified memory model introduced in OpenMP 5.0 cannot avoid the occurrence of data mapping issues. To mitigate the difficulty of detecting data mapping issues, we propose ARBALEST, an on-the-fly data mapping issue detector for OpenMP applications. For each variable mapped to the accelerator, ARBALEST's detection algorithm leverages a state machine to track the last write's visibility. ARBALEST requires constant storage space for each memory location and takes amortized constant time per memory access. To demonstrate ARBALEST's effectiveness, an experimental comparison with four other dynamic analysis tools (Valgrind, Archer, AddressSanitizer, MemorySanitizer) has been carried out on a number of open-source benchmark suites. The evaluation results show that ARBALEST delivers demonstrably better precision than the other four tools, and its execution time overhead is comparable to that of state-of-the-art dynamic analysis tools.",
      "paperUrl": "https://doi.org/10.1109/ipdps49936.2021.00055",
      "sourceUrl": "",
      "tags": [
        "Dynamic Analysis"
      ],
      "matchedAuthors": [
        "Joachim Protze"
      ]
    },
    {
      "id": "openalex-w3216421790",
      "source": "openalex-discovery",
      "title": "A Translation Validation Algorithm for LLVM Register Allocators",
      "authors": [
        {
          "name": "Zhengyao Lin",
          "affiliation": ""
        },
        {
          "name": "Theodoros Kasampalis",
          "affiliation": ""
        },
        {
          "name": "Vikram Adve",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "IDEALS (University of Illinois Urbana-Champaign) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Register allocation is a crucial and complex phase of any modern production compiler. In this work, we present a translation validation algorithm that verifies each single instance of register allocation. Our algorithm is external to the compiler and independent of the register allocation algorithm. We support all major register allocation optimizations such as live range splitting, register coalescing, and rematerialization. We developed a prototype of this approach for the production-quality register allocation phase of LLVM. We evaluated this prototype for compiling the source code of SPECint 2006 benchmark, and we were able to verify the register allocation of over 88% of supported functions in the benchmark, using all 4 register allocators available in LLVM 5.0.2.",
      "paperUrl": "http://hdl.handle.net/2142/112734",
      "sourceUrl": "",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "Vikram Adve"
      ]
    },
    {
      "id": "openalex-w3157676465",
      "source": "openalex-discovery",
      "title": "A Reinforcement Learning Environment for Polyhedral Optimizations",
      "authors": [
        {
          "name": "Alexander Brauckmann",
          "affiliation": ""
        },
        {
          "name": "Andrés Goens",
          "affiliation": ""
        },
        {
          "name": "Jerónimo Castrillón",
          "affiliation": ""
        }
      ],
      "year": "2021",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The polyhedral model allows a structured way of defining semantics-preserving transformations to improve the performance of a large class of loops. Finding profitable points in this space is a hard problem which is usually approached by heuristics that generalize from domain-expert knowledge. Existing problem formulations in state-of-the-art heuristics depend on the shape of particular loops, making it hard to leverage generic and more powerful optimization techniques from the machine learning domain. In this paper, we propose PolyGym, a shape-agnostic formulation for the space of legal transformations in the polyhedral model as a Markov Decision Process (MDP). Instead of using transformations, the formulation is based on an abstract space of possible schedules. In this formulation, states model partial schedules, which are constructed by actions that are reusable across different loops. With a simple heuristic to traverse the space, we demonstrate that our formulation is powerful enough to match and outperform state-of-the-art heuristics. On the Polybench benchmark suite, we found transformations that led to a speedup of 3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our generic MDP formulation enables using reinforcement learning to learn optimization policies over a wide range of loops. This also contributes to the emerging field of machine learning in compilers, as it exposes a novel problem formulation that can push the limits of existing methods.",
      "paperUrl": "https://arxiv.org/pdf/2104.13732",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2104.13732",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexander Brauckmann"
      ]
    },
    {
      "id": "openalex-w4213283227",
      "source": "openalex-discovery",
      "title": "(WiP) LLTFI: Low-Level Tensor Fault Injector",
      "authors": [
        {
          "name": "Abraham Chan",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Udit Kumar Agarwal",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Karthik Pattabiraman",
          "affiliation": "University of British Columbia"
        }
      ],
      "year": "2021",
      "venue": "2021 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW) | Vol. 32 (Issue None)",
      "type": "research-paper",
      "abstract": "As machine learning (ML) has become more prevalent across many critical domains, so has the need to understand ML system resilience. While previous work has focused on building ML fault injectors at the application level, there has been little work enabling fault injection of ML applications at a lower level. We present LLTFI, a tool under development, which allows users to run fault injection experiments on C/C++, TensorFlow and PyTorch applications at the LLVM IR level. LLTFI provides users with greater fault injection granularity and a better ability to understand how faults manifest and propagate between programmed and ML components. We demonstrate how LLTFI can be applied to a ML application with an end-to-end example.",
      "paperUrl": "https://doi.org/10.1109/issrew53611.2021.00045",
      "sourceUrl": "",
      "tags": [
        "C++",
        "IR",
        "ML"
      ],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w3112145364",
      "source": "openalex-discovery",
      "title": "ρFEM: Efficient Backward-edge Protection Using Reversed Forward-edge Mappings",
      "authors": [
        {
          "name": "Paul Muntean",
          "affiliation": ""
        },
        {
          "name": "Mathias Neumayer",
          "affiliation": ""
        },
        {
          "name": "Zhiqiang Lin",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Gang Tan",
          "affiliation": "Pennsylvania State University"
        },
        {
          "name": "Jens Großklags",
          "affiliation": ""
        },
        {
          "name": "Claudia Eckert",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "Annual Computer Security Applications Conference | Vol. 200 (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we propose reversed forward-edge mapper (ρFEM), a Clang/LLVM compiler-based tool, to protect the backward edges of a program's control flow graph (CFG) against runtime control-flow hijacking (e.g., code reuse attacks). It protects backward-edge transfers in C/C++ originating from virtual and non-virtual functions by first statically constructing a precise virtual table hierarchy, with which to form a precise forward-edge mapping between callees and non-virtual calltargets based on precise function signatures, and then checks each instrumented callee return against the previously computed set at runtime. We have evaluated ρFEM using the Chrome browser, NodeJS, Nginx, Memcached, and the SPEC CPU2017 benchmark. Our results show that ρFEM enforces less than 2.77 return targets per callee in geomean, even for applications heavily relying on backward edges. ρFEM's runtime overhead is less than 1% in geomean for the SPEC CPU2017 benchmark and 3.44% in geomean for the Chrome browser.",
      "paperUrl": "https://doi.org/10.1145/3427228.3427246",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Gang Tan"
      ]
    },
    {
      "id": "openalex-w3107591246",
      "source": "openalex-discovery",
      "title": "Who is Debugging the Debuggers? Exposing Debug Information Bugs in Optimized Binaries",
      "authors": [
        {
          "name": "Giuseppe Antonio Di Luna",
          "affiliation": ""
        },
        {
          "name": "Davide Italiano",
          "affiliation": ""
        },
        {
          "name": "Luca Massarelli",
          "affiliation": ""
        },
        {
          "name": "Sebastian Österlund",
          "affiliation": ""
        },
        {
          "name": "Cristiano Giuffrida",
          "affiliation": ""
        },
        {
          "name": "Leonardo Querzoni",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Despite the advancements in software testing, bugs still plague deployed software and result in crashes in production. When debugging issues -- sometimes caused by \"heisenbugs\" -- there is the need to interpret core dumps and reproduce the issue offline on the same binary deployed. This requires the entire toolchain (compiler, linker, debugger) to correctly generate and use debug information. Little attention has been devoted to checking that such information is correctly preserved by modern toolchains' optimization stages. This is particularly important as managing debug information in optimized production binaries is non-trivial, often leading to toolchain bugs that may hinder post-deployment debugging efforts. In this paper, we present Debug$^{2}$, a framework to find debug information bugs in modern toolchains. Our framework feeds random source programs to the target toolchain and surgically compares the debugging behavior of their optimized/unoptimized binary variants. Such differential analysis allows Debug$^{2}$ to check invariants at each debugging step and detect bugs from invariant violations. Our invariants are based on the (in)consistency of common debug entities, such as source lines, stack frames, and function arguments. We show that, while simple, this strategy yields powerful cross-toolchain and cross-language invariants, which can pinpoint several bugs in modern toolchains. We have used Debug$^{2}$ to find 23 bugs in the LLVM toolchain (clang/lldb), 8 bugs in the GNU toolchain (GCC/gdb), and 3 in the Rust toolchain (rustc/lldb) -- with 14 bugs already fixed by the developers.",
      "paperUrl": "https://arxiv.org/pdf/2011.13994",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2011.13994",
      "tags": [
        "Clang",
        "Debug Information",
        "LLDB",
        "Rust",
        "Testing"
      ],
      "matchedAuthors": [
        "Cristiano Giuffrida"
      ]
    },
    {
      "id": "openalex-w3043667402",
      "source": "openalex-discovery",
      "title": "Varity: Quantifying Floating-Point Variations in HPC Systems Through Randomized Testing",
      "authors": [
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Floating-point arithmetic can be confusing and it is sometimes misunderstood by programmers. While numerical reproducibility is desirable in HPC, it is often unachievable due to the different ways compilers treat floating-point arithmetic and generate code around it. This reproducibility problem is exacerbated in heterogeneous HPC systems where code can be executed on different floating-point hardware, e.g., a host and a device architecture, producing in some situations different numerical results. We present VARITY, a tool to quantify floatingpoint variations in heterogeneous HPC systems. Our approach generates random test programs for multiple architectures (host and device) using the compilers that are available in the system. Using differential testing, it compares floating-point results and identifies unexpected variations in the program results. The results can guide programmers in choosing the compilers that produce the most similar results in a system, which is useful when numerical reproducibility is critical. By running 50,000 experiments with Varity on a system with IBM POWER9 CPUs, NVIDIA V100 GPUs, and four compilers (gcc, clang, xl, and nvcc), we identify and document several programs that produce significantly different results for a given input when different compilers or architectures are used, even when a similar optimization level is used everywhere.",
      "paperUrl": "https://doi.org/10.1109/ipdps47924.2020.00070",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Testing"
      ],
      "matchedAuthors": [
        "Ignacio Laguna"
      ]
    },
    {
      "id": "openalex-w3093846115",
      "source": "openalex-discovery",
      "title": "Tutorial: LLVM for Security Practitioners",
      "authors": [
        {
          "name": "John Criswell",
          "affiliation": "University of Rochester"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Many security researchers need to build tools that analyze and transform code. For example, researchers may want to build security hardening tools, tools that find vulnerabilities within software, or tools that prove that a program is invulnerable to attack. This tutorial will guide attendees through creating extensions to the LLVM compiler that perform simple analysis and transformation operations.",
      "paperUrl": "https://doi.org/10.1109/secdev45635.2020.00013",
      "sourceUrl": "",
      "tags": [
        "Security"
      ],
      "matchedAuthors": [
        "John Criswell"
      ]
    },
    {
      "id": "openalex-w3036184137",
      "source": "openalex-discovery",
      "title": "TDO-CIM: Transparent Detection and Offloading for Computation In-memory",
      "authors": [
        {
          "name": "Kanishkan Vadivel",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Lorenzo Chelini",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Ali BanaGozar",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Gagandeep Singh",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Stefano Corda",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Roel Jordans",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Henk Corporaal",
          "affiliation": "Eindhoven University of Technology"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Computation in-memory is a promising non-von Neumann approach aiming at\\ncompletely diminishing the data transfer to and from the memory subsystem.\\nAlthough a lot of architectures have been proposed, compiler support for such\\narchitectures is still lagging behind. In this paper, we close this gap by\\nproposing an end-to-end compilation flow for in-memory computing based on the\\nLLVM compiler infrastructure. Starting from sequential code, our approach\\nautomatically detects, optimizes, and offloads kernels suitable for in-memory\\nacceleration. We demonstrate our compiler tool-flow on the PolyBench/C\\nbenchmark suite and evaluate the benefits of our proposed in-memory\\narchitecture simulated in Gem5 by comparing it with a state-of-the-art von\\nNeumann architecture.\\n",
      "paperUrl": "https://research.tue.nl/en/publications/c17d6ddb-cfc9-4039-950c-3c066d0b6645",
      "sourceUrl": "https://doi.org/10.23919/date48585.2020.9116464",
      "tags": [
        "Infrastructure"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini"
      ]
    },
    {
      "id": "openalex-w3033606455",
      "source": "openalex-discovery",
      "title": "Scalable validation of binary lifters",
      "authors": [
        {
          "name": "Sandeep Dasgupta",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "S. Dinesh",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Deepan Venkatesh",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Vikram Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Christopher W. Fletcher",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Validating the correctness of binary lifters is pivotal to gain trust in binary analysis, especially when used in scenarios where correctness is important. Existing approaches focus on validating the correctness of lifting instructions or basic blocks in isolation and do not scale to full programs. In this work, we show that formal translation validation of single instructions for a complex ISA like x86-64 is not only practical, but can be used as a building block for scalable full-program validation. Our work is the first to do translation validation of single instructions on an architecture as extensive as x86-64, uses the most precise formal semantics available, and has the widest coverage in terms of the number of instructions tested for correctness. Next, we develop a novel technique that uses validated instructions to enable program-level validation, without resorting to performance-heavy semantic equivalence checking. Specifically, we compose the validated IR sequences using a tool we develop called Compositional Lifter to create a reference standard. The semantic equivalence check between the reference and the lifter output is then reduced to a graph-isomorphism check through the use of semantic preserving transformations. The translation validation of instructions in isolation revealed 29 new bugs in McSema – a mature open-source lifter from x86-64 to LLVM IR. Towards the validation of full programs, our approach was able to prove the translational correctness of 2254/2348 functions taken from LLVM's single-source benchmark test-suite.",
      "paperUrl": "https://doi.org/10.1145/3385412.3385964",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Vikram Adve"
      ]
    },
    {
      "id": "openalex-w3014574736",
      "source": "openalex-discovery",
      "title": "Saturation Memory Access: Mitigating Memory Spatial Errors without Terminating Programs",
      "authors": [
        {
          "name": "Dongwei Chen",
          "affiliation": ""
        },
        {
          "name": "Daliang Xu",
          "affiliation": ""
        },
        {
          "name": "Dong Tong",
          "affiliation": ""
        },
        {
          "name": "kang dae sun",
          "affiliation": ""
        },
        {
          "name": "Xuetao Guan",
          "affiliation": ""
        },
        {
          "name": "Chun Yang",
          "affiliation": ""
        },
        {
          "name": "Xu Cheng",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Memory spatial errors, i.e., buffer overflow vulnerabilities, have been a well-known issue in computer security for a long time and remain one of the root causes of exploitable vulnerabilities. Most of the existing mitigation tools adopt a fail-stop strategy to protect programs from intrusions, which means the victim program will be terminated upon detecting a memory safety violation. Unfortunately, the fail-stop strategy harms the availability of software. In this paper, we propose Saturation Memory Access (SMA), a memory spatial error mitigation mechanism that prevents out-of-bounds access without terminating a program. SMA is based on a key observation that developers generally do not rely on out-of-bounds accesses to implement program logic. SMA modifies dynamic memory allocators and adds paddings to objects to form an enlarged object boundary. By dynamically correcting all the out-of-bounds accesses to operate on the enlarged protecting boundaries, SMA can tolerate out-of-bounds accesses. For the sake of compatibility, we chose tagged pointers to record the boundary metadata of a memory object in the pointer itself, and correct the address upon detecting out-of-bounds access. We have implemented the prototype of SMA on LLVM 10.0. Our results show that our compiler enables the programs to execute successfully through buffer overflow attacks. Experiments on MiBench show that our prototype incurs an overhead of 78\\%. Further optimizations would require ISA supports.",
      "paperUrl": "https://arxiv.org/pdf/2002.02831",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2002.02831",
      "tags": [
        "Optimizations",
        "Security"
      ],
      "matchedAuthors": [
        "Xu Cheng"
      ]
    },
    {
      "id": "openalex-w3046121809",
      "source": "openalex-discovery",
      "title": "SPAM: Stateless Permutation of Application Memory",
      "authors": [
        {
          "name": "Mohamed Tarek Ibn Ziad",
          "affiliation": "Columbia University"
        },
        {
          "name": "Miguel A. Arroyo",
          "affiliation": "Columbia University"
        },
        {
          "name": "Simha Sethumadhavan",
          "affiliation": "Columbia University"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we propose the Stateless Permutation of Application Memory (SPAM), a software defense that enables fine-grained data permutation for C programs. The key benefits include resilience against attacks that directly exploit software errors (i.e., spatial and temporal memory safety violations) in addition to attacks that exploit hardware vulnerabilities such as ColdBoot, RowHammer or hardware side-channels to disclose or corrupt memory using a single cohesive technique. Unlike prior work, SPAM is stateless by design making it automatically applicable to multi-threaded applications. We implement SPAM as an LLVM compiler pass with an extension to the compiler-rt runtime. We evaluate it on the C subset of the SPEC2017 benchmark suite and three real-world applications: the Nginx web server, the Duktape Javascript interpreter, and the WolfSSL cryptographic library. We further show SPAM's scalability by running a multi-threaded benchmark suite. SPAM has greater security coverage and comparable performance overheads to state-of-the-art software techniques for memory safety on contemporary x86_64 processors. Our security evaluation confirms SPAM's effectiveness in preventing intra/inter spatial/temporal memory violations by making the attacker success chances as low as 1/16!.",
      "paperUrl": "https://arxiv.org/pdf/2007.13808",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2007.13808",
      "tags": [
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Simha Sethumadhavan"
      ]
    },
    {
      "id": "openalex-w3119678735",
      "source": "openalex-discovery",
      "title": "Robust Practical Binary Optimization at Run-time using LLVM",
      "authors": [
        {
          "name": "Alexis Engelke",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Martin Schulz",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In High Performance Computing (HPC) the performance of applications is paramount, which is has led to a wide body of work on optimizing compilers. However, compilers alone are naturally limited in their potential, as they cannot exploit run-time information to specialize generated code. For this reason, run-time components to perform such specializations are gaining traction. In particular, application or use-case driven specializations have a high potential for optimizations, as they can benefit from explicit guidance from the application or the user, which allows for targeted changes incorporating specific knowledge. Because full compilers and sources are usually unavailable during computation, specializations have to be done at machine code level. However, existing libraries for binary specialization face structural limitations of separating optimization and code generation, in addition to numerous implementation gaps. In this paper, we describe BinOpt, a novel and robust library for performing application-driven binary optimization and specialization using LLVM. A machine code function is lifted directly to LLVM-IR, optimized in LLVM while making use of application-specified information, used to generate a new specialization for a function, and integrated back to the original code. We apply this technique to existing optimized code and show that significant performance improvements can be observed with only a small optimization time overhead.",
      "paperUrl": "https://doi.org/10.1109/llvmhpchipar51896.2020.00011",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Libraries",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexis Engelke",
        "Martin Schulz"
      ]
    },
    {
      "id": "openalex-w3015291177",
      "source": "openalex-discovery",
      "title": "RetroWrite: Statically Instrumenting COTS Binaries for Fuzzing and Sanitization",
      "authors": [
        {
          "name": "S. Dinesh",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Nathan Burow",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Dongyan Xu",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Mathias Payer",
          "affiliation": "Purdue University West Lafayette"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "End users of closed-source software currently cannot easily analyze the securityof programs or patch them if ﬂaws are found. Notably, end users can include developers who use third party libraries. The current state of the art for coverage-guidedbinary fuzzing or binary sanitization is dynamic binary translation, which resultsin prohibitive overhead. Existing static rewriting techniques cannot fully recoversymbolization information, and so have diﬃculty modifying binaries to track codecoverage for fuzzing or add security checks for sanitizers.The ideal solution for adding instrumentation is a static rewriter that can intelligently add in the required instrumentation as if it were inserted at compile time.This requires analysis to statically disambiguate between references and scalars, aproblem known to be undecidable in the general case. We show that recovering thisinformation is possible in practice for the most common class of software and libraries: 64 bit, position independent code. Based on our observation, we design abinary-rewriting instrumentation to support American Fuzzy Lop (AFL) and AddressSanitizer (ASan), and show that we achieve compiler levels of performance, while retaining precision. Binaries rewritten for coverage-guided fuzzing using RetroWriteare identical in performance to compiler-instrumented binaries and outperforms thedefault QEMU-based instrumentation by 7.5x while triggering more bugs. Our implementation of binary-only Address Sanitizer is 3x faster than Valgrind memcheck,the state-of-the-art binary-only memory checker, and detects 80% more bugs in oursecurity evaluation.",
      "paperUrl": "https://ieeexplore.ieee.org/ielx7/9144328/9152199/09152762.pdf",
      "sourceUrl": "https://doi.org/10.1109/sp40000.2020.00009",
      "tags": [
        "Libraries",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w3008200134",
      "source": "openalex-discovery",
      "title": "Relaxing the one definition rule in interpreted C++",
      "authors": [
        {
          "name": "Javier López-Gómez",
          "affiliation": "Universidad Carlos III de Madrid"
        },
        {
          "name": "Javier Fernández",
          "affiliation": "Universidad Carlos III de Madrid"
        },
        {
          "name": "David del Rio Astorga",
          "affiliation": "Universidad Carlos III de Madrid"
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": "Princeton University"
        },
        {
          "name": "N. A. Naumann",
          "affiliation": "European Organization for Nuclear Research"
        },
        {
          "name": "J. Daniel García",
          "affiliation": "Universidad Carlos III de Madrid"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Most implementations of the C++ programming language\\ngenerate binary executable code. However, interpreted execution of C++ sources has its own use cases as the Cling\\ninterpreter from CERN’s ROOT project has shown. Some\\nlimitations are derived from the ODR (One Definition Rule)\\nthat rules out multiple definitions of entities within a single translation unit (TU). ODR is there to ensure uniform\\nview of a given C++ entity across translation units. Ensuring uniform view of C++ entities helps when producing ABI\\ncompatible binaries. Interpreting C++ presumes a single evergrowing translation unit that define away some of the ODR\\nuse-cases. Therefore, it may well be desirable to relax the\\nODR and, consequently, to support the ability of developers\\nto override any existing definition for a given declaration.\\nThis approach is especially well-suited for iterative prototyping. In this paper, we extend Cling, a Clang/LLVM-based\\nC++ interpreter, to enable redefinitions of C++ entities at\\nthe prompt. To achieve this, top-level declarations are nested\\ninto inline namespaces and the translation unit lookup table\\nis adjusted to invalidate previous definitions that would otherwise result in ambiguities. Formally, this technique refactors the code to an equivalent that does not violate the ODR,\\nas each definition is nested in a different namespace. Furthermore, any previous definition that has been shadowed\\nis still accessible by means of its fully-qualified name. A prototype implementation of the presented technique has been integrated into the Cling C++ interpreter, showing that our\\ntechnique is feasible and usable.",
      "paperUrl": "https://doi.org/10.1145/3377555.3377901",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Javier López-Gómez",
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w3092898347",
      "source": "openalex-discovery",
      "title": "Really Embedding Domain-Specific Languages into C++",
      "authors": [
        {
          "name": "Hal Finkel",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Alexander McCaskey",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Tobi Popoola",
          "affiliation": "Boise State University"
        },
        {
          "name": "Dmitry I. Lyakh",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Argonne National Laboratory"
        }
      ],
      "year": "2020",
      "venue": "Vol. 5 (Issue None)",
      "type": "research-paper",
      "abstract": "Domain-specific languages (DSLs) are both pervasive and powerful, but remain difficult to integrate into large projects. As a result, while DSLs can bring distinct advantages in performance, reliability, and maintainability, their use often involves trading off other good software-engineering practices. In this paper, we describe an extension to the Clang C++ compiler to support syntax plugins, and we demonstrate how this mechanism allows making use of DSLs inside of a C++ code base without needing to separate the DSL source code from the surrounding C++ code.",
      "paperUrl": "https://arxiv.org/pdf/2010.08439",
      "sourceUrl": "https://doi.org/10.1109/llvmhpchipar51896.2020.00012",
      "tags": [
        "C++",
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Hal Finkel",
        "Johannes Doerfert"
      ]
    },
    {
      "id": "openalex-w3107362213",
      "source": "openalex-discovery",
      "title": "Random testing for C and C++ compilers with YARPGen",
      "authors": [
        {
          "name": "Vsevolod Livinskii",
          "affiliation": "University of Utah"
        },
        {
          "name": "Dmitry Babokin",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 4 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel® C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20% for LLVM and 40% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3428264",
      "sourceUrl": "https://doi.org/10.1145/3428264",
      "tags": [
        "C++",
        "Optimizations",
        "Testing"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Vsevolod Livinskii"
      ]
    },
    {
      "id": "openalex-w4362642506",
      "source": "openalex-discovery",
      "title": "ROPfuscator: Robust Obfuscation with ROP",
      "authors": [
        {
          "name": "Giulio De Pasquale",
          "affiliation": ""
        },
        {
          "name": "Fukutomo Nakanishi",
          "affiliation": ""
        },
        {
          "name": "Daniele Ferla",
          "affiliation": ""
        },
        {
          "name": "Lorenzo Cavallaro",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Software obfuscation plays a crucial role in protecting intellectual property in software from reverse engineering attempts. While some obfuscation techniques originate from the obfuscation-reverse engineering arms race, others stem from different research areas, such as binary software exploitation. Return-oriented programming (ROP) gained popularity as one of the most effective exploitation techniques for memory error vulnerabilities. ROP interferes with our natural perception of a process control flow, inspiring us to repurpose ROP as a robust and effective form of software obfuscation. Although previous work already explores ROP's effectiveness as an obfuscation technique, evolving reverse engineering research raises the need for principled reasoning to understand the strengths and limitations of ROP-based mechanisms against man-at-the-end (MATE) attacks. To this end, we present ROPfuscator, a compiler-driven obfuscation pass based on ROP for any programming language supported by LLVM. We incorporate opaque predicates and constants and a novel instruction hiding technique to withstand sophisticated MATE attacks. More importantly, we introduce a realistic and unified threat model to thoroughly evaluate ROPfuscator and provide principled reasoning on ROP-based obfuscation techniques that answer to code coverage, incurred overhead, correctness, robustness, and practicality challenges.",
      "paperUrl": "https://arxiv.org/pdf/2012.09163",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2012.09163",
      "tags": [],
      "matchedAuthors": [
        "Lorenzo Cavallaro"
      ]
    },
    {
      "id": "openalex-w3093179495",
      "source": "openalex-discovery",
      "title": "Probabilistic Programming with CuPPL",
      "authors": [
        {
          "name": "Alexander J. Collins",
          "affiliation": "Nvidia (United Kingdom)"
        },
        {
          "name": "Vinod Grover",
          "affiliation": "Nvidia (United Kingdom)"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Probabilistic Programming Languages (PPLs) are a powerful tool in machine learning, allowing highly expressive generative models to be expressed succinctly. They couple complex inference algorithms, implemented by the language, with an expressive modelling language that allows a user to implement any computable function as the generative model. Such languages are usually implemented on top of existing high level programming languages and do not make use of hardware accelerators. PPLs that do make use of accelerators exist, but restrict the expressivity of the language in order to do so. In this paper, we present a language and toolchain that generates highly efficient code for both CPUs and GPUs. The language is functional in style, and the tool chain is built on top of LLVM. Our implementation uses de-limited continuations on CPU to perform inference, and custom CUDA codes on GPU. We obtain significant speed ups across a suite of PPL workloads, compared to other state of the art approaches on CPU. Furthermore, our compiler can also generate efficient code that runs on CUDA GPUs.",
      "paperUrl": "https://arxiv.org/pdf/2010.08454",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2010.08454",
      "tags": [
        "CUDA",
        "GPU",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Vinod Grover"
      ]
    },
    {
      "id": "openalex-w3013302896",
      "source": "openalex-discovery",
      "title": "ProGraML: Graph-based Deep Learning for Program Optimization and Analysis",
      "authors": [
        {
          "name": "Chris Cummins",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Zacharias V. Fisches",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Tal Ben‐Nun",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Torsten Hoefler",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Hugh Leather",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. We introduce ProGraML - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The ProGraML representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. ProGraML provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k LLVM-IR files covering six source programming languages, ProGraML achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.",
      "paperUrl": "https://doi.org/10.48550/arxiv.2003.10536",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Optimizations",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w3044607730",
      "source": "openalex-discovery",
      "title": "PredCom: A Predictive Approach to Collecting Approximated Communication Traces",
      "authors": [
        {
          "name": "Shinobu Miwa",
          "affiliation": "University of Electro-Communications"
        },
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "Martin Schulz",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Parallel and Distributed Systems | Vol. 32 (Issue 1)",
      "type": "research-paper",
      "abstract": "Communication traces collected from MPI applications are an important source of information for performance optimization as they can help analysts determine communication patterns and identify inefficiencies. However, their collection, especially at scale, is time consuming, since it usually requires running the complete target application on a large number of nodes. In this work, we present PredCom, a tool-chain to generate a predictive communication proxy based on information gathered from a few small scale runs, which allows us to extract approximate communication traces with an accuracy high enough for most analysis goals. For this, we combine LLVM passes on the original source code (to capture static program structure) with parameter prediction (to capture dynamic and scaling behavior). This approach drastically reduces the time needed for collecting the communication traces, even for traces on large numbers of MPI processes. Here, we demonstrate that PredCom generates communication traces of various applications up to 1612x faster with an accuracy loss of 0.11 on average compared to the original large-scale traces, and we show that the generated traces can be used to optimize process placement.",
      "paperUrl": "https://www.osti.gov/biblio/1769152",
      "sourceUrl": "https://doi.org/10.1109/tpds.2020.3011121",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Ignacio Laguna",
        "Martin Schulz"
      ]
    },
    {
      "id": "openalex-w3014155037",
      "source": "openalex-discovery",
      "title": "Potential of LLVM for SX-Aurora",
      "authors": [
        {
          "name": "Simon Moll",
          "affiliation": "Saarland University"
        },
        {
          "name": "Matthias Kurtenacker",
          "affiliation": "Saarland University"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-030-39181-2_10",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Matthias Kurtenacker",
        "Sebastian Hack",
        "Simon Moll"
      ]
    },
    {
      "id": "openalex-w3090813146",
      "source": "openalex-discovery",
      "title": "Parallelizing Parallel Programs",
      "authors": [
        {
          "name": "Roberto Castañeda Lozano",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Murray Cole",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Björn Franke",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "<i>Parallelization</i> traditionally refers to the challenge of analyzing legacy sequential code, to generate equivalent, higher performing parallel code. Emerging mainstream parallelism has created a new challenge: <i>legacy parallel code</i>, typically hand-optimized for a particular system, becomes outdated as the hardware evolves. Modernizing such code line-by-line is not effective: instead we need to understand the program’s overall algorithmic intent, to re-express it for the new target. Re-expression can be simplified and futureproofed by coding with parallel patterns [4, 8, 9]. We address the remaining challenge of identifying the algorithm patterns in the original parallel source. We describe a novel dynamic approach to the identification of implicit algorithmic patterns. Our core principle is that the essence of a pattern is to be found in the dynamic data flows it invokes between operations and their repetition, rather than any specific source level encoding. This makes it neutral with respect to source code and hence immediately applicable to both legacy sequential and parallel code.<br/>The high level structure of our approach is captured in Figure 1. Our LLVM pass instruments the legacy program, whose traced execution generates a <i>dynamic dataflow graph (DDG)</i>. Our constraint-programming based pattern finding tool analyzes the DDG against a library of pattern definitions, reporting found instances back to the programmer.",
      "paperUrl": "https://www.research.ed.ac.uk/en/publications/34650916-bb7f-43a7-95d0-cf1620ee0ec9",
      "sourceUrl": "https://doi.org/10.1145/3410463.3414663",
      "tags": [],
      "matchedAuthors": [
        "Roberto Castañeda Lozano"
      ]
    },
    {
      "id": "openalex-w3092253422",
      "source": "openalex-discovery",
      "title": "PET-to-MLIR: A polyhedral front-end for MLIR",
      "authors": [
        {
          "name": "Konrad Komisarczyk",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Lorenzo Chelini",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Kanishkan Vadivel",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Roel Jordans",
          "affiliation": "Eindhoven University of Technology"
        },
        {
          "name": "Henk Corporaal",
          "affiliation": "Eindhoven University of Technology"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present PET-to-MLIR, a new tool to enter the MLIR compiler framework from C source. The tool is based on the popular PET and ISL libraries for extracting and manipulating quasi-affine sets and relations, and Loop Tactics, a declarative optimizer. The use of PET brings advanced diagnosis and full support for C by relying on the Clang parser. ISL allows easy manipulation of the polyhedral representation and efficient code generation. Loop Tactics, on the other hand, enable us to detect computational motifs transparently and lift the entry point in MLIR, thus enabling domain-specific optimizations in general-purpose code. We demonstrate our tool using the Polybench/C benchmark suite and show that it can lower most of the benchmarks to the MLIR’s affine dialect successfully. We believe that our tool can benefit research in the compiler community by providing an automatic way to translate C code to the MLIR affine dialect.",
      "paperUrl": "https://research.tue.nl/nl/publications/32992200-7156-4192-98a4-af949c216f28",
      "sourceUrl": "https://doi.org/10.1109/dsd51259.2020.00091",
      "tags": [
        "Clang",
        "Libraries",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Lorenzo Chelini"
      ]
    },
    {
      "id": "openalex-w3007718266",
      "source": "openalex-discovery",
      "title": "Optimizing occupancy and ILP on the GPU using a combinatorial approach",
      "authors": [
        {
          "name": "Ghassan Shobaki",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Austin Kerbow",
          "affiliation": "Advanced Micro Devices (United States)"
        },
        {
          "name": "Stanislav Mekhanoshin",
          "affiliation": "Advanced Micro Devices (United States)"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper presents the first general solution to the problem of optimizing both occupancy and Instruction-Level Parallelism (ILP) when compiling for a Graphics Processing Unit (GPU). Exploiting ILP (minimizing schedule length) requires using more registers, but using more registers decreases occupancy (the number of thread groups that can be run in parallel). The problem of balancing these two conflicting objectives to achieve the best overall performance is a challenging open problem in code optimization. In this paper, we present a two-pass Branch-and-Bound (B&B) algorithm for solving this problem by treating occupancy as a primary objective and ILP as a secondary objective. In the first pass, the algorithm searches for a maximum-occupancy schedule, while in the second pass it iteratively searches for the shortest schedule that gives the maximum occupancy found in the first pass. The proposed scheduling algorithm was implemented in the LLVM compiler and applied to an AMD GPU. The algorithm's performance was evaluated using benchmarks from the PlaidML machine learning framework relative to LLVM's scheduling algorithm, AMD's production scheduling algorithm and an existing B&B scheduling algorithm that uses a different approach. The results show that the proposed B&B scheduling algorithm speeds up almost every benchmark by up to 35% relative to LLVM's scheduler, up to 31% relative to AMD's scheduler and up to 18% relative to the existing B&B scheduler. The geometric-mean improvements are 16.3% relative to LLVM's scheduler, 5.5% relative to AMD's production scheduler and 6.2% relative to the existing B&B scheduler. If more compile time can be tolerated, a geometric-mean improvement of 6.3% relative to AMD's scheduler can be achieved.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3368826.3377918",
      "sourceUrl": "https://doi.org/10.1145/3368826.3377918",
      "tags": [
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Ghassan Shobaki"
      ]
    },
    {
      "id": "openalex-w3126561079",
      "source": "openalex-discovery",
      "title": "OpenACC Profiling Support for Clang and LLVM using Clacc and TAU",
      "authors": [
        {
          "name": "Camille Coti",
          "affiliation": "University of Oregon"
        },
        {
          "name": "Joel Denny",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Kevin Huck",
          "affiliation": "University of Oregon"
        },
        {
          "name": "Seyong Lee",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Allen D. Malony",
          "affiliation": "University of Oregon"
        },
        {
          "name": "Sameer Shende",
          "affiliation": "University of Oregon"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Since its launch in 2010, OpenACC has evolved into one of the most widely used portable programming models for accelerators on HPC systems today. Clacc is a project funded by the US Exascale Computing Project (ECP) to bring OpenACC support for C and C++ to the popular Clang and LLVM compiler infrastructure. In this paper, we describe Clacc's support for the OpenACC Profiling Interface, a critical component of the OpenACC specification that standardizes an interface that profiling tools and libraries can depend upon across OpenACC implementations. As part of Clacc's general strategy to build OpenACC support upon OpenMP, we describe how Clacc builds OpenACC Profiling Interface support upon an extended version of OMPT. We then describe how a major profiling and tracing toolkit within ECP, the TAU Performance System, takes advantage of this support. We also describe TAU's selective instrumentation support for OpenACC. Finally, using Clacc and TAU, we present example visualizations for several SPEC ACCEL OpenACC benchmarks running on an IBM AC922 node, and we show that the associated performance overhead is negligible.",
      "paperUrl": "https://doi.org/10.1109/hustprotools51951.2020.00012",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "Libraries",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w3110753053",
      "source": "openalex-discovery",
      "title": "On Symbolic Execution of Decompiled Programs",
      "authors": [
        {
          "name": "Lukáš Korenčik",
          "affiliation": "Masaryk University"
        },
        {
          "name": "Petr Ročkai",
          "affiliation": "Masaryk University"
        },
        {
          "name": "Henrich Lauko",
          "affiliation": "Masaryk University"
        },
        {
          "name": "Jǐŕı Barnat",
          "affiliation": "Masaryk University"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we present a combination of existing and new tools that together make it possible to apply formal verification methods to programs in the form of ×86_64 machine code. Our approach first uses a decompilation tool (remill) to extract low-level intermediate representation (LLVM) from the machine code. This step consists of instruction translation (i.e. recovery of operation semantics), control flow extraction and address identification.The main contribution of this paper is the second step, which builds on data flow analysis and refinement of indirect (i.e. data-dependent) control flow. This step makes the processed bitcode much more amenable to formal analysis.To demonstrate the viability of our approach, we have compiled a set of benchmark programs into native executables and analysed them using two LLVM-based tools: DIVINE, a software model checker and KLEE, a symbolic execution engine. We have compared the outcomes to direct analysis of the same programs.",
      "paperUrl": "https://doi.org/10.1109/qrs51102.2020.00044",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Henrich Lauko"
      ]
    },
    {
      "id": "openalex-w3009395978",
      "source": "openalex-discovery",
      "title": "Modeling the Invariance of Virtual Pointers in LLVM",
      "authors": [
        {
          "name": "Krzysztof Pszeniczny",
          "affiliation": ""
        },
        {
          "name": "Piotr Padlewski",
          "affiliation": ""
        },
        {
          "name": "Richard Gregory Smith",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Devirtualization is a compiler optimization that replaces indirect (virtual) function calls with direct calls. It is particularly effective in object-oriented languages, such as Java or C++, in which virtual methods are typically abundant. We present a novel abstract model to express the lifetimes of C++ dynamic objects and invariance of virtual table pointers in the LLVM intermediate representation. The model and the corresponding implementation in Clang and LLVM enable full devirtualization of virtual calls whenever the dynamic type is statically known and elimination of redundant virtual table loads in other cases. Due to the complexity of C++, this has not been achieved by any other C++ compiler so far. Although our model was designed for C++, it is also applicable to other languages that use virtual dispatch. Our benchmarks show an average of 0.8% performance improvement on real-world C++ programs, with more than 30% speedup in some cases. The implementation is already a part of the upstream LLVM/Clang and can be enabled with the -fstrict-vtable-pointers flag.",
      "paperUrl": "https://arxiv.org/pdf/2003.04228",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2003.04228",
      "tags": [
        "C++",
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Krzysztof Pszeniczny",
        "Piotr Padlewski"
      ]
    },
    {
      "id": "openalex-w3007888060",
      "source": "openalex-discovery",
      "title": "Mixed-data-model heterogeneous compilation and OpenMP offloading",
      "authors": [
        {
          "name": "Andreas Kurth",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Koen Wolters",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Björn Forsberg",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Alessandro Capotondi",
          "affiliation": "University of Modena and Reggio Emilia"
        },
        {
          "name": "Andrea Marongiu",
          "affiliation": "University of Modena and Reggio Emilia"
        },
        {
          "name": "Tobias Grosser",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Luca Benini",
          "affiliation": "ETH Zurich"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Heterogeneous computers combine a general-purpose host processor with domain-specific programmable many-core accelerators, uniting high versatility with high performance and energy efficiency. While the host manages ever-more application memory, accelerators are designed to work mainly on their local memory. This difference in addressed memory leads to a discrepancy between the optimal address width of the host and the accelerator. Today 64-bit host processors are commonplace, but few accelerators exceed 32-bit addressable local memory, a difference expected to increase with 128-bit hosts in the exascale era. Managing this discrepancy requires support for multiple data models in heterogeneous compilers. So far, compiler support for multiple data models has not been explored, which hampers the programmability of such systems and inhibits their adoption. In this work, we perform the first exploration of the feasibility and performance of implementing a mixed-data-mode heterogeneous system. To support this, we present and evaluate the first mixed-data-model compiler, supporting arbitrary address widths on host and accelerator. To hide the inherent complexity and to enable high programmer productivity, we implement transparent offloading on top of OpenMP. The proposed compiler techniques are implemented in LLVM and evaluated on a 64+32-bit heterogeneous SoC. Results on benchmarks from the PolyBench-ACC suite show that memory can be transparently shared between host and accelerator at overheads below 0.7 % compared to 32-bit-only execution, enabling mixed-data-model computers to execute at near-native performance.",
      "paperUrl": "http://hdl.handle.net/11585/766846",
      "sourceUrl": "https://doi.org/10.1145/3377555.3377891",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w3007772124",
      "source": "openalex-discovery",
      "title": "MLIR: A Compiler Infrastructure for the End of Moore's Law",
      "authors": [
        {
          "name": "Chris Lattner",
          "affiliation": ""
        },
        {
          "name": "Jacques A. Pienaar",
          "affiliation": ""
        },
        {
          "name": "Mehdi Amini",
          "affiliation": ""
        },
        {
          "name": "Uday Bondhugula",
          "affiliation": ""
        },
        {
          "name": "River Riddle",
          "affiliation": ""
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        },
        {
          "name": "Tatiana Shpeisman",
          "affiliation": ""
        },
        {
          "name": "Andy Davis",
          "affiliation": ""
        },
        {
          "name": "Nicolas Vasilache",
          "affiliation": ""
        },
        {
          "name": "Oleksandr Zinenko",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This work presents MLIR, a novel approach to building reusable and extensible compiler infrastructure. MLIR aims to address software fragmentation, improve compilation for heterogeneous hardware, significantly reduce the cost of building domain specific compilers, and aid in connecting existing compilers together. MLIR facilitates the design and implementation of code generators, translators and optimizers at different levels of abstraction and also across application domains, hardware targets and execution environments. The contribution of this work includes (1) discussion of MLIR as a research artifact, built for extension and evolution, and identifying the challenges and opportunities posed by this novel design point in design, semantics, optimization specification, system, and engineering. (2) evaluation of MLIR as a generalized infrastructure that reduces the cost of building compilers-describing diverse use-cases to show research and educational opportunities for future programming languages, compilers, execution environments, and computer architecture. The paper also presents the rationale for MLIR, its original design principles, structures and semantics.",
      "paperUrl": "https://arxiv.org/pdf/2002.11054",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2002.11054",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Albert Cohen",
        "Chris Lattner",
        "Mehdi Amini",
        "Oleksandr Zinenko",
        "River Riddle",
        "Tatiana Shpeisman"
      ]
    },
    {
      "id": "openalex-w2996951577",
      "source": "openalex-discovery",
      "title": "LLOV",
      "authors": [
        {
          "name": "Utpal Bora",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Santanu Das",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Pankaj Kukreja",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Saurabh Joshi",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Sanjay Rajopadhye",
          "affiliation": "Colorado State University"
        }
      ],
      "year": "2020",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 17 (Issue 4)",
      "type": "research-paper",
      "abstract": "In the era of Exascale computing, writing efficient parallel programs is indispensable, and, at the same time, writing sound parallel programs is very difficult. Specifying parallelism with frameworks such as OpenMP is relatively easy, but data races in these programs are an important source of bugs. In this article, we propose LLOV, a fast, lightweight, language agnostic, and static data race checker for OpenMP programs based on the LLVM compiler framework. We compare LLOV with other state-of-the-art data race checkers on a variety of well-established benchmarks. We show that the precision, accuracy, and the F1 score of LLOV is comparable to other checkers while being orders of magnitude faster. To the best of our knowledge, LLOV is the only tool among the state-of-the-art data race checkers that can verify a C/C++ or FORTRAN program to be data race free.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3418597",
      "sourceUrl": "https://doi.org/10.1145/3418597",
      "tags": [
        "C++"
      ],
      "matchedAuthors": [
        "Ramakrishna Upadrasta",
        "Sanjay Rajopadhye",
        "Santanu Das",
        "Saurabh Joshi",
        "Utpal Bora"
      ]
    },
    {
      "id": "openalex-w3092164060",
      "source": "openalex-discovery",
      "title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients",
      "authors": [
        {
          "name": "William S. Moses",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Valentin Churavy",
          "affiliation": "Massachusetts Institute of Technology"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. 33 (Issue None)",
      "type": "research-paper",
      "abstract": "Applying differentiable programming techniques and machine learning algorithms to foreign programs requires developers to either rewrite their code in a machine learning framework, or otherwise provide derivatives of the foreign code. This paper presents Enzyme, a high-performance automatic differentiation (AD) compiler plugin for the LLVM compiler framework capable of synthesizing gradients of statically analyzable programs expressed in the LLVM intermediate representation (IR). Enzyme synthesizes gradients for programs written in any language whose compiler targets LLVM IR including C, C++, Fortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD capabilities in these languages. Unlike traditional source-to-source and operator-overloading tools, Enzyme performs AD on optimized IR. On a machine-learning focused benchmark suite including Microsoft's ADBench, AD on optimized IR achieves a geometric mean speedup of 4.5x over AD on IR before optimization allowing Enzyme to achieve state-of-the-art performance. Packaging Enzyme for PyTorch and TensorFlow provides convenient access to gradients of foreign code with state-of-the art performance, enabling foreign code to be directly incorporated into existing machine learning workflows.",
      "paperUrl": "https://arxiv.org/pdf/2010.01709",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2010.01709",
      "tags": [
        "C++",
        "IR",
        "MLIR",
        "Performance",
        "Rust",
        "Swift"
      ],
      "matchedAuthors": [
        "Valentin Churavy",
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w3206899898",
      "source": "openalex-discovery",
      "title": "Incremental Compilation Support in Clang",
      "authors": [
        {
          "name": "Vassil Vassilev",
          "affiliation": "Princeton University"
        },
        {
          "name": "D. J. Lange",
          "affiliation": "Princeton University"
        }
      ],
      "year": "2020",
      "venue": "Zenodo (CERN European Organization for Nuclear Research) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Cling is a C++ interpreter built on top of clang and llvm. In a nutshell, it uses clang's incremental compilation facilities to process code chunk-by-chunk by assuming an ever-growing translation unit. Code is then lowered into llvm IR and run by the llvm jit. Cling has implemented language \"extensions\" such as execution statements on the global scope and error recovery. Cling is in the core of high-energy physics research -- it is heavily used during data analysis of exabytes of particle physics data coming from the Large Hadron Collider (LHC) and other particle physics experiments. We recently started a project aim at improving cling's sustainability and to make it a standalone tool. In this poster we would like to present one of the project's main directions -- move parts of cling upstream along with the clang and llvm features that enable them. Over the years we have slowly moved some patches upstream. However we still have around 100 patches in the clang fork. Most of them are in the context of extending the incremental compilation support for clang. The incremental compilation poses some challenges in the clang infrastructure. For example, we need to tune CodeGen to work with multiple llvm::Module instances, and finalize per each end-of-translation unit (we have multiple of them). Other changes include small adjustments in the FileManager's caching mechanism, and bug fixes in the SourceManager (code which can be reached mostly from within our setup). One conclusion we can draw from our research is that the clang infrastructure fits amazingly well to something which was not its main use case. The grand total of our diffs against clang-9 is: `62 files changed, 1294 insertions(+), 231 deletions(-)`. Cling is currently being upgraded from llvm-5 to llvm-9. A major weakness of cling's infrastructure is that it does not work with the clang Action infrastructure due to the lack of an IncrementalAction. We will present a possible way forward would be to implement a clang::IncrementalAction as a starting point.",
      "paperUrl": "https://doi.org/10.5281/zenodo.4134135",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "IR",
        "JIT"
      ],
      "matchedAuthors": [
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w3095568965",
      "source": "openalex-discovery",
      "title": "Improved Loop Execution Modeling in the Clang Static Analyzer",
      "authors": [
        {
          "name": "Péter Szécsi",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Gábor Horváth",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2020",
      "venue": "Acta Cybernetica | Vol. 25 (Issue 4)",
      "type": "research-paper",
      "abstract": "The LLVM Clang Static Analyzer is a source code analysis tool which aims to find bugs in C, C++, and Objective-C programs using symbolic execution, i.e. it simulates the possible execution paths of the code. Currently the simulation of the loops is somewhat naive (but efficient), unrolling the loops a predefined constant number of times. However, this approach can result in a loss of coverage in various cases. This study aims to introduce two alternative approaches which can extend the current method and can be applied simultaneously: (1) determining loops worth to fully unroll with applied heuristics, and (2) using a widening mechanism to simulate an arbitrary number of iteration steps. These methods were evaluated on numerous open source projects, and proved to increase coverage in most of the cases. This work also laid the infrastructure for future loop modeling improvements.",
      "paperUrl": "https://cyber.bibl.u-szeged.hu/index.php/actcybern/article/download/4104/4011",
      "sourceUrl": "https://doi.org/10.14232/actacyb.283176",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w3030873103",
      "source": "openalex-discovery",
      "title": "IDD – A Platform Enabling Differential Debugging",
      "authors": [
        {
          "name": "Martin Vassilev",
          "affiliation": "Plovdiv University"
        },
        {
          "name": "Vassil Vassilev",
          "affiliation": "Princeton University"
        },
        {
          "name": "Alexander Penev",
          "affiliation": "Plovdiv University"
        }
      ],
      "year": "2020",
      "venue": "Cybernetics and Information Technologies | Vol. 20 (Issue 1)",
      "type": "research-paper",
      "abstract": "Abstract Debugging is a very time consuming task which is not well supported by existing tools. The existing methods do not provide tools enabling optimal developers’ productivity when debugging regressions in complex systems. In this paper we describe a possible solution aiding differential debugging. The differential debugging technique performs analysis of the regressed system and identifying the cause of the unexpected behavior by comparing to a previous version of the same system. The prototype, idd, inspects two versions of the executable – a baseline and a regressed version. The interactive debugging session runs side by side both executables and allows to examine and to compare various internal states. The architecture can work with multiple information sources comparing data from different tools. We also show how idd can detect performance regressions using information from third-party performance facilities. We illustrate how in practice we can quickly discover regressions in large systems such as the clang compiler.",
      "paperUrl": "https://doi.org/10.2478/cait-2020-0004",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Martin Vassilev",
        "Vassil Vassilev"
      ]
    },
    {
      "id": "openalex-w3101863737",
      "source": "openalex-discovery",
      "title": "Guided linking: dynamic linking without the costs",
      "authors": [
        {
          "name": "Sean Bartell",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Will Dietz",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Vikram Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 4 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Dynamic linking is extremely common in modern software systems, thanks to the flexibility and space savings it offers. However, this flexibility comes at a cost: it’s impossible to perform interprocedural optimizations that involve calls to a dynamic library. The basic problem is that the run-time behavior of the dynamic linker can’t be predicted at compile time, so the compiler can make no assumptions about how such calls will behave. This paper introduces guided linking , a technique for optimizing dynamically linked software when some information about the dynamic linker’s behavior is known in advance. The developer provides an arbitrary set of programs, libraries, and plugins to our tool, along with constraints that limit the possible dynamic linking behavior of the software. By taking advantage of the constraints, our tool enables any existing optimization to be applied across dynamic linking boundaries. For example, the NoOverride constraint can be applied to a function when the developer knows it will never be overridden with a different definition at run time; guided linking then enables the function to be inlined into its callers in other libraries. We also introduce a novel code size optimization that deduplicates identical functions even across different parts of the software set. By applying guided linking to the Python interpreter and its dynamically loaded modules, supplying the constraint that no other programs or modules will be used, we increase speed by an average of 9%. By applying guided linking to a dynamically linked distribution of Clang and LLVM, and using the constraint that no other software will use the LLVM libraries, we can increase speed by 5% and reduce file size by 13%. If we relax the constraint to allow other software to use the LLVM libraries, we can still increase speed by 5% and reduce file size by 5%. If we use guided linking to combine 11 different versions of the Boost library, using minimal constraints, we can reduce the total library size by 57%.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3428213",
      "sourceUrl": "https://doi.org/10.1145/3428213",
      "tags": [
        "Clang",
        "Libraries",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Vikram Adve",
        "Will Dietz"
      ]
    },
    {
      "id": "openalex-w3118461745",
      "source": "openalex-discovery",
      "title": "Fuzzing C++ class interfaces for generating and running tests with libFuzzer",
      "authors": [
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Summary form only given, as follows. The complete presentation was not made available for publication as part of the conference proceedings. \"Fuzzing C++ class interfaces for generating and running tests with libFuzzer\" presents how fuzzing can be used to automatically generate and run test cases based only on the class' interface. Driven by the coverage guidance of libFuzzer, we can generate more and more complex combinations of interface calls. Like other fuzzer solutions this automatic tester can run for arbitrary time to discover edge cases the programmer ignored. The close to minimal subset of discovered test cases providing the maximum coverage can be stored as a \"regression test\" to re-run later. The implemented prototype performed well both on artificial test cases and real-world C++ containers. The prototype is available at: https://gitlab.com/wilzegers/autotest.",
      "paperUrl": "https://doi.org/10.1109/issrew51248.2020.00027",
      "sourceUrl": "",
      "tags": [
        "C++"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4242169025",
      "source": "openalex-discovery",
      "title": "Fault injection at the instruction set architecture (ISA) level",
      "authors": [
        {
          "name": "Karthik Pattabiraman",
          "affiliation": "University of British Columbia"
        },
        {
          "name": "Guanpeng Li",
          "affiliation": "University of British Columbia"
        }
      ],
      "year": "2020",
      "venue": "Institution of Engineering and Technology eBooks | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Fault Injection (FI) is a commonly used technique to evaluate the reliability of systems. As soft errors become more commonplace in computer systems, it is often necessary to involve the software in the overall system's resilience. Therefore, it is important to inject faults at the ISA level to emulate soft errors that are visible to the software, in order to test software resilience mechanisms. Consequently, there is a need to develop Instruction Set Architecture (ISA)-level FI tools and techniques. We start by outlining the goals of ISA-level FI, followed by the main metrics that can be measured by the same. We then present a survey of techniques in the literature that attempt to inject faults at the ISA-level and up in the system stack. Finally, we present an overview of LLFI and PINFI, two fault injectors developed inour research group, that allow programers to inject faults at the LLVM compiler's Intermediate Representation (IR) level and x86 assembly code level, respectively. We conclude with a survey of the open challenges in the area.",
      "paperUrl": "https://doi.org/10.1049/pbcs057e_ch9",
      "sourceUrl": "",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "Karthik Pattabiraman"
      ]
    },
    {
      "id": "openalex-w3121052301",
      "source": "openalex-discovery",
      "title": "Extending the LLVM/Clang Framework for OpenMP Metadirective Support",
      "authors": [
        {
          "name": "Alok Mishra",
          "affiliation": "Stony Brook University"
        },
        {
          "name": "Abid M. Malik",
          "affiliation": "Brookhaven National Laboratory"
        },
        {
          "name": "Barbara Chapman",
          "affiliation": "Brookhaven National Laboratory"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "OpenMP 5.0 introduces many new directives to meet the demand of emerging high performance computing systems. Among these new directives, the metadirective and declare variant directives are important to control the execution behavior of a given application by compile-time adaptation based on the OpenMP context. The metadirective directive allows the selection of OpenMP directives based on the enclosing OpenMP context as well as on user-defined conditions. The declare variant directive declares a specialized variant of a base function and specifies the context in which that specialized variant is used. Support for these directives are available in few compilers with some limitations. Support for metadirective is not available in Clang. This paper presents our implementation of the metadirective directive in Clang. In this paper, we present an implementation which supports the OpenMP 5.0 metadirective specification. However, in addition, this work also implements a dynamic extension to the user-specified conditions. A dynamic evaluation of user-defined conditions provides programmers more freedom to express a range of adaptive algorithms that improve overall performance of a given application. For example, a program can estimate the cost of execution, with respect to time taken or energy consumption, of a kernel based on some dynamic or static variables and decide whether or not to offload the kernel to GPU using the metadirective. Since there is a significant lack of knowledge about the usage and performance analysis of metadirective, the work also studies its impact on application characteristics. To achieve this, we have modified several benchmark codes in the Rodinia benchmark suite. The Rodinia benchmark includes applications and kernels which target multi-core CPU and GPU platforms which helps programmers study the emerging computing platforms. Our modification to the Rodinia benchmarks enables the application developer to study the behavior of metadirective. Our analysis reveal that the main advantage of the dynamic implementation of metadirective is that it adds minimal to no overhead to the user application, in addition to allowing flexibility to the programmers to introduce portability and adaptability to their code. Our modification of the Rodinia benchmark suite provides several guidelines for programmers to achieve better performance with metadirective.",
      "paperUrl": "https://doi.org/10.1109/llvmhpchipar51896.2020.00009",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Alok Mishra",
        "Barbara Chapman"
      ]
    },
    {
      "id": "openalex-w3092398737",
      "source": "openalex-discovery",
      "title": "Extending C++ for Heterogeneous Quantum-Classical Computing",
      "authors": [
        {
          "name": "Thien Nguyen",
          "affiliation": ""
        },
        {
          "name": "Anthony Santana",
          "affiliation": ""
        },
        {
          "name": "Tyler Kharazi",
          "affiliation": ""
        },
        {
          "name": "Daniel Claudino",
          "affiliation": ""
        },
        {
          "name": "Hal Finkel",
          "affiliation": ""
        },
        {
          "name": "Alexander McCaskey",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present qcor - a language extension to C++ and compiler implementation that enables heterogeneous quantum-classical programming, compilation, and execution in a single-source context. Our work provides a first-of-its-kind C++ compiler enabling high-level quantum kernel (function) expression in a quantum-language agnostic manner, as well as a hardware-agnostic, retargetable compiler workflow targeting a number of physical and virtual quantum computing backends. qcor leverages novel Clang plugin interfaces and builds upon the XACC system-level quantum programming framework to provide a state-of-the-art integration mechanism for quantum-classical compilation that leverages the best from the community at-large. qcor translates quantum kernels ultimately to the XACC intermediate representation, and provides user-extensible hooks for quantum compilation routines like circuit optimization, analysis, and placement. This work details the overall architecture and compiler workflow for qcor, and provides a number of illuminating programming examples demonstrating its utility for near-term variational tasks, quantum algorithm expression, and feed-forward error correction schemes.",
      "paperUrl": "https://arxiv.org/pdf/2010.03935",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2010.03935",
      "tags": [
        "Backend",
        "C++",
        "Clang",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Hal Finkel"
      ]
    },
    {
      "id": "openalex-w3114481667",
      "source": "openalex-discovery",
      "title": "Exploiting BSP Abstractions for Compiler Based Optimizations of GPU Applications on multi-GPU Systems",
      "authors": [
        {
          "name": "Alexander Matz",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "heiDOK (Heidelberg University) | Vol. None (Issue None)",
      "type": "thesis",
      "abstract": "Graphics Processing Units (GPUs) are accelerators for computers and provide massive amounts of computational power and bandwidth for amenable applications. While effectively utilizing an individual GPU already requires a high level of skill, effectively utilizing multiple GPUs introduces completely new types of challenges. This work sets out to investigate how the hierarchical execution model of GPUs can be exploited to simplify the utilization of such multi-GPU systems.&#13;\\nThe investigation starts with an analysis of the memory access patterns exhibited by applications from common GPU benchmark suites. Memory access patterns are collected using custom instrumentation and a simple simulation then analyzes the patterns and identifies implicit communication across the different levels of the execution hierarchy. The analysis reveals that for most GPU applications memory accesses are highly localized and there exists a way to partition the workload so that the communication volume grows slower than the aggregated bandwidth for growing numbers of GPUs.&#13;\\nNext, an application model based on Z-polyhedra is derived that formalizes the distribution of work across multiple GPUs and allows the identification of data dependencies. The model is then used to implement a prototype compiler that consumes single-GPU programs and produces executables that distribute GPU workloads across all available GPUs in a system. It uses static analysis to identify memory access patterns and polyhedral code generation in combination with a dynamic tracking system to efficiently resolve data dependencies. The prototype is implemented as an extension to the LLVM/Clang compiler and published in full source.&#13;\\nThe prototype compiler is then evaluated using a set of benchmark applications. While the prototype is limited in its applicability by technical issues, it provides impressive speedups of up to 12.4x on 16 GPUs for amenable applications. An in-depth analysis of the application runtime reveals that dependency resolution takes up less than 10% of the runtime, often significantly less.&#13;\\nA discussion follows and puts the work into context by presenting and differentiating related work, reflecting critically on the work itself and an outlook of the aspects that could be explored as part of this research. The work concludes with a summary and a closing opinion.",
      "paperUrl": "http://archiv.ub.uni-heidelberg.de/volltextserver/29213/1/00-dissertation.pdf",
      "sourceUrl": "https://doi.org/10.11588/heidok.00029213",
      "tags": [
        "Clang",
        "GPU",
        "Optimizations",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Alexander Matz"
      ]
    },
    {
      "id": "openalex-w3090046998",
      "source": "openalex-discovery",
      "title": "Experiments and optimizations for TVM on RISC-V Architectures with P Extension",
      "authors": [
        {
          "name": "Yi-Ru Chen",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Hui-Hsin Liao",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chia‐Hsuan Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Che-Chia Lin",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chao-Lin Lee",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Ming Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Chun‐Chieh Yang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "TVM is a AI compiler supports both graph-level and operator-level optimization on machine learning models. It provides an optimizable flow to deploy on diverse target devices. By exploiting TVM schedule, we can optimize the codegen behavior for our RISC-V architecture. Since RISCV is configurable with the selection on different extension, we can enable multiple extensions depends on the application scene. In our work, we present the flow for enabling and optimizing the RISC-V P extension toward QNN models from TVM. With support from LLVM and a customized deep learning runtime (DLR), we verified our work on both FLOAT32 and prequantized models from Tensorflow Lite. Experiments shows that comparing with FLOAT32 models, our work can achieve 2.7-7.0 times of performance improvement with regard to total instruction count at runtime for pre-quantized version with a set of benchmarks including Mobilenet and Inception-v3. As for accuracy issue, the degradation is tiny for quantization version among 500 images. All experiments are running on RISC-V simulator, Spike with P extension support.",
      "paperUrl": "https://doi.org/10.1109/vlsi-dat49148.2020.9196477",
      "sourceUrl": "",
      "tags": [
        "AI",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w3033154127",
      "source": "openalex-discovery",
      "title": "Effective function merging in the SSA form",
      "authors": [
        {
          "name": "Rodrigo C. O. Rocha",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Pavlos Petoumenos",
          "affiliation": "University of Manchester"
        },
        {
          "name": "Zheng Wang",
          "affiliation": "University of Leeds"
        },
        {
          "name": "Murray Cole",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Hugh Leather",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Function merging is an important optimization for reducing code size. This technique eliminates redundant code across functions by merging them into a single function. While initially limited to identical or trivially similar functions, the most recent approach can identify all merging opportunities in arbitrary pairs of functions. However, this approach has a serious limitation which prevents it from reaching its full potential. Because it cannot handle phi-nodes, the state-of-the-art applies register demotion to eliminate them before applying its core algorithm. While a superficially minor workaround, this has a three-fold negative effect: by artificially lengthening the instruction sequences to be aligned, it hinders the identification of mergeable instruction; it prevents a vast number of functions from being profitably merged; it increases compilation overheads, both in terms of compile-time and memory usage. We present SalSSA, a novel approach that fully supports the SSA form, removing any need for register demotion. By doing so, we notably increase the number of profitably merged functions. We implement SalSSA in LLVM and apply it to the SPEC 2006 and 2017 suites. Experimental results show that our approach delivers on average, 7.9% to 9.7% reduction on the final size of the compiled code. This translates to around 2x more code size reduction over the state-of-the-art. Moreover, as a result of aligning shorter sequences of instructions and reducing the number of wasteful merge operations, our new approach incurs an average compile-time overhead of only 5%, 3x less than the state-of-the-art, while also reducing memory usage by over 2x.",
      "paperUrl": "https://research.manchester.ac.uk/en/publications/90df1bdc-9903-420c-9ea3-5fdee8f48257",
      "sourceUrl": "https://doi.org/10.1145/3385412.3386030",
      "tags": [],
      "matchedAuthors": [
        "Hugh Leather",
        "Rodrigo C. O. Rocha",
        "Zheng Wang"
      ]
    },
    {
      "id": "openalex-w3091055609",
      "source": "openalex-discovery",
      "title": "Effective Loop Fusion in Polyhedral Compilation Using Fusion Conflict Graphs",
      "authors": [
        {
          "name": "Aravind Acharya",
          "affiliation": "Indian Institute of Science Bangalore"
        },
        {
          "name": "Uday Bondhugula",
          "affiliation": "Indian Institute of Science Bangalore"
        },
        {
          "name": "Albert Cohen",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 17 (Issue 4)",
      "type": "research-paper",
      "abstract": "Polyhedral auto-transformation frameworks are known to find efficient loop transformations that maximize locality and parallelism and minimize synchronization. While complex loop transformations are routinely modeled in these frameworks, they tend to rely on ad hoc heuristics for loop fusion. Although there exist multiple loop fusion models with cost functions to maximize locality and parallelism, these models involve separate optimization steps rather than seamlessly integrating with other loop transformations like loop permutation, scaling, and shifting. Incorporating parallelism-preserving loop fusion heuristics into existing affine transformation frameworks like Pluto, LLVM-Polly, PPCG, and PoCC requires solving a large number of Integer Linear Programming formulations, which increase auto-transformation times significantly. In this work, we incorporate polynomial time loop fusion heuristics into the Pluto-lp-dfp framework. We present a data structure called the fusion conflict graph (FCG), which enables us to efficiently model loop fusion in the presence of other affine loop transformations. We propose a clustering heuristic to group the vertices of the FCG, which further enables us to provide three different polynomial time greedy fusion heuristics, namely, maximal fusion , typed fusion , and hybrid fusion , while maintaining the compile time improvements of Pluto-lp-dfp over Pluto. Our experiments reveal that the hybrid fusion model, in conjunction with Pluto’s cost function, finds efficient transformations that outperform PoCC and Pluto by mean factors of 1.8× and 1.07×, respectively, with a maximum performance improvement of 14× over PoCC and 2.6× over Pluto.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3416510",
      "sourceUrl": "https://doi.org/10.1145/3416510",
      "tags": [
        "Loop transformations",
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Albert Cohen"
      ]
    },
    {
      "id": "openalex-w3047133959",
      "source": "openalex-discovery",
      "title": "Duplo: a framework for OCaml post-link optimisation",
      "authors": [
        {
          "name": "Nandor Licker",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Timothy M. Jones",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 4 (Issue ICFP)",
      "type": "research-paper",
      "abstract": "We present a novel framework, Duplo , for the low-level post-link optimisation of OCaml programs, achieving a speedup of 7% and a reduction of at least 15% of the code size of widely-used OCaml applications. Unlike existing post-link optimisers, which typically operate on target-specific machine code, our framework operates on a Low-Level Intermediate Representation (LLIR) capable of representing both the OCaml programs and any C dependencies they invoke through the foreign-function interface (FFI). LLIR is analysed, transformed and lowered to machine code by our post-link optimiser, LLIR-OPT. Most importantly, LLIR allows the optimiser to cross the OCaml-C language boundary, mitigating the overhead incurred by the FFI and enabling analyses and transformations in a previously unavailable context. The optimised IR is then lowered to amd64 machine code through the existing target-specific code generator of LLVM, modified to handle garbage collection just as effectively as the native OCaml backend. We equip our optimiser with a suite of SSA-based transformations and points-to analyses capable of capturing the semantics and representing the memory models of both languages, along with a cross-language inliner to embed C methods into OCaml callers. We evaluate the gains of our framework, which can be attributed to both our optimiser and the more sophisticated amd64 backend of LLVM, on a wide-range of widely-used OCaml applications, as well as an existing suite of micro- and macro-benchmarks used to track the performance of the OCaml compiler.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3408980",
      "sourceUrl": "https://doi.org/10.1145/3408980",
      "tags": [
        "Backend",
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Nandor Licker"
      ]
    },
    {
      "id": "openalex-w4287644574",
      "source": "openalex-discovery",
      "title": "DiffTune: Optimizing CPU Simulator Parameters with Learned\\n Differentiable Surrogates",
      "authors": [
        {
          "name": "Alex Renda",
          "affiliation": "Moscow Institute of Thermal Technology"
        },
        {
          "name": "Yishen Chen",
          "affiliation": "Moscow Institute of Thermal Technology"
        },
        {
          "name": "Charith Mendis",
          "affiliation": "Moscow Institute of Thermal Technology"
        },
        {
          "name": "Michael Carbin",
          "affiliation": "Moscow Institute of Thermal Technology"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "CPU simulators are useful tools for modeling CPU execution behavior. However,\\nthey suffer from inaccuracies due to the cost and complexity of setting their\\nfine-grained parameters, such as the latencies of individual instructions. This\\ncomplexity arises from the expertise required to design benchmarks and\\nmeasurement frameworks that can precisely measure the values of parameters at\\nsuch fine granularity. In some cases, these parameters do not necessarily have\\na physical realization and are therefore fundamentally approximate, or even\\nunmeasurable.\\n In this paper we present DiffTune, a system for learning the parameters of\\nx86 basic block CPU simulators from coarse-grained end-to-end measurements.\\nGiven a simulator, DiffTune learns its parameters by first replacing the\\noriginal simulator with a differentiable surrogate, another function that\\napproximates the original function; by making the surrogate differentiable,\\nDiffTune is then able to apply gradient-based optimization techniques even when\\nthe original function is non-differentiable, such as is the case with CPU\\nsimulators. With this differentiable surrogate, DiffTune then applies\\ngradient-based optimization to produce values of the simulator's parameters\\nthat minimize the simulator's error on a dataset of ground truth end-to-end\\nperformance measurements. Finally, the learned parameters are plugged back into\\nthe original simulator.\\n DiffTune is able to automatically learn the entire set of\\nmicroarchitecture-specific parameters within the Intel x86 simulation model of\\nllvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling\\nmodel. DiffTune's learned parameters lead llvm-mca to an average error that not\\nonly matches but lowers that of its original, expert-provided parameter values.\\n",
      "paperUrl": "https://arxiv.org/pdf/2010.04017",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2010.04017",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Yishen Chen"
      ]
    },
    {
      "id": "openalex-w3092461782",
      "source": "openalex-discovery",
      "title": "DiffTune: Optimizing CPU Simulator Parameters with Learned Differentiable Surrogates",
      "authors": [
        {
          "name": "Alex Renda",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Yishen Chen",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Charith Mendis",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Michael Carbin",
          "affiliation": "Massachusetts Institute of Technology"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "CPU simulators are useful tools for modeling CPU execution behavior. However, they suffer from inaccuracies due to the cost and complexity of setting their fine-grained parameters, such as the latencies of individual instructions. This complexity arises from the expertise required to design benchmarks and measurement frameworks that can precisely measure the values of parameters at such fine granularity. In some cases, these parameters do not necessarily have a physical realization and are therefore fundamentally approximate, or even unmeasurable. In this paper we present DiffTune, a system for learning the parameters of x86 basic block CPU simulators from coarse-grained end-to-end measurements. Given a simulator, DiffTune learns its parameters by first replacing the original simulator with a differentiable surrogate, another function that approximates the original function; by making the surrogate differentiable, DiffTune is then able to apply gradient-based optimization techniques even when the original function is non-differentiable, such as is the case with CPU simulators. With this differentiable surrogate, DiffTune then applies gradient-based optimization to produce values of the simulator's parameters that minimize the simulator's error on a dataset of ground truth end-to-end performance measurements. Finally, the learned parameters are plugged back into the original simulator. DiffTune is able to automatically learn the entire set of microarchitecture-specific parameters within the Intel x86 simulation model of llvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling model. DiffTune's learned parameters lead llvm-mca to an average error that not only matches but lowers that of its original, expert-provided parameter values.",
      "paperUrl": "https://arxiv.org/pdf/2010.04017",
      "sourceUrl": "https://doi.org/10.1109/micro50266.2020.00045",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Yishen Chen"
      ]
    },
    {
      "id": "openalex-w3107212594",
      "source": "openalex-discovery",
      "title": "Detecting Uninitialized Variables in C++ with the Clang Static Analyzer",
      "authors": [
        {
          "name": "Kristóf Umann",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2020",
      "venue": "Acta Cybernetica | Vol. 25 (Issue 4)",
      "type": "research-paper",
      "abstract": "Uninitialized variables have been a source of errors since the beginning of software engineering. Some programming languages (e.g. Java and Python) will automatically zero-initialize such variables, but others, like C and C++, leave their state undefined. While laying aside initialization in C and C++ might be a performance advantage if an initial value can't be supplied, working with such variables is an undefined behavior, and is a common source of instabilities and crashes. To avoid such errors, whenever meaningful initialization is possible, it should be used. Tools for detecting these errors run time have existed for decades, but those require the problematic code to be executed. Since in many cases the number of possible execution paths are combinatoric, static analysis techniques emerged as an alternative. In this paper, we overview the technique for detecting uninitialized C++ variables using the Clang Static Analyzer, and describe various heuristics to guess whether a specific variable was left in an undefined state intentionally. We implemented a prototype tool based on our idea and successfully tested it on large open source projects.",
      "paperUrl": "https://cyber.bibl.u-szeged.hu/index.php/actcybern/article/download/4100/4014",
      "sourceUrl": "https://doi.org/10.14232/actacyb.282900",
      "tags": [
        "C++",
        "Clang",
        "Performance",
        "Programming Languages",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Kristóóf Umann",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w4287824563",
      "source": "openalex-discovery",
      "title": "DeepDataFlow",
      "authors": [
        {
          "name": "Chris Cummins",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This dataset contains 493k LLVM-IRs taken from a wide range of projects and source programming languages, and includes labels for several compiler data analyses. We also include the logs for the machine learning jobs which produced our published experimental results. The uncompressed dataset uses the following layout: <code>labels/</code> Directory containing machine learning features and labels for programs for compiler data flow analyses. <code>labels/&lt;analysis&gt;/&lt;source&gt;.&lt;id&gt;.&lt;lang&gt;.ProgramFeaturesList.pb</code> A ProgramFeaturesList protocol buffer containing a list of features resulting from running a data flow analysis on a program. <code>graphs/</code> Directory containing ProGraML representations of LLVM IRs. <code>graphs/&lt;source&gt;.&lt;id&gt;.&lt;lang&gt;.ProgramGraph.pb</code> A ProgramGraph protocol buffer of an LLVM IR in the ProGraML representation. <code>ll/</code> Directory containing LLVM-IR files. <code>ir/&lt;source&gt;.&lt;id&gt;.&lt;lang&gt;.ll</code> An LLVM IR in text format, as produced by <code>clang -emit-llvm -S</code> or equivalent. <code>test/</code> A directory containing symlinks to graphs in the <code>graphs/</code> directory, indicating which graphs should be used as part of the test set. <code>train/</code> A directory containing symlinks to graphs in the <code>graphs/</code> directory, indicating which graphs should be used as part of the training set. <code>val/</code> A directory containing symlinks to graphs in the <code>graphs/</code> directory, indicating which graphs should be used as part of the validation set. <code>vocab/</code> Directory containing vocabulary files. <code>vocab/&lt;type&gt;.csv</code> A vocabulary file, which lists unique node texts, their frequency in the dataset, and the cumulative proportion of total unique node texts that is covered. For further information please see our ProGraML repository.",
      "paperUrl": "https://arxiv.org/pdf/2003.10536",
      "sourceUrl": "https://doi.org/10.5281/zenodo.4122436",
      "tags": [
        "Clang",
        "IR",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Chris Cummins"
      ]
    },
    {
      "id": "openalex-w3109289290",
      "source": "openalex-discovery",
      "title": "Deep Data Flow Analysis",
      "authors": [
        {
          "name": "Chris Cummins",
          "affiliation": "Meta (Israel)"
        },
        {
          "name": "Zacharias V. Fisches",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Tal Ben‐Nun",
          "affiliation": "École Polytechnique Fédérale de Lausanne"
        },
        {
          "name": "Torsten Hoefler",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Hugh Leather",
          "affiliation": "Meta (Israel)"
        },
        {
          "name": "Michael O’Boyle",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compiler architects increasingly look to machine learning when building heuristics for compiler optimization. The promise of automatic heuristic design, freeing the compiler engineer from the complex interactions of program, architecture, and other optimizations, is alluring. However, most machine learning methods cannot replicate even the simplest of the abstract interpretations of data flow analysis that are critical to making good optimization decisions. This must change for machine learning to become the dominant technology in compiler heuristics. To this end, we propose ProGraML - Program Graphs for Machine Learning - a language-independent, portable representation of whole-program semantics for deep learning. To benchmark current and future learning techniques for compiler analyses we introduce an open dataset of 461k Intermediate Representation (IR) files for LLVM, covering five source programming languages, and 15.4M corresponding data flow results. We formulate data flow analysis as an MPNN and show that, using ProGraML, standard analyses can be learned, yielding improved performance on downstream compiler optimization tasks.",
      "paperUrl": "https://arxiv.org/pdf/2012.01470",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2012.01470",
      "tags": [
        "IR",
        "Optimizations",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Chris Cummins",
        "Hugh Leather"
      ]
    },
    {
      "id": "openalex-w3109233608",
      "source": "openalex-discovery",
      "title": "Dataflow-based pruning for speeding up superoptimization",
      "authors": [
        {
          "name": "Manasij Mukherjee",
          "affiliation": "University of Utah"
        },
        {
          "name": "Pranav Kant",
          "affiliation": "University of Utah"
        },
        {
          "name": "Zhengyang Liu",
          "affiliation": "University of Utah"
        },
        {
          "name": "John Regehr",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2020",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 4 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Superoptimization is a compilation strategy that uses search to improve code quality, rather than relying on a canned sequence of transformations, as traditional optimizing compilers do. This search can be seen as a program synthesis problem: from unoptimized code serving as a specification, the synthesis procedure attempts to create a more efficient implementation. An important family of synthesis algorithms works by enumerating candidates and then successively checking if each refines the specification, using an SMT solver. The contribution of this paper is a pruning technique which reduces the enumerative search space using fast dataflow-based techniques to discard synthesis candidates that contain symbolic constants and uninstantiated instructions. We demonstrate the effectiveness of this technique by improving the runtime of an enumerative synthesis procedure in the Souper superoptimizer for the LLVM intermediate representation. The techniques presented in this paper eliminate 65% of the solver calls made by Souper, making it 2.32x faster (14.54 hours vs 33.76 hours baseline, on a large multicore) at solving all 269,113 synthesis problems that Souper encounters when optimizing the C and C++ programs from SPEC CPU 2017.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3428245",
      "sourceUrl": "https://doi.org/10.1145/3428245",
      "tags": [
        "C++"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Manasij Mukherjee",
        "Zhengyang Liu"
      ]
    },
    {
      "id": "openalex-w3017954454",
      "source": "openalex-discovery",
      "title": "Data Parallel C++",
      "authors": [
        {
          "name": "Ben Ashbaugh",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "Alexey Bader",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "James Brodman",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "Jeff R. Hammond",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "Michael Kinsner",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "S. J. Pennycook",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "Roland Schulz",
          "affiliation": "Intel (United Kingdom)"
        },
        {
          "name": "Jason Sewall",
          "affiliation": "Intel (United Kingdom)"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "SYCL™ is a heterogeneous programming framework built on top of modern C++. Data Parallel C++, recently introduced as part of Intel's oneAPI project, is an implementation of SYCL. Data Parallel C++ (DPC++) is being developed as an open-source project on top of Clang and LLVM. It combines C++, SYCL, and new extensions to improve programmer productivity when writing highly performant code for heterogeneous architectures.",
      "paperUrl": "https://doi.org/10.1145/3388333.3388653",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Alexey Bader"
      ]
    },
    {
      "id": "openalex-w3110545230",
      "source": "openalex-discovery",
      "title": "Copy-and-Patch Binary Code Generation",
      "authors": [
        {
          "name": "Haoran Xu",
          "affiliation": ""
        },
        {
          "name": "Fredrik Kjølstad",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Runtime compilation of runtime-constructed code is becoming standard practice in libraries, DSLs, and database management systems. Since compilation is expensive, systems that are sensitive to compile times such as relational database query compilers compile only hot code and interprets the rest with a much slower interpreter. We present a code generation technique that lowers an AST to binary code by stitching together code from a large library of binary AST node implementations. We call the implementations stencils because they have holes where values must be inserted during code generation. We show how to construct such a stencil library and describe the copy-and-patch technique that generates optimized binary code. The result is a code generator with negligible cost: it produces code from an AST in less time than it takes to construct the AST. Compared to LLVM, compilation is two orders of magnitude faster than -O0 and three orders of magnitude faster than higher optimization levels. The generated code runs an order of magnitude faster than interpretation and runs even faster than LLVM -O0. Thus, copy-and-patch can effectively replace both interpreters and LLVM -O0, making code generation more effective in compile-time sensitive applications.",
      "paperUrl": "https://arxiv.org/abs/2011.13127",
      "sourceUrl": "",
      "tags": [
        "Libraries"
      ],
      "matchedAuthors": [
        "Haoran Xu"
      ]
    },
    {
      "id": "openalex-w3059812300",
      "source": "openalex-discovery",
      "title": "Compiling ONNX Neural Network Models Using MLIR",
      "authors": [
        {
          "name": "Jin Tian",
          "affiliation": ""
        },
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": ""
        },
        {
          "name": "Tung D. Le",
          "affiliation": ""
        },
        {
          "name": "Tong Chen",
          "affiliation": ""
        },
        {
          "name": "Gong Su",
          "affiliation": ""
        },
        {
          "name": "Haruki Imai",
          "affiliation": ""
        },
        {
          "name": "Yasushi Negishi",
          "affiliation": ""
        },
        {
          "name": "Anh Leu",
          "affiliation": ""
        },
        {
          "name": "Kevin O’Brien",
          "affiliation": ""
        },
        {
          "name": "Kiyokuni Kawachiya",
          "affiliation": ""
        },
        {
          "name": "Alexandre E. Eichenberger",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Deep neural network models are becoming increasingly popular and have been used in various tasks such as computer vision, speech recognition, and natural language processing. Machine learning models are commonly trained in a resource-rich environment and then deployed in a distinct environment such as high availability machines or edge devices. To assist the portability of models, the open-source community has proposed the Open Neural Network Exchange (ONNX) standard. In this paper, we present a high-level, preliminary report on our onnx-mlir compiler, which generates code for the inference of deep neural network models described in the ONNX format. Onnx-mlir is an open-source compiler implemented using the Multi-Level Intermediate Representation (MLIR) infrastructure recently integrated in the LLVM project. Onnx-mlir relies on the MLIR concept of dialects to implement its functionality. We propose here two new dialects: (1) an ONNX specific dialect that encodes the ONNX standard semantics, and (2) a loop-based dialect to provide for a common lowering point for all ONNX dialect operations. Each intermediate representation facilitates its own characteristic set of graph-level and loop-based optimizations respectively. We illustrate our approach by following several models through the proposed representations and we include some early optimization work and performance results.",
      "paperUrl": "https://arxiv.org/pdf/2008.08272",
      "sourceUrl": "https://doi.org/10.48550/arxiv.2008.08272",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Kevin O'Brien",
        "Tung D. Le"
      ]
    },
    {
      "id": "openalex-w3036948649",
      "source": "openalex-discovery",
      "title": "Compiler Support for Operator Overloading and Algorithmic Differentiation in C++",
      "authors": [
        {
          "name": "Alexander Hück",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "TUbilio (Technical University of Darmstadt) | Vol. None (Issue None)",
      "type": "thesis",
      "abstract": "Multiphysics software needs derivatives for, e.g., solving a system of non-linear equations, conducting model verification, or sensitivity studies. In C++, algorithmic differentiation (AD), based on operator overloading (overloading), can be used to calculate derivatives up to machine precision. To that end, the built-in floating-point type is replaced by the user-defined AD type. It overloads all required operators, and calculates the original value and the corresponding derivative based on the chain rule of calculus. While changing the underlying type seems straightforward, several complications arise concerning software and performance engineering. This includes (1) fundamental language restrictions of C++ w.r.t. user-defined types, (2) type correctness of distributed computations with the Message Passing Interface (MPI) library, and (3) identification and mitigation of AD induced overheads. To handle these issues, AD experts may spend a significant amount of time to enhance a code with AD, verify the derivatives and ensure optimal application performance. Hence, in this thesis, we propose a modern compiler-based tooling approach to support and accelerate the AD-enhancement process of C++ target codes. In particular, we make contributions to three aspects of AD. The initial type change - While the change to the AD type in a target code is conceptually straightforward, the type change often leads to a multitude of compiler error messages. This is due to the different treatment of built-in floating-point types and user-defined types by the C++ language standard. Previously legal code constructs in the target code subsequently violate the language standard when the built-in floating-point type is replaced with a user-defined AD type. We identify and classify these problematic code constructs and their root cause is shown. Solutions by localized source transformation are proposed. To automate this rather mechanical process, we develop a static code analyser and source transformation tool, called OO-Lint, based on the Clang compiler framework. It flags instances of these problematic code constructs and applies source transformations to make the code compliant with the requirements of the language standard. To show the overall relevance of complications with user-defined types, OO-Lint is applied to several well-known scientific codes, some of which have already been AD enhanced by others. In all of these applications, except the ones manually treated for AD overloading, problematic code constructs are detected. Type correctness of MPI communication - MPI is the de-facto standard for programming high performance, distributed applications. At the same time, MPI has a complex interface whose usage can be error-prone. For instance, MPI derived data types require manual construction by specifying memory locations of the underlying data. Specifying wrong offsets can lead to subtle bugs that are hard to detect. In the context of AD, special libraries exist that handle the required derivative book-keeping by replacing the MPI communication calls with overloaded variants. However, on top of the AD type change, the MPI communication routines have to be changed manually. In addition, the AD type fundamentally changes memory layout assumptions as it has a different extent than the built-in types. Previously legal layout assumptions have, thus, to be reverified. As a remedy, to detect any type-related errors, we developed a memory sanitizer tool, called TypeART, based on the LLVM compiler framework and the MPI correctness checker MUST. It tracks all memory allocations relevant to MPI communication to allow for checking the underlying type and extent of the typeless memory buffer address passed to any MPI routine. The overhead induced by TypeART w.r.t. several target applications is manageable. AD domain-specific profiling - Applying AD in a black-box manner, without consideration of the target code structure, can have a significant impact on both runtime and memory consumption. An AD expert is usually required to apply further AD-related optimizations for the reduction of these induced overheads. Traditional profiling techniques are, however, insufficient as they do not reveal any AD domain-specific metrics. Of interest for AD code optimization are, e.g., specific code patterns, especially on a function level, that can be treated efficiently with AD. To that end, we developed a static profiling tool, called ProAD, based on the LLVM compiler framework. For each function, it generates the computational graph based on the static data flow of the floating-point variables. The framework supports pattern analysis on the computational graph to identify the optimal application of the chain rule. We show the potential of the optimal application of AD with two case studies. In both cases, significant runtime improvements can be achieved when the knowledge of the code structure, provided by our tool, is exploited. For instance, with a stencil code, a speedup factor of about 13 is achieved compared to a naive application of AD and a factor of 1.2 compared to hand-written derivative code.",
      "paperUrl": "https://tuprints.ulb.tu-darmstadt.de/11522",
      "sourceUrl": "https://doi.org/10.25534/tuprints-00011522",
      "tags": [
        "C++",
        "Clang",
        "Libraries",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexander Hück"
      ]
    },
    {
      "id": "openalex-w3096509020",
      "source": "openalex-discovery",
      "title": "ComPy-Learn: A toolbox for exploring machine learning representations for compilers",
      "authors": [
        {
          "name": "Alexander Brauckmann",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Andrés Goens",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Jerónimo Castrillón",
          "affiliation": "TU Dresden"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Deep Learning methods have not only shown to improve software performance in compiler heuristics, but also e.g. to improve security in vulnerability prediction or to boost developer productivity in software engineering tools. A key to the success of such methods across these use cases is the expressiveness of the representation used to abstract from the program code. Recent work has shown that different such representations have unique advantages in terms of performance. However, determining the best-performing one for a given task is often not obvious and requires empirical evaluation. Therefore, we present ComPy-Learn, a toolbox for conveniently defining, extracting, and exploring representations of program code. With syntax-level language information from the Clang compiler frontend and low-level information from the LLVM compiler backend, the tool supports the construction of linear and graph representations and enables an efficient search for the best-performing representation and model for tasks on program code.",
      "paperUrl": "https://doi.org/10.1109/fdl50818.2020.9232946",
      "sourceUrl": "",
      "tags": [
        "Backend",
        "Clang",
        "Frontend",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Alexander Brauckmann"
      ]
    },
    {
      "id": "openalex-w3130803778",
      "source": "openalex-discovery",
      "title": "CODIR: Towards an MLIR Codelet Model Dialect",
      "authors": [
        {
          "name": "Ryan Kabrick",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Diego A. Roa Perdomo",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Siddhisanket Raskar",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Jose M. Monsalve Diaz",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Dawson Fox",
          "affiliation": "University of Delaware"
        },
        {
          "name": "Guang R. Gao",
          "affiliation": "University of Delaware"
        }
      ],
      "year": "2020",
      "venue": "Vol. abs 1512 1274 (Issue None)",
      "type": "research-paper",
      "abstract": "Since IBM first introduced the Instruction Set Architecture (ISA), there has been tremendous research conducted on novel hardware architectures and new high-level software frameworks, perhaps to a fault. With innovation came an ever growing gap between a program's description and how it is executed on hardware. Successful exploitation of these innovations calls for a unifying contract to be in place which distinguishes the program description from the low-level execution on target hardware; namely, a Program Execution Model (PXM). By decoupling the programming API from the hardware description the obstacles imposed by low-level architectures and high-level language interoperability are avoided by utilizing program manipulation, source-level optimizations, and eventual lowering into machine code across a wide variety of different heterogeneous hardware. To achieve these goals in software, compilers must adapt to the ever-changing climate of hardware and software development. They must be able to represent data at varying levels so as to provide maximum opportunities for optimizations based on information which would otherwise be lost without utilizing progressive lowering. The target hardware should also not limit the amount of possible software-based optimizations. We propose CODIR, the Codelet Model domain-specific language extension and intermediate representation built using the already existing Multi-Level Intermediate Representation (MLIR) compiler infrastructure. It aims to progressively lower a high-level language to an intermediate representation well suited for maximizing optimization opportunities available to the compiler while additionally providing a plethora of hardware support. Underlying CODIR is the Codelet Model which we believe to be a sound program execution model for mapping computation to the machine and utilizing all available hardware resources.",
      "paperUrl": "https://doi.org/10.1109/ipdrm51949.2020.00009",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Jose M Monsalve Diaz"
      ]
    },
    {
      "id": "openalex-w3023141480",
      "source": "openalex-discovery",
      "title": "BinRec",
      "authors": [
        {
          "name": "Anil Altinay",
          "affiliation": "University of California System"
        },
        {
          "name": "Joseph Nash",
          "affiliation": "University of California System"
        },
        {
          "name": "Taddeus Kroes",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Prabhu Rajasekaran",
          "affiliation": "University of California System"
        },
        {
          "name": "Dixin Zhou",
          "affiliation": "University of California System"
        },
        {
          "name": "Adrian Dabrowski",
          "affiliation": "University of California System"
        },
        {
          "name": "David Gen�s",
          "affiliation": "University of California System"
        },
        {
          "name": "Yeoul Na",
          "affiliation": "University of California System"
        },
        {
          "name": "Stijn Volckaert",
          "affiliation": "KU Leuven"
        },
        {
          "name": "Cristiano Giuffrida",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Herbert Bos",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Michael Franz",
          "affiliation": "University of California System"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Binary lifting and recompilation allow a wide range of install-Time program transformations, such as security hardening, deobfuscation, and reoptimization. Existing binary lifting tools are based on static disassembly and thus have to rely on heuristics to disassemble binaries. In this paper, we present BinRec, a new approach to heuristic-free binary recompilation which lifts dynamic traces of a binary to a compiler-level intermediate representation (IR) and lowers the IR back to a \"recovered\" binary. This enables BinRec to apply rich program transformations, such as compiler-based optimization passes, on top of the recovered representation. We identify and address a number of challenges in binary lifting, including unique challenges posed by our dynamic approach. In contrast to existing frameworks, our dynamic frontend can accurately disassemble and lift binaries without heuristics, and we can successfully recover obfuscated code and all SPEC INT 2006 benchmarks including C++ applications. We evaluate BinRec in three application domains: i) binary reoptimization, ii) deobfuscation (by recovering partial program semantics from virtualization-obfuscated code), and iii) binary hardening (by applying existing compiler-level passes such as AddressSanitizer and SafeStack on binary code).",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3342195.3387550",
      "sourceUrl": "https://doi.org/10.1145/3342195.3387550",
      "tags": [
        "C++",
        "Frontend",
        "IR",
        "Security"
      ],
      "matchedAuthors": [
        "Cristiano Giuffrida",
        "Herbert Bos",
        "Michael Franz",
        "Yeoul Na"
      ]
    },
    {
      "id": "openalex-w3124802618",
      "source": "openalex-discovery",
      "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization Pragmas Using Bayesian Optimization",
      "authors": [
        {
          "name": "Xingfu Wu",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Michael Kruse",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Prasanna Balaprakash",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Hal Finkel",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Paul Hovland",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Valerie Taylor",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Mary Hall",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "An autotuning is an approach that explores a search space of possible implementations/configurations of a kernel or an application by selecting and evaluating a subset of implementations/configurations on a target platform and/or use models to identify a high performance implementation/configuration. In this paper, we develop an autotuning framework that leverages Bayesian optimization to explore the parameter space search. We select six of the most complex benchmarks from the application domains of the PolyBench benchmarks (syr2k, 3mm, heat-3d, lu, covariance, and Floyd-Warshall) and apply the newly developed LLVM Clang/Polly loop optimization pragmas to the benchmarks to optimize them. We then use the autotuning framework to optimize the pragma parameters to improve their performance. The experimental results show that our autotuning approach outperforms the other compiling methods to provide the smallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and covariance with two large datasets in 200 code evaluations for effectively searching the parameter spaces with up to 170,368 different configurations. We compare four different supervised learning methods within Bayesian optimization and evaluate their effectiveness. We find that the Floyd-Warshall benchmark did not benefit from autotuning because Polly uses heuristics to optimize the benchmark to make it run much slower. To cope with this issue, we provide some compiler option solutions to improve the performance.",
      "paperUrl": "https://doi.org/10.1109/pmbs51919.2020.00012",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Hal Finkel",
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w3037712104",
      "source": "openalex-discovery",
      "title": "AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep Reinforcement Learning",
      "authors": [
        {
          "name": "Ameer Haj-Ali",
          "affiliation": ""
        },
        {
          "name": "Qijing Huang",
          "affiliation": ""
        },
        {
          "name": "John Xiang",
          "affiliation": ""
        },
        {
          "name": "William S. Moses",
          "affiliation": ""
        },
        {
          "name": "Krste Asanović",
          "affiliation": ""
        },
        {
          "name": "John Wawrzynek",
          "affiliation": ""
        },
        {
          "name": "Ion Stoica",
          "affiliation": ""
        }
      ],
      "year": "2020",
      "venue": "arXiv (Cornell University) | Vol. 2 (Issue None)",
      "type": "research-paper",
      "abstract": "The performance of the code a compiler generates depends on the order in which it applies the optimization passes. Choosing a good order--often referred to as the phase-ordering problem, is an NP-hard problem. As a result, existing solutions rely on a variety of heuristics. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. To this end, we implement AutoPhase: a framework that takes a program and uses deep reinforcement learning to find a sequence of compilation passes that minimizes its execution time. Without loss of generality, we construct this framework in the context of the LLVM compiler toolchain and target high-level synthesis programs. We use random forests to quantify the correlation between the effectiveness of a given pass and the program's features. This helps us reduce the search space by avoiding phase orderings that are unlikely to improve the performance of a given program. We compare the performance of AutoPhase to state-of-the-art algorithms that address the phase-ordering problem. In our evaluation, we show that AutoPhase improves circuit performance by 28% when compared to using the -O3 compiler flag, and achieves competitive results compared to the state-of-the-art solutions, while requiring fewer samples. Furthermore, unlike existing state-of-the-art solutions, our deep reinforcement learning solution shows promising result in generalizing to real benchmarks and 12,874 different randomly generated programs, after training on a hundred randomly generated programs.",
      "paperUrl": "https://arxiv.org/pdf/2003.00671",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w3018653512",
      "source": "openalex-discovery",
      "title": "An updated LLVM-based quantum research compiler with further OpenQASM support",
      "authors": [
        {
          "name": "Andrew Litteken",
          "affiliation": "University of Chicago"
        },
        {
          "name": "Yung-Ching Fan",
          "affiliation": "University of Chicago"
        },
        {
          "name": "Devina Singh",
          "affiliation": "Princeton University"
        },
        {
          "name": "Margaret Martonosi",
          "affiliation": "Princeton University"
        },
        {
          "name": "Frederic T. Chong",
          "affiliation": "University of Chicago"
        }
      ],
      "year": "2020",
      "venue": "Quantum Science and Technology | Vol. 5 (Issue 3)",
      "type": "research-paper",
      "abstract": "Abstract Quantum computing is a rapidly growing field with the potential to change how we solve previously intractable problems. Emerging hardware is approaching a complexity that requires increasingly sophisticated programming and control. Scaffold is an older quantum programming language that was originally designed for resource estimation for far-future, large quantum machines, and ScaffCC is the corresponding LLVM-based compiler. For the first time, we provide a full and complete overview of the language itself, the compiler as well as its pass structure. While previous works Abhari et al (2015 Parallel Comput. 45 2–17), Abhari et al (2012 Scaffold: quantum programming language https://cs.princeton.edu/research/techreps/TR-934-12 ), have piecemeal descriptions of different portions of this toolchain, we provide a more full and complete description in this paper. We also introduce updates to ScaffCC including conditional measurement and multidimensional qubit arrays designed to keep in step with modern quantum assembly languages, as well as an alternate toolchain targeted at maintaining correctness and low resource count for noisy-intermediate scale quantum (NISQ) machines, and compatibility with current versions of LLVM and Clang. Our goal is to provide the research community with a functional LLVM framework for quantum program analysis, optimization, and generation of executable code.",
      "paperUrl": "https://doi.org/10.1088/2058-9565/ab8c2c",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Quantum Computing"
      ],
      "matchedAuthors": [
        "Andrew Litteken"
      ]
    },
    {
      "id": "openalex-w3092642880",
      "source": "openalex-discovery",
      "title": "A Vector-Length Agnostic Compiler for the Connex-S Accelerator with Scratchpad Memory",
      "authors": [
        {
          "name": "Alexandru E. Şuşu",
          "affiliation": "Universitatea Națională de Știință și Tehnologie Politehnica București"
        }
      ],
      "year": "2020",
      "venue": "ACM Transactions on Embedded Computing Systems | Vol. 19 (Issue 6)",
      "type": "research-paper",
      "abstract": "Compiling sequential C programs for Connex-S, a competitive, scalable and customizable, wide vector accelerator for intensive embedded applications with 32 to 4,096 16-bit integer lanes and a limited capacity local scratchpad memory, is challenging. Our compiler toolchain uses the LLVM framework and targets OPINCAA, a JIT vector assembler and coordination C++ library for Connex-S accelerating computations for an arbitrary CPU. Therefore, we address in the compiler middle end aspects of efficient vectorization, communication, and synchronization. We perform quantitative static analysis of the program useful, among others, for the symbolic-size compiler memory allocator and the coordination mechanism of OPINCAA. We also discuss the LLVM back end for the Connex-S processor and the methodology to automatically generate instruction selection code for emulating efficiently arithmetic and logical operations for non-native types such as 32-bit integer and 16-bit floating-point. By using JIT vector assembling and by encoding the vector length of Connex-S as a parameter in the generated OPINCAA program, we achieve vector-length agnosticism to support execution on distinct embedded devices, such as several digital cameras with different resolutions, each equipped with custom-width Connex-S accelerators meant to save energy for the image processing kernels. Since Connex-S has a limited capacity local scratchpad memory of 256 KB normally, we present how we also use the PPCG C-to-C code generator to perform data tiling to minimize the total kernel execution time, subject to fitting larger program data in the local memory. We devise an accurate cost model for the Connex-S accelerator to choose optimal performance tile sizes at compile time. We successfully compile several simple benchmarks frequently used, for example, in high-performance and computer vision embedded applications. We report speedup factors of up to 11.33 when running them on a Connex-S accelerator with 128 16-bit integer lanes w.r.t. the dual-core ARM Cortex A9 host clocked at a frequency 6.67 times higher, with a total of two 128-bit Neon SIMD units.",
      "paperUrl": "https://doi.org/10.1145/3406536",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Embedded",
        "JIT",
        "Performance",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Alexandru E. Şuşu"
      ]
    },
    {
      "id": "openalex-w3186753954",
      "source": "openalex-discovery",
      "title": "A New Backend for Standard ML of New Jersey",
      "authors": [
        {
          "name": "Kavon Farvardin",
          "affiliation": "University of Chicago"
        },
        {
          "name": "John Reppy",
          "affiliation": "University of Chicago"
        }
      ],
      "year": "2020",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper describes the design and implementation of a new backend for the Standard ML of New Jersey (SML/NJ) system that is based on the LLVM compiler infrastructure. We first describe the history and design of the current backend, which is based on the MLRisc framework. While MLRisc has many similarities to LLVM, it provides a lower-level, policy-agnostic, approach to code generation that enables customization of the code generator for non-standard runtime models (i.e., register pinning, calling conventions, etc.). In particular, SML/NJ uses a stackless runtime model based on continuation-passing style with heap-allocated continuation closures. This feature, and others, pose challenges to building a backend using LLVM. We describe these challenges and how we address them in our backend.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3462172.3462191",
      "sourceUrl": "https://doi.org/10.1145/3462172.3462191",
      "tags": [
        "Backend",
        "Infrastructure",
        "ML"
      ],
      "matchedAuthors": [
        "John Reppy",
        "Kavon Farvardin"
      ]
    },
    {
      "id": "openalex-w2948519161",
      "source": "openalex-discovery",
      "title": "Unification-based Pointer Analysis without Oversharing",
      "authors": [
        {
          "name": "Jakub Kuderski",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "Jorge A. Navas",
          "affiliation": "SRI International"
        },
        {
          "name": "Arie Gurfinkel",
          "affiliation": "University of Waterloo"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Pointer analysis is indispensable for effectively verifying heap-manipulating programs. Even though it has been studied extensively, there are no publicly available pointer analyses that are moderately precise while scalable to large real-world programs. In this paper, we show that existing context-sensitive unification-based pointer analyses suffer from the problem of oversharing -- propagating too many abstract objects across the analysis of different procedures, which prevents them from scaling to large programs. We present a new pointer analysis for LLVM, called TeaDsa, without such an oversharing. We show how to further improve precision and speed of TeaDsa with extra contextual information, such as flow-sensitivity at call- and return-sites, and type information about memory accesses. We evaluate TeaDsa on the verification problem of detecting unsafe memory accesses and compare it against two state-of-the-art pointer analyses: SVF and SeaDsa. We show that TeaDsa is one order of magnitude faster than either SVF or SeaDsa, strictly more precise than SeaDsa, and, surprisingly, sometimes more precise than SVF.",
      "paperUrl": "https://arxiv.org/pdf/1906.01706",
      "sourceUrl": "https://doi.org/10.23919/fmcad.2019.8894275",
      "tags": [],
      "matchedAuthors": [
        "Jakub Kuderski"
      ]
    },
    {
      "id": "openalex-w2953452248",
      "source": "openalex-discovery",
      "title": "Understanding GCC builtins to develop better tools",
      "authors": [
        {
          "name": "Manuel Rigger",
          "affiliation": "Johannes Kepler University of Linz"
        },
        {
          "name": "Stefan Marr",
          "affiliation": "University of Kent"
        },
        {
          "name": "Bram Adams",
          "affiliation": "Polytechnique Montréal"
        },
        {
          "name": "Hanspeter Mössenböck",
          "affiliation": "Johannes Kepler University of Linz"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "C programs can use compiler builtins to provide functionality that the C language lacks. On Linux, GCC provides several thousands of builtins that are also supported by other mature compilers, such as Clang and ICC. Maintainers of other tools lack guidance on whether and which builtins should be implemented to support popular projects. To assist tool developers who want to support GCC builtins, we analyzed builtin use in 4,913 C projects from GitHub. We found that 37% of these projects relied on at least one builtin. Supporting an increasing proportion of projects requires support of an exponentially increasing number of builtins; however, implementing only 10 builtins already covers over 30% of the projects. Since we found that many builtins in our corpus remained unused, the effort needed to support 90% of the projects is moderate, requiring about 110 builtins to be implemented. For each project, we analyzed the evolution of builtin use over time and found that the majority of projects mostly added builtins. This suggests that builtins are not a legacy feature and must be supported in future tools. Systematic testing of builtin support in existing tools revealed that many lacked support for builtins either partially or completely; we also discovered incorrect implementations in various tools, including the formally verified CompCert compiler.",
      "paperUrl": "https://arxiv.org/pdf/1907.00863",
      "sourceUrl": "https://doi.org/10.1145/3338906.3338907",
      "tags": [
        "Clang",
        "Testing"
      ],
      "matchedAuthors": [
        "Bram Adams"
      ]
    },
    {
      "id": "openalex-w2996480042",
      "source": "openalex-discovery",
      "title": "Tapir",
      "authors": [
        {
          "name": "Tao B. Schardl",
          "affiliation": ""
        },
        {
          "name": "William S. Moses",
          "affiliation": ""
        },
        {
          "name": "Charles E. Leiserson",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "ACM Transactions on Parallel Computing | Vol. 6 (Issue 4)",
      "type": "research-paper",
      "abstract": "Tapir (pronounced TAY-per) is a compiler intermediate representation (IR) that embeds recursive fork-join parallelism, as supported by task-parallel programming platforms such as Cilk and OpenMP, into a mainstream compiler’s IR. Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into a parallel runtime. These calls prevent the compiler from performing optimizations on and across parallel control constructs. Remedying this situation has generally been thought to require an extensive reworking of compiler analyses and code transformations to handle parallel semantics. Tapir leverages the “serial-projection property,” which is commonly satisfied by task-parallel programs, to handle the semantics of these programs without an extensive rework of the compiler. For recursive fork-join programs that satisfy the serial-projection property, Tapir enables effective compiler optimization of parallel programs with only minor changes to existing compiler analyses and code transformations. Tapir uses the serial-projection property to order logically parallel fine-grained tasks in the program’s control-flow graph. This ordered representation of parallel tasks allows the compiler to optimize parallel codes effectively with only minor modifications. For example, to implement Tapir/LLVM, a prototype of Tapir in the LLVM compiler, we added or modified less than 3,000 lines of LLVM’s half-million-line core middle-end functionality. These changes sufficed to enable LLVM’s existing compiler optimizations for serial code—including loop-invariant-code motion, common-subexpression elimination, and tail-recursion elimination—to work with parallel control constructs such as parallel loops and Cilk’s Cilk_Spawn keyword. Tapir also supports parallel optimizations, such as loop scheduling, which restructure the parallel control flow of the program. By making use of existing LLVM optimizations and new parallel optimizations, Tapir/LLVM can optimize recursive fork-join programs more effectively than traditional compilation methods. On a suite of 35 Cilk application benchmarks, Tapir/LLVM produces more efficient executables for 30 benchmarks, with faster 18-core running times for 26 of them, compared to a nearly identical compiler that compiles parallel linguistic constructs the traditional way.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3365655",
      "sourceUrl": "https://doi.org/10.1145/3365655",
      "tags": [
        "IR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w2955366617",
      "source": "openalex-discovery",
      "title": "TWINS - This Workflow Is Not Scrum: Agile Process Adaptation for Open Source Software Projects",
      "authors": [
        {
          "name": "Paul T. Robinson",
          "affiliation": ""
        },
        {
          "name": "Sarah Beecham",
          "affiliation": "University of Limerick"
        }
      ],
      "year": "2019",
      "venue": "Vol. 19 (Issue None)",
      "type": "research-paper",
      "abstract": "It is becoming commonplace for companies to contribute to open source software (OSS) projects. At the same time, many software organizations are applying Scrum software development practices, for productivity and quality gains. Scrum calls for self-organizing teams, in which the development team has total control over its development process. However, OSS projects typically have their own processes and standards, which might not mesh well with a company's internal processes, such as Scrum. This paper presents an experience report from Sony Interactive Entertainment (SIE), where the \"toolchain CPU compiler\" team directly participates in the \"LLVM\" OSS project. The team ran into a number of difficulties when using Scrum to manage their development. In particular, the team often failed to complete Scrum sprints where tasks required interaction with the open source community. We look at how the team redefined task flows to alleviate these difficulties, and eventually evolved a highly modified process, dubbed TWINS (This Workflow Is Not Scrum). We assess the revised process, and compare it to other established agile methods, finding it bears a strong resemblance to Scrumban (the SIE team was not aware of Scrumban previously). The TWINS framework presented here may help other organizations who develop software in-house and engage in OSS projects, to gain the best of both worlds.",
      "paperUrl": "https://doi.org/10.1109/icssp.2019.00014",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Paul T. Robinson"
      ]
    },
    {
      "id": "openalex-w3006438355",
      "source": "openalex-discovery",
      "title": "TCD: Statically Detecting Type Confusion Errors in C++ Programs",
      "authors": [
        {
          "name": "Changwei Zou",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Yulei Sui",
          "affiliation": "University of Technology Sydney"
        },
        {
          "name": "Hua Yan",
          "affiliation": "UNSW Sydney"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "For performance reasons, C++, albeit unsafe, is often the programming language of choice for developing software infrastructures. A serious type of security vulnerability in C++ programs is type confusion, which may lead to program crashes and control flow hijack attacks. While existing mitigation solutions almost exclusively rely on dynamic analysis techniques, which suffer from low code coverage and high overhead, static analysis has rarely been investigated. This paper presents TCD, a static type confusion detector built on top of a precise demand-driven field-, context-and flow-sensitive pointer analysis. Unlike existing pointer analyses, TCD is type-aware as it not only preserves the type information in the pointed-to objects but also handles complex language features of C++ such as multiple inheritance and placement new, making it therefore possible to reason about type casting in C++ programs. We have implemented TCD in LLVM and evaluated it using seven C++ applications (totaling 526,385 lines of C++ code) from Qt, a widely-adopted C++ toolkit for creating GUIs and cross-platform software. TCD has found five type confusion bugs, including one reported previously in prior work and four new ones, in under 7.3 hours, with a low false positive rate of 28.2%.",
      "paperUrl": "https://doi.org/10.1109/issre.2019.00037",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Dynamic Analysis",
        "Infrastructure",
        "Performance",
        "Security",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Jingling Xue",
        "Yulei Sui"
      ]
    },
    {
      "id": "openalex-w2973792890",
      "source": "openalex-discovery",
      "title": "Supporting on-stack replacement in unstructured languages by loop reconstruction and extraction",
      "authors": [
        {
          "name": "Raphael Mosaner",
          "affiliation": ""
        },
        {
          "name": "David Leopoldseder",
          "affiliation": ""
        },
        {
          "name": "Manuel Rigger",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "Roland Schatz",
          "affiliation": ""
        },
        {
          "name": "Hanspeter Mössenböck",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "On-stack replacement (OSR) is a common technique employed by dynamic compilers to reduce program warm-up time. OSR allows switching from interpreted to compiled code during the execution of this code. The main targets are long running loops, which need to be represented explicitly, with dedicated information about condition and body, to be optimized at run time. Bytecode interpreters, however, represent control flow implicitly via unstructured jumps and thus do not exhibit the required high-level loop representation. To enable OSR also for jump-based - often called unstructured - languages, we propose the partial reconstruction of loops in order to explicitly represent them in a bytecode interpreter. Besides an outline of the general idea, we implemented our approach in Sulong, a bytecode interpreter for LLVM bitcode, which allows the execution of C/C++. We conducted an evaluation with a set of C benchmarks, which showed speed-ups in warm-up of up to 9x for certain benchmarks. This facilitates execution of programs with long-running loops in rarely called functions, which would yield significant slowdown without OSR. While shown with a prototype implementation, the overall idea of our approach is generalizable for all bytecode interpreters.",
      "paperUrl": "https://arxiv.org/pdf/1909.08815",
      "sourceUrl": "https://doi.org/10.1145/3357390.3361030",
      "tags": [
        "C++"
      ],
      "matchedAuthors": [
        "Roland Schatz"
      ]
    },
    {
      "id": "openalex-w3015172527",
      "source": "openalex-discovery",
      "title": "Segment Streaming for the Three-Phase Execution Model: Design and Implementation",
      "authors": [
        {
          "name": "Muhammad Refaat Sedky Soliman",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "Giovani Gracioli",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Rohan Tabish",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Rodolfo Pellizzoni",
          "affiliation": "University of Waterloo"
        },
        {
          "name": "Marco Caccamo",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Scheduling tasks using the three-phase execution model (load-execute-unload) can effectively reduce the contention on shared resources in real-time systems. Due to system and program constraints, a task is generally segmented and executed over multiple intervals. Several works showed that co-scheduling memory (unload-load) and computation phases can improve the system schedulability by hiding the memory transfer time. However, this is limited to segments of different tasks and hence executing segments of the same task back-to-back is not allowed. In this paper, we propose a new streaming model to allow overlapping the memory and execution phases of segments of the same task. This is accomplished by a segmentation framework implemented within an LLVM-based compiler-level tool along with a Real-Time Operating System (RTOS) API to handle load/unload requests. Memory phases are processed by a DMA engine that loads/unloads the task content into ScratchPad Memory (SPM). We provide a schedulability analysis of the proposed model under fixed priority partitioned scheme and an RTOS implementation of the API on a latest-generation Multiprocessor System-on-Chip (MPSoC).",
      "paperUrl": "https://doi.org/10.1109/rtss46320.2019.00032",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Marco Caccamo",
        "Rodolfo Pellizzoni"
      ]
    },
    {
      "id": "openalex-w2970672767",
      "source": "openalex-discovery",
      "title": "Scalable Context-Sensitive Pointer Analysis for LLVM",
      "authors": [
        {
          "name": "Jakub Kuderski",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "UWSpace (University of Waterloo) | Vol. None (Issue None)",
      "type": "thesis",
      "abstract": "Pointer analysis is indispensable for effectively verifying heap-manipulating programs.&#13;\\nEven though it has been studied extensively, there are no publicly available pointer analyses&#13;\\nfor low-level languages that are moderately precise while scalable to large real-world programs.&#13;\\nIn this thesis, we show that existing context-sensitive unification-based pointer analyses suffer&#13;\\nfrom the problem of oversharing – propagating too many abstract objects across the analysis&#13;\\nof different procedures, which prevents them from scaling to large programs.&#13;\\nWe present a new pointer analysis for LLVM, called TeaDsa, with such an oversharing&#13;\\nsignificantly reduced. We show how to further improve precision and speed of TeaDsa&#13;\\nwith extra contextual information, such as flow-sensitivity at call- and return-sites, and&#13;\\ntype information about memory accesses. We evaluate TeaDsa on the verification problem&#13;\\nof detecting unsafe memory accesses and compare it against two state-of-the-art pointer&#13;\\nanalyses: SVF and SeaDsa. We show that TeaDsa is one order of magnitude faster than&#13;\\neither SVF or SeaDsa, strictly more precise than SeaDsa, and, surprisingly, sometimes&#13;\\nmore precise than SVF.",
      "paperUrl": "http://hdl.handle.net/10012/14875",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Jakub Kuderski"
      ]
    },
    {
      "id": "openalex-w2969433881",
      "source": "openalex-discovery",
      "title": "PrivAnalyzer: Measuring the Efficacy of Linux Privilege Use",
      "authors": [
        {
          "name": "John Criswell",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Jie Zhou",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Spyridoula Gravani",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Xiaoyu Hu",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Operating systems such as Linux break the power of the root user into separate privileges (which Linux calls capabilities) and give processes the ability to enable privileges only when needed and to discard them permanently when the program no longer needs them. However, there is no method of measuring how well the use of such facilities reduces the risk of privilege escalation attacks if the program has a vulnerability. This paper presents PrivAnalyzer, an automated tool that measures how effectively programs use Linux privileges. PrivAnalyzer consists of three components: 1) AutoPriv, an existing LLVM-based C/C++ compiler which uses static analysis to transform a program that uses Linux privileges into a program that safely removes them when no longer needed, 2) ChronoPriv, a new LLVM C/C++ compiler pass that performs dynamic analysis to determine for how long a program retains various privileges, and 3) ROSA, a new bounded model checker that can model the damage a program can do at each program point if an attacker can exploit the program and abuse its privileges. We use PrivAnalyzer to determine how long five privileged open source programs retain the ability to cause serious damage to a system and find that merely transforming a program to drop privileges does not significantly improve security. However, we find that simple refactoring can considerably increase the efficacy of Linux privileges. In two programs that we refactored, we reduced the percentage of execution in which a device file can be read and written from 97% and 88% to 4% and 1%, respectively.",
      "paperUrl": "https://doi.org/10.1109/dsn.2019.00065",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Dynamic Analysis",
        "Security",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "John Criswell"
      ]
    },
    {
      "id": "openalex-w2912702751",
      "source": "openalex-discovery",
      "title": "PPOpenCL: a performance-portable OpenCL compiler with host and kernel thread code fusion",
      "authors": [
        {
          "name": "Ying Liu",
          "affiliation": "Chinese Academy of Sciences"
        },
        {
          "name": "Lei Huang",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Ming-Chuan Wu",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Huimin Cui",
          "affiliation": "Institute of Computing Technology"
        },
        {
          "name": "Fang Lv",
          "affiliation": "Chinese Academy of Sciences"
        },
        {
          "name": "Xiaobing Feng",
          "affiliation": "University of Chinese Academy of Sciences"
        },
        {
          "name": "Jingling Xue",
          "affiliation": "UNSW Sydney"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "OpenCL offers code portability but no performance portability. Given an OpenCL program X specifically written for one platform P, existing OpenCL compilers, which usually optimize its host and kernel codes individually, often yield poor performance for another platform Q. Instead of obtaining a performance-improved version of X for Q via manual tuning, we aim to achieve this automatically by a source-to-source OpenCL compiler framework, PPOpenCL. By fusing X's host and kernel thread codes (with the operations in different work-items in the same work-group represented explicitly), we are able to apply data flow analyses, and subsequently, performance-enhancing optimizations on a fused control flow graph specifically for platform Q. Validation against OpenCL benchmarks shows that PPOpenCL (implemented in Clang 3.9.1) can achieve significantly improved portable performance on seven platforms considered.",
      "paperUrl": "https://doi.org/10.1145/3302516.3307350",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "OpenCL",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Huimin Cui",
        "Jingling Xue",
        "Xiaobing Feng",
        "Ying Liu"
      ]
    },
    {
      "id": "openalex-w2946780589",
      "source": "openalex-discovery",
      "title": "Nonio — modular automatic compiler phase selection and ordering specialization framework for modern compilers",
      "authors": [
        {
          "name": "Ricardo Nobre",
          "affiliation": "Universidade do Porto"
        },
        {
          "name": "João Bispo",
          "affiliation": "Universidade do Porto"
        },
        {
          "name": "Tiago Carvalho",
          "affiliation": "Universidade do Porto"
        },
        {
          "name": "João M. P. Cardoso",
          "affiliation": "Universidade do Porto"
        }
      ],
      "year": "2019",
      "venue": "SoftwareX | Vol. 10 (Issue None)",
      "type": "research-paper",
      "abstract": "This article presents Nonio, a modular, easy-to-use, design space exploration framework focused on exploring custom combinations of compiler flags and compiler sequences. We describe the framework and discuss its use with two of the most popular compiler toolchains, GCC and Clang+LLVM. Particularly, we discuss implementation details in the context of flag selection, when using GCC, and phase selection and ordering, when using Clang+LLVM. The framework software organization allows to easily add new components as plug-ins (e.g., an exploration algorithm, an objective metric, integration with another compiler toolchain). The software architecture provides well-defined interfaces, in order to enable seamless composition and interaction between different components. We present, as an example, a use case where we rely on Nonio to obtain custom compiler flags for reducing the execution time and the energy consumption of a C program, in relation to the best predetermined optimization settings provided by the compiler (e.g., –O3). Keywords: Compiler optimizations, Design space exploration, Flag selection, Performance",
      "paperUrl": "http://www.softxjournal.com/article/S2352711018301614/pdf",
      "sourceUrl": "https://doi.org/10.1016/j.softx.2019.100238",
      "tags": [
        "Clang",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "João Bispo"
      ]
    },
    {
      "id": "openalex-w2927161859",
      "source": "openalex-discovery",
      "title": "Mitigating JIT compilation latency in virtual execution environments",
      "authors": [
        {
          "name": "Martin Kristien",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Tom Spink",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Harry Wagstaff",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Björn Franke",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Igor Böhm",
          "affiliation": ""
        },
        {
          "name": "Nigel Topham",
          "affiliation": "University of Edinburgh"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Many Virtual Execution Environments (VEEs) rely on Justin-time (JIT) compilation technology for code generation at runtime, e.g. in Dynamic Binary Translation (DBT) systems or language Virtual Machines (VMs). While JIT compilation improves native execution performance as opposed to e.g. interpretive execution, the JIT compilation process itself introduces latency. In fact, for highly optimizing JIT compilers or compilers not specifically designed for JIT compilation, e.g. LLVM, this latency can cause a substantial overhead. While existing work has introduced asynchronously decoupled JIT compilation task farms to hide this JIT compilation latency, we show that this on its own is not sufficient to mitigate the impact of JIT compilation latency on overall performance. In this paper, we introduce a novel JIT compilation scheduling policy, which performs continuous low-cost profiling of code regions already dispatched for JIT compilation, right up to the point where compilation commences. We have integrated our novel JIT compilation scheduling approach into a commercial LLVM-based DBT system and demonstrate speedups of 1.32× on average, and up to 2.31×, over its state-of-the-art concurrent task-farm based JIT compilation scheme across the SPEC CPU2006 and BioPerf benchmark suites.",
      "paperUrl": "https://research-repository.st-andrews.ac.uk/bitstream/10023/24325/1/Kristien_2019_Mitigating_JIT_Compilation_Latency_AAM.pdf",
      "sourceUrl": "https://doi.org/10.1145/3313808.3313818",
      "tags": [
        "JIT",
        "Performance"
      ],
      "matchedAuthors": [
        "Tom Spink"
      ]
    },
    {
      "id": "openalex-w2993507867",
      "source": "openalex-discovery",
      "title": "Memory Sizing of a Scalable SRAM In-Memory Computing Tile Based Architecture",
      "authors": [
        {
          "name": "Roman Gauchi",
          "affiliation": "Institut polytechnique de Grenoble"
        },
        {
          "name": "Maha Kooli",
          "affiliation": "Université Grenoble Alpes"
        },
        {
          "name": "Pascal Vivet",
          "affiliation": "Laboratoire d'Électronique des Technologies de l'Information"
        },
        {
          "name": "Jean-Philippe Noël",
          "affiliation": "Université Grenoble Alpes"
        },
        {
          "name": "Édith Beigné",
          "affiliation": "Université Grenoble Alpes"
        },
        {
          "name": "Subhasish Mitra",
          "affiliation": "Palo Alto University"
        },
        {
          "name": "Henri‐Pierre Charles",
          "affiliation": "Institut polytechnique de Grenoble"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern computing applications require more and more data to be processed. Unfortunately, the trend in memory technologies does not scale as fast as the computing performances, leading to the so called memory wall. New architectures are currently explored to solve this issue, for both embedded and off-chip memories. Recent techniques that bringing computing as close as possible to the memory array such as, In-Memory Computing (IMC), Near-Memory Computing (NMC), Processing-In-Memory (PIM), allow to reduce the cost of data movement between computing cores and memories. For embedded computing, In-Memory Computing scheme presents advantageous computing and energy gains for certain class of applications. However, current solutions are not scaling to large size memories and high amount of data to compute. In this paper, we propose a new methodology to tile a SRAM/IMC based architecture and scale the memory requirements according to an application set. By using a high level LLVM-based simulation platform, we extract IMC memory requirements for a certain class of applications. Then, we detail the physical and performance costs of tiling SRAM instances. By exploring multi-tile SRAM Place&Route in 28nm FD-SOI, we explore the respective performance, energy and cost of memory interconnect. As a result, we obtain a detailed wire cost model in order to explore memory sizing trade-offs. To achieve a large capacity IMC memory, by splitting the memory in multiple sub-tiles, we can achieve lower energy (up to 78% gain) and faster (up to 49% gain) IMC tile compared to a single large IMC memory instance.",
      "paperUrl": "https://cea.hal.science/cea-02399937",
      "sourceUrl": "https://doi.org/10.1109/vlsi-soc.2019.8920373",
      "tags": [
        "Embedded",
        "Performance"
      ],
      "matchedAuthors": [
        "Maha Kooli"
      ]
    },
    {
      "id": "openalex-w2943963155",
      "source": "openalex-discovery",
      "title": "MLIR Primer: A Compiler Infrastructure for the End of Moore’s Law",
      "authors": [
        {
          "name": "Chris Lattner",
          "affiliation": ""
        },
        {
          "name": "Jacques A. Pienaar",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "http://ai.google/research/pubs/pub48035/",
      "sourceUrl": "",
      "tags": [
        "Infrastructure",
        "MLIR"
      ],
      "matchedAuthors": [
        "Chris Lattner"
      ]
    },
    {
      "id": "openalex-w3035997769",
      "source": "openalex-discovery",
      "title": "HaskellCompass: Extending the CodeCompass comprehension framework for Haskell",
      "authors": [
        {
          "name": "Boldizsár Németh",
          "affiliation": ""
        },
        {
          "name": "Tibor Brunner",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "While functional programming is an emerging and efficient way to express complex software, the tool support for functional programming languages, like Haskell, is not always satisfactory. There are no tools that can provide code insight for large Haskell projects that limits the potential of the language's usability.CodeCompass is an open-source LLVM/Clang-based tool developed by Ericsson Ltd. and Eötvös Loránd University, Budapest to help the understanding of large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements and is regularly used by hundreds of designers and developers.In this paper we present a Haskell plugin for the CodeCompass tool that is able to analyse large Haskell projects and provide useful information for the developers. We suggest useful comprehension queries and actions that enhance developer efficiency. Additionally we show the implementation of such a solution and measure the performance cost of using it.",
      "paperUrl": "https://doi.org/10.1109/informatics47936.2019.9119295",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "Performance",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Tibor Brunner"
      ]
    },
    {
      "id": "openalex-w2996635007",
      "source": "openalex-discovery",
      "title": "FailAmp",
      "authors": [
        {
          "name": "Ian Briggs",
          "affiliation": "University of Utah"
        },
        {
          "name": "Arnab Das",
          "affiliation": "University of Utah"
        },
        {
          "name": "Mark Baranowski",
          "affiliation": "University of Utah"
        },
        {
          "name": "Vishal Sharma",
          "affiliation": "Microsoft Research (United Kingdom)"
        },
        {
          "name": "Sriram Krishnamoorthy",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Zvonimir Rakamarić",
          "affiliation": "University of Utah"
        },
        {
          "name": "Ganesh Gopalakrishnan",
          "affiliation": "University of Utah"
        }
      ],
      "year": "2019",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 16 (Issue 4)",
      "type": "research-paper",
      "abstract": "We present FailAmp, a novel LLVM program transformation algorithm that makes programs employing structured index calculations more robust against soft errors. Without FailAmp, an offset error can go undetected; with FailAmp, all subsequent offsets are relativized, building on the faulty one. FailAmp can exploit ISAs such as ARM to further reduce overheads. We verify correctness properties of FailAMP using an SMT solver, and present a thorough evaluation using many high-performance computing benchmarks under a fault injection campaign. FailAmp provides full soft-error detection for address calculation while incurring an average overhead of around 5%.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3369381",
      "sourceUrl": "https://doi.org/10.1145/3369381",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Ganesh Gopalakrishnan",
        "Sriram Krishnamoorthy"
      ]
    },
    {
      "id": "openalex-w3000238554",
      "source": "openalex-discovery",
      "title": "FPChecker: Detecting Floating-Point Exceptions in GPU Applications",
      "authors": [
        {
          "name": "Ignacio Laguna",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Floating-point arithmetic is widely used in applications from several fields including scientific computing, machine learning, graphics, and finance. Many of these applications are rapidly adopting the use of GPUs to speedup computations. GPUs, however, have limited support to detect floating-point exceptions, which hinders the development of reliable applications in GPU-based systems. We present FPCHECKER, the first tool to automatically detect floating-point exceptions in GPU applications. FPCHECKER uses the clang/LLVM compiler to instrument GPU kernels and to detect exceptions at runtime. Once an exception is detected, it reports to the programmer the code location of the exception as well as other useful information. The programmer can then use this report to avoid the exception, e.g., by modifying the application algorithm or changing the input. We present the design of FPCHECKER, an evaluation of the overhead of the tool, and a real-world case scenario on which the tool is used to identify a hidden exception. The slowdown of FPCHECKER is moderate and the code is publicly available as open source.",
      "paperUrl": "https://doi.org/10.1109/ase.2019.00118",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "GPU"
      ],
      "matchedAuthors": [
        "Ignacio Laguna"
      ]
    },
    {
      "id": "openalex-w2916770803",
      "source": "openalex-discovery",
      "title": "Exploring an Alternative Cost Function for Combinatorial Register-Pressure-Aware Instruction Scheduling",
      "authors": [
        {
          "name": "Ghassan Shobaki",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Austin Kerbow",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "Christopher Pulido",
          "affiliation": "California State University, Sacramento"
        },
        {
          "name": "William D. Dobson",
          "affiliation": "California State University, Sacramento"
        }
      ],
      "year": "2019",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 16 (Issue 1)",
      "type": "research-paper",
      "abstract": "Multiple combinatorial algorithms have been proposed for doing pre-allocation instruction scheduling with the objective of minimizing register pressure or balancing register pressure and instruction-level parallelism. The cost function that is minimized in most of these algorithms is the peak register pressure (or the peak excess register pressure). In this work, we explore an alternative register-pressure cost function, which is the Sum of Live Interval Lengths (SLIL). Unlike the peak cost function, which captures register pressure only at the highest pressure point in the schedule, the proposed SLIL cost function captures register pressure at all points in the schedule. Minimizing register pressure at all points is desirable in larger scheduling regions with multiple high-pressure points. This article describes a Branch-and-Bound (B8B) algorithm for minimizing the SLIL cost function. The algorithm is based on two SLIL-specific dynamic lower bounds as well as the history utilization technique proposed in our previous work. The proposed algorithm is implemented into the LLVM Compiler and evaluated experimentally relative to our previously proposed B8B algorithm for minimizing the peak excess register pressure. The experimental results show that the proposed algorithm for minimizing the SLIL cost function produces substantially less spilling than the previous algorithm that minimizes the peak cost function. Execution-time results on various processors show that the proposed B8B algorithm significantly improves the performance of many CPU2006 benchmarks by up to 49% relative to LLVM's default scheduler. The geometric-mean improvement for FP2006 on Intel Core i7 is 4.22%.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3301489",
      "sourceUrl": "https://doi.org/10.1145/3301489",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Ghassan Shobaki"
      ]
    },
    {
      "id": "openalex-w2899856510",
      "source": "openalex-discovery",
      "title": "Exploring C semantics and pointer provenance",
      "authors": [
        {
          "name": "Kayvan Memarian",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Victor B. F. Gomes",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Brooks Davis",
          "affiliation": "SRI International"
        },
        {
          "name": "Stephen Kell",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Alexander Richardson",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Robert N. M. Watson",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Peter Sewell",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 3 (Issue POPL)",
      "type": "research-paper",
      "abstract": "The semantics of pointers and memory objects in C has been a vexed question for many years. C values cannot be treated as either purely abstract or purely concrete entities: the language exposes their representations, but compiler optimisations rely on analyses that reason about provenance and initialisation status, not just runtime representations. The ISO WG14 standard leaves much of this unclear, and in some respects differs with de facto standard usage --- which itself is difficult to investigate. In this paper we explore the possible source-language semantics for memory objects and pointers, in ISO C and in C as it is used and implemented in practice, focussing especially on pointer provenance. We aim to, as far as possible, reconcile the ISO C standard, mainstream compiler behaviour, and the semantics relied on by the corpus of existing C code. We present two coherent proposals, tracking provenance via integers and not; both address many design questions. We highlight some pros and cons and open questions, and illustrate the discussion with a library of test cases. We make our semantics executable as a test oracle, integrating it with the Cerberus semantics for much of the rest of C, which we have made substantially more complete and robust, and equipped with a web-interface GUI. This allows us to experimentally assess our proposals on those test cases. To assess their viability with respect to larger bodies of C code, we analyse the changes required and the resulting behaviour for a port of FreeBSD to CHERI, a research architecture supporting hardware capabilities, which (roughly speaking) traps on the memory safety violations which our proposals deem undefined behaviour. We also develop a new runtime instrumentation tool to detect possible provenance violations in normal C code, and apply it to some of the SPEC benchmarks. We compare our proposal with a source-language variant of the twin-allocation LLVM semantics proposal of Lee et al. Finally, we describe ongoing interactions with WG14, exploring how our proposals could be incorporated into the ISO standard.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3290380",
      "sourceUrl": "https://doi.org/10.1145/3290380",
      "tags": [],
      "matchedAuthors": [
        "Alexander Richardson",
        "Brooks Davis",
        "Robert N. M. Watson"
      ]
    },
    {
      "id": "openalex-w2964162716",
      "source": "openalex-discovery",
      "title": "Devise Rust Compiler Optimizations on RISC-V Architectures with SIMD Instructions",
      "authors": [
        {
          "name": "Heng Lin",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Piyo Chen",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Shin Hwang",
          "affiliation": "National Taiwan University of Science and Technology"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Recently, Rust has become a popular system programming language and been widely used in microkernel OS designs, cryptocurrency designs, deep learning applications, and web browsers. Rust is designed for highly safe and concurrent systems and provides similar performance to C++. In the architecture work, RISC-V is with an open instruction set architecture in growing use due to its performance, power efficiency, and open architectures. Extension of ISAs in RISC-V is another advantage. Venders can select the extension ISAs to make the design more flexible. There is SIMD extension ISAs called \"V\" extension in RISC-V. However, currently Rust was not well-supported on RISC-V platforms. In this paper, we describe how to enable the Rust flow and framework based on LLVM infrastructure for RISC-V architectures with SIMD V extensions. For the basic infrastructure support, we enable two flows for RISC-V architecture. In addition, we also enable RISC-V SIMD instructions for Rust SIMD vector with LLVM support. In the experiment, we illustrate our code generation for Blas style matrix and vector computations for Rust on RISC-V V instructions. The experiment is run on the spike simulator which we implement the V extension for RISC-V architectures.",
      "paperUrl": "https://doi.org/10.1145/3339186.3339193",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Infrastructure",
        "Optimizations",
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w4287997381",
      "source": "openalex-discovery",
      "title": "Deep Learning-based Hybrid Graph-Coloring Algorithm for Register\\n Allocation",
      "authors": [
        {
          "name": "Dibyendu Das",
          "affiliation": ""
        },
        {
          "name": "Shahid Asghar Ahmad",
          "affiliation": ""
        },
        {
          "name": "Kumar Venkataramanan",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Register allocation, which is a crucial phase of a good optimizing compiler,\\nrelies on graph coloring. Hence, an efficient graph coloring algorithm is of\\nparamount importance. In this work we try to learn a good heuristic for\\ncoloring interference graphs that are used in the register allocation phase. We\\naim to handle moderate sized interference graphs which have 100 nodes or less.\\nFor such graphs we can get the optimal allocation of colors to the nodes. Such\\noptimal coloring is then used to train our Deep Learning network which is based\\non several layers of LSTM that output a color for each node of the graph.\\nHowever, the current network may allocate the same color to the nodes connected\\nby an edge resulting in an invalid coloring of the interference graph. Since it\\nis difficult to encode constraints in an LSTM to avoid invalid coloring, we\\naugment our deep learning network with a color correction phase that runs after\\nthe colors have been allocated by the network. Thus, our algorithm is hybrid in\\nnature consisting of a mix of a deep learning algorithm followed by a more\\ntraditional correction phase. We have trained our network using several\\nthousand random graphs of varying sparsity. On application of our hybrid\\nalgorithm to various popular graphs found in literature we see that our\\nalgorithm does very well when compared to the optimal coloring of these graphs.\\nWe have also run our algorithm against LLVMs popular greedy register allocator\\nfor several SPEC CPU 2017 benchmarks and notice that the hybrid algorithm\\nperforms on par or better than such a well-tuned allocator for most of these\\nbenchmarks.\\n",
      "paperUrl": "https://arxiv.org/pdf/1912.03700",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1912.03700",
      "tags": [],
      "matchedAuthors": [
        "Dibyendu Das"
      ]
    },
    {
      "id": "openalex-w2992281624",
      "source": "openalex-discovery",
      "title": "Deep Learning-based Hybrid Graph-Coloring Algorithm for Register Allocation",
      "authors": [
        {
          "name": "Dibyendu Das",
          "affiliation": ""
        },
        {
          "name": "Shahid Asghar Ahmad",
          "affiliation": "Advanced Micro Devices (Canada)"
        },
        {
          "name": "Kumar Venkataramanan",
          "affiliation": "Advanced Micro Devices (Canada)"
        }
      ],
      "year": "2019",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Register allocation, which is a crucial phase of a good optimizing compiler, relies on graph coloring. Hence, an efficient graph coloring algorithm is of paramount importance. In this work we try to learn a good heuristic for coloring interference graphs that are used in the register allocation phase. We aim to handle moderate sized interference graphs which have 100 nodes or less. For such graphs we can get the optimal allocation of colors to the nodes. Such optimal coloring is then used to train our Deep Learning network which is based on several layers of LSTM that output a color for each node of the graph. However, the current network may allocate the same color to the nodes connected by an edge resulting in an invalid coloring of the interference graph. Since it is difficult to encode constraints in an LSTM to avoid invalid coloring, we augment our deep learning network with a color correction phase that runs after the colors have been allocated by the network. Thus, our algorithm is hybrid in nature consisting of a mix of a deep learning algorithm followed by a more traditional correction phase. We have trained our network using several thousand random graphs of varying sparsity. On application of our hybrid algorithm to various popular graphs found in literature we see that our algorithm does very well when compared to the optimal coloring of these graphs. We have also run our algorithm against LLVMs popular greedy register allocator for several SPEC CPU 2017 benchmarks and notice that the hybrid algorithm performs on par or better than such a well-tuned allocator for most of these benchmarks.",
      "paperUrl": "https://arxiv.org/pdf/1912.03700",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1912.03700",
      "tags": [],
      "matchedAuthors": [
        "Dibyendu Das"
      ]
    },
    {
      "id": "openalex-w3006559397",
      "source": "openalex-discovery",
      "title": "Compiler-Assisted Selection of Hardware Acceleration Candidates from Application Source Code",
      "authors": [
        {
          "name": "Georgios Zacharopoulos",
          "affiliation": "Università della Svizzera italiana"
        },
        {
          "name": "Lorenzo Ferretti",
          "affiliation": "Università della Svizzera italiana"
        },
        {
          "name": "Giovanni Ansaloni",
          "affiliation": "Università della Svizzera italiana"
        },
        {
          "name": "Giuseppe Di Guglielmo",
          "affiliation": "Columbia University"
        },
        {
          "name": "Luca P. Carloni",
          "affiliation": "Columbia University"
        },
        {
          "name": "Laura Pozzi",
          "affiliation": "Università della Svizzera italiana"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Hardware design is a difficult task. Beside ensuring functional correctness of an implementation, hardware developers are confronted with multiple and often conflicting constraints, such as performance and area cost targets, that require lengthy explorations. This issue is compounded when considering the acceleration of complex applications, of which some parts are implemented in software, and others are accelerated in hardware. Hardware/Software partitioning must be settled early in the development cycle, and is far from trivial, since at this stage detailed performance measurements are not available, while wrong choices can lead to vastly sub-optimal solutions or to wasted implementation efforts. To address this challenge, we present a framework to automatically identify, from un-modified software code, software segments that are promising candidates for hardware acceleration, to evaluate their potential speedup and resource requirements, and to select a subset of them under resource constraint. Our strategy is based on Intermediate Representation (IR) analysis passes, which we embed in the LLVM compiler toolchain, and does not require any time-consuming synthesis. We explore its effectiveness on the reference software implementation of a complex application, the H.264 Decoder from University of Illinois, and demonstrate that our methodology selects higher-performance sets of accelerators, when compared to strategies only based on profiling information.",
      "paperUrl": "https://doi.org/10.1109/iccd46524.2019.00024",
      "sourceUrl": "",
      "tags": [
        "IR",
        "Performance"
      ],
      "matchedAuthors": [
        "Georgios Zacharopoulos",
        "Giovanni Ansaloni",
        "Laura Pozzi"
      ]
    },
    {
      "id": "openalex-w3126096841",
      "source": "openalex-discovery",
      "title": "Compiler fuzzing: how much does it matter?",
      "authors": [
        {
          "name": "Michaël Marcozzi",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Qiyi Tang",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Cristian Cadar",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 3 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Despite much recent interest in randomised testing (fuzzing) of compilers, the practical impact of fuzzer-found compiler bugs on real-world applications has barely been assessed. We present the first quantitative and qualitative study of the tangible impact of miscompilation bugs in a mature compiler. We follow a rigorous methodology where the bug impact over the compiled application is evaluated based on (1) whether the bug appears to trigger during compilation; (2) the extent to which generated assembly code changes syntactically due to triggering of the bug; and (3) whether such changes cause regression test suite failures, or whether we can manually find application inputs that trigger execution divergence due to such changes. The study is conducted with respect to the compilation of more than 10 million lines of C/C++ code from 309 Debian packages, using 12% of the historical and now fixed miscompilation bugs found by four state-of-the-art fuzzers in the Clang/LLVM compiler, as well as 18 bugs found by human users compiling real code or as a by-product of formal verification efforts. The results show that almost half of the fuzzer-found bugs propagate to the generated binaries for at least one package, in which case only a very small part of the binary is typically affected, yet causing two failures when running the test suites of all the impacted packages. User-reported and formal verification bugs do not exhibit a higher impact, with a lower rate of triggered bugs and one test failure. The manual analysis of a selection of the syntactic changes caused by some of our bugs (fuzzer-found and non fuzzer-found) in package assembly code, shows that either these changes have no semantic impact or that they would require very specific runtime circumstances to trigger execution divergence.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3360581",
      "sourceUrl": "https://doi.org/10.1145/3360581",
      "tags": [
        "C++",
        "Clang",
        "Testing"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson",
        "Cristian Cadar"
      ]
    },
    {
      "id": "openalex-w2795671832",
      "source": "openalex-discovery",
      "title": "Combinatorial Register Allocation and Instruction Scheduling",
      "authors": [
        {
          "name": "Roberto Castañeda Lozano",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Mats Carlsson",
          "affiliation": "RISE Research Institutes of Sweden"
        },
        {
          "name": "Gabriel Hjort Blindell",
          "affiliation": "KTH Royal Institute of Technology"
        },
        {
          "name": "Christian Schulte",
          "affiliation": "KTH Royal Institute of Technology"
        }
      ],
      "year": "2019",
      "venue": "ACM Transactions on Programming Languages and Systems | Vol. 41 (Issue 3)",
      "type": "research-paper",
      "abstract": "This article introduces a combinatorial optimization approach to register allocation and instruction scheduling, two central compiler problems. Combinatorial optimization has the potential to solve these problems optimally and to exploit processor-specific features readily. Our approach is the first to leverage this potential in practice : it captures the complete set of program transformations used in state-of-the-art compilers, scales to medium-sized functions of up to 1,000 instructions, and generates executable code. This level of practicality is reached by using constraint programming, a particularly suitable combinatorial optimization technique. Unison, the implementation of our approach, is open source, used in industry, and integrated with the LLVM toolchain. An extensive evaluation confirms that Unison generates better code than LLVM while scaling to medium-sized functions. The evaluation uses systematically selected benchmarks from MediaBench and SPEC CPU2006 and different processor architectures (Hexagon, ARM, MIPS). Mean estimated speedup ranges from 1.1% to 10% and mean code size reduction ranges from 1.3% to 3.8% for the different architectures. A significant part of this improvement is due to the integrated nature of the approach. Executing the generated code on Hexagon confirms that the estimated speedup results in actual speedup. Given a fixed time limit, Unison solves optimally functions of up to 946 instructions, nearly an order of magnitude larger than previous approaches. The results show that our combinatorial approach can be applied in practice to trade compilation time for code quality beyond the usual compiler optimization levels, identify improvement opportunities in heuristic algorithms, and fully exploit processor-specific features.",
      "paperUrl": "https://arxiv.org/pdf/1804.02452",
      "sourceUrl": "https://doi.org/10.1145/3332373",
      "tags": [],
      "matchedAuthors": [
        "Christian Schulte",
        "Gabriel Hjort Blindell",
        "Mats Carlsson",
        "Roberto Castañeda Lozano"
      ]
    },
    {
      "id": "openalex-w2940374126",
      "source": "openalex-discovery",
      "title": "ClangJIT: Enhancing C++ with Just-in-Time Compilation",
      "authors": [
        {
          "name": "Hal Finkel",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "David Poliakoff",
          "affiliation": "Lawrence Livermore National Laboratory"
        },
        {
          "name": "David F. Richards",
          "affiliation": "Lawrence Livermore National Laboratory"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The C++ programming language is not only a keystone of the high-performance-computing ecosystem but has proven to be a successful base for portable parallel-programming frameworks. As is well known, C++ programmers use templates to specialize algorithms, thus allowing the compiler to generate highly-efficient code for specific parameters, data structures, and so on. This capability has been limited to those specializations that can be identified when the application is compiled, and in many critical cases, compiling all potentially-relevant specializations is not practical. ClangJIT provides a well-integrated C++ language extension allowing template-based specialization to occur during program execution. This capability has been implemented for use in large-scale applications, and we demonstrate that just-in-time-compilation-based dynamic specialization can be integrated into applications, often requiring minimal changes (or no changes) to the applications themselves, providing significant performance improvements, programmer-productivity improvements, and decreased compilation time.",
      "paperUrl": "https://arxiv.org/pdf/1904.08555",
      "sourceUrl": "https://doi.org/10.1109/p3hpc49587.2019.00013",
      "tags": [
        "C++",
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Hal Finkel"
      ]
    },
    {
      "id": "openalex-w3106731480",
      "source": "openalex-discovery",
      "title": "Beyond debug information: Improving program reconstruction in LLDB using C++ modules",
      "authors": [
        {
          "name": "Raphael Isemann",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://odr.chalmers.se/bitstream/20.500.12380/300037/1/CSE%2019-49%20Isemann.pdf",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Debug Information",
        "LLDB"
      ],
      "matchedAuthors": [
        "Raphael Isemann"
      ]
    },
    {
      "id": "openalex-w2952416601",
      "source": "openalex-discovery",
      "title": "BOLT: A Practical Binary Optimizer for Data Centers and Beyond",
      "authors": [
        {
          "name": "Maksim Panchenko",
          "affiliation": "Menlo School"
        },
        {
          "name": "Rafael Auler",
          "affiliation": "Menlo School"
        },
        {
          "name": "Bill Nell",
          "affiliation": "Meta (United States)"
        },
        {
          "name": "Guilherme Ottoni",
          "affiliation": "Menlo School"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Performance optimization for large-scale applications has recently become more important as computation continues to move towards data centers. Data-center applications are generally very large and complex, which makes code layout an important optimization to improve their performance. This has motivated recent investigation of practical techniques to improve code layout at both compile time and link time. Although post-link optimizers had some success in the past, no recent work has explored their benefits in the context of modern data-center applications. In this paper, we present BOLT, an open-source post-link optimizer built on top of the LLVM framework. Utilizing sample-based profiling, BOLT boosts the performance of real-world applications even for highly optimized binaries built with both feedback-driven optimizations (FDO) and link-time optimizations (LTO). We demonstrate that post-link performance improvements are complementary to conventional compiler optimizations, even when the latter are done at a whole-program level and in the presence of profile information. We evaluated BOLT on both Facebook data-center workloads and open-source compilers. For data-center applications, BOLT achieves up to 7.0% performance speedups on top of profile-guided function reordering and LTO. For the GCC and Clang compilers, our evaluation shows that BOLT speeds up their binaries by up to 20.4% on top of FDO and LTO, and up to 52.1% if the binaries are built without FDO and LTO.",
      "paperUrl": "https://doi.org/10.1109/cgo.2019.8661201",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "LTO",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Maksim Panchenko"
      ]
    },
    {
      "id": "openalex-w3011000655",
      "source": "openalex-discovery",
      "title": "BHive: A Benchmark Suite and Measurement Framework for Validating x86-64 Basic Block Performance Models",
      "authors": [
        {
          "name": "Yishen Chen",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Ajay Brahmakshatriya",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Charith Mendis",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Alex Renda",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Eric Atkinson",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Ondřej Sýkora",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Saman Amarasinghe",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Michael Carbin",
          "affiliation": "Massachusetts Institute of Technology"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Compilers and performance engineers use hardware performance models to simplify program optimizations. Performance models provide a necessary abstraction over complex modern processors. However, constructing and maintaining a performance model can be onerous, given the numerous microarchi-tectural optimizations employed by modern processors. Despite their complexity and reported inaccuracy (e.g., deviating from native measurement by more than 30%), existing performance models-such as IACA and llvm-mca-have not been systematically validated, because there is no scalable machine code profiler that can automatically obtain throughput of arbitrary basic blocks while conforming to common modeling assumptions. In this paper, we present a novel profiler that can profile arbitrary memory-accessing basic blocks without any user intervention. We used this profiler to build BHive, a benchmark for systematic validation of performance models of x86-64 basic blocks. We used BHive to evaluate four existing performance models: IACA, llvm-mca, Ithemal, and OSACA. We automatically cluster basic blocks in the benchmark suite based on their utilization of CPU resources. Using this clustering, our benchmark can give a detailed analysis of a performance model's strengths and weaknesses on different workloads (e.g., vectorized vs. scalar basic blocks). We additionally demonstrate that our dataset well captures basic properties of two Google applications: Spanner and Dremel.",
      "paperUrl": "https://doi.org/10.1109/iiswc47752.2019.9042166",
      "sourceUrl": "",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Saman Amarasinghe",
        "Yishen Chen"
      ]
    },
    {
      "id": "openalex-w2951841630",
      "source": "openalex-discovery",
      "title": "AutoPhase: Compiler Phase-Ordering for HLS with Deep Reinforcement Learning",
      "authors": [
        {
          "name": "Qijing Huang",
          "affiliation": "Universidad Técnica de Ambato"
        },
        {
          "name": "Ameer Haj-Ali",
          "affiliation": "Universidad Técnica de Ambato"
        },
        {
          "name": "William S. Moses",
          "affiliation": "Universidad Técnica de Ambato"
        },
        {
          "name": "John Xiang",
          "affiliation": "Universidad Técnica de Ambato"
        },
        {
          "name": "Ion Stoica",
          "affiliation": "Centro Regional de Derechos Humanos y Justicia de Género, Corporación Humanas"
        },
        {
          "name": "Krste Asanović",
          "affiliation": "University of California, Berkeley"
        },
        {
          "name": "John Wawrzynek",
          "affiliation": "University of California, Berkeley"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The performance of the code generated by a compiler depends on the order in which the optimization passes are applied. In high-level synthesis, the quality of the generated circuit relates directly to the code generated by the front-end compiler. Choosing a good order-often referred to as the phase-ordering problem-is an NP-hard problem. In this paper, we evaluate a new technique to address the phase-ordering problem: deep reinforcement learning. We implement a framework in the context of the LLVM compiler to optimize the ordering for HLS programs and compare the performance of deep reinforcement learning to state-of-the-art algorithms that address the phase-ordering problem. Overall, our framework runs one to two orders of magnitude faster than these algorithms, and achieves a 16% improvement in circuit performance over the -O3 compiler flag.",
      "paperUrl": "https://doi.org/10.1109/fccm.2019.00049",
      "sourceUrl": "",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w3211126531",
      "source": "openalex-discovery",
      "title": "Artifact for BOLT: A Practical Binary Optimizer for Data Centers and Beyond",
      "authors": [
        {
          "name": "Maksim Panchenko",
          "affiliation": "Meta (Israel)"
        },
        {
          "name": "Rafael Auler",
          "affiliation": "Meta (Israel)"
        },
        {
          "name": "Bill Nell",
          "affiliation": "Meta (Israel)"
        },
        {
          "name": "Guilherme Ottoni",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "Figshare | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This artifact contains the software required to reproduce the experimental findings of the paper \"BOLT: A Practical Binary Optimizer for Data Centers and Beyond\", CGO 2019. The open-source workload evaluated on this paper is clang 7 and gcc 8.2. Our goal is to demonstrate that building clang/gcc with all optimizations, including LTO and PGO, still leaves opportunities for a post-link optimizer such as BOLT to do a better job at basic block placement and function reordering, significantly improving workload performance. The paper advocates for a two-step profiling pipeline (PGO and BOLT) evaluated on a data-center environment, showing that doing a single pass of profile collection is not enough to leverage the full potential of profile-guide optimizations. In this two-step approach, PGO or AutoFDO can be used to feed the compiler with profile information, which is an important enabler of better inlining decisions, while a second pass is used to collect profile for a post-link optimizer such as BOLT, enabling us to improve basic block order and function order in the final binary and further increase performance.",
      "paperUrl": "https://doi.org/10.5281/zenodo.2542117",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "LTO",
        "Optimizations",
        "Performance",
        "PGO"
      ],
      "matchedAuthors": [
        "Maksim Panchenko"
      ]
    },
    {
      "id": "openalex-w2978627403",
      "source": "openalex-discovery",
      "title": "Analyzing control flow integrity with LLVM-CFI",
      "authors": [
        {
          "name": "Paul Muntean",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Matthias Neumayer",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Zhiqiang Lin",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Gang Tan",
          "affiliation": "Pennsylvania State University"
        },
        {
          "name": "Jens Großklags",
          "affiliation": "Technical University of Munich"
        },
        {
          "name": "Claudia Eckert",
          "affiliation": "Technical University of Munich"
        }
      ],
      "year": "2019",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Control-flow hijacking attacks are used to perform malicious com-putations.\\nCurrent solutions for assessing the attack surface afteracontrol flow\\nintegrity(CFI) policy was applied can measure onlyindirect transfer averages in\\nthe best case without providing anyinsights w.r.t. the absolute calltarget\\nreduction per callsite, and gad-get availability. Further, tool comparison is\\nunderdeveloped or notpossible at all. CFI has proven to be one of the most\\npromising pro-tections against control flow hijacking attacks, thus many\\neffortshave been made to improve CFI in various ways. However, there isa lack\\nof systematic assessment of existing CFI protections. In this paper, we\\npresentLLVM-CFI, a static source code analy-sis framework for analyzing\\nstate-of-the-art static CFI protectionsbased on the Clang/LLVM compiler\\nframework.LLVM-CFIworksby precisely modeling a CFI policy and then evaluating\\nit within aunified approach.LLVM-CFIhelps determine the level of\\nsecurityoffered by different CFI protections, after the CFI protections\\nweredeployed, thus providing an important step towards exploit\\ncre-ation/prevention and stronger defenses. We have usedLLVM-CFIto assess eight\\nstate-of-the-art static CFI defenses on real-worldprograms such as Google\\nChrome and Apache Httpd.LLVM-CFIprovides a precise analysis of the residual\\nattack surfaces, andaccordingly ranks CFI policies against each\\nother.LLVM-CFIalsosuccessfully paves the way towards construction of COOP-like\\ncodereuse attacks and elimination of the remaining attack surface bydisclosing\\nprotected calltargets under eight restrictive CFI policies.\\n",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3359789.3359806",
      "sourceUrl": "https://doi.org/10.1145/3359789.3359806",
      "tags": [
        "Clang",
        "Security"
      ],
      "matchedAuthors": [
        "Gang Tan"
      ]
    },
    {
      "id": "openalex-w2995914412",
      "source": "openalex-discovery",
      "title": "An open-source solution to performance portability for Summit and Sierra supercomputers",
      "authors": [
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": ""
        },
        {
          "name": "Alexey Bataev",
          "affiliation": ""
        },
        {
          "name": "Alexandre E. Eichenberger",
          "affiliation": ""
        },
        {
          "name": "Carlo Bertolli",
          "affiliation": ""
        },
        {
          "name": "John K. P. O'Brien",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "IBM Journal of Research and Development | Vol. 64 (Issue 3/4)",
      "type": "research-paper",
      "abstract": "Programming models that use a higher level of abstraction to express parallelism can target both CPUs and any attached devices, alleviating the maintainability and portability concerns facing today's heterogenous systems. This article describes the design, implementation, and delivery of a compliant OpenMP device offloading implementation for IBM-NVIDIA heterogeneous servers composing the Summit and Sierra supercomputers in the mainline open-source Clang/LLVM compiler and OpenMP runtime projects. From a performance perspective, reconciling the GPU programming model, best suited for massively parallel workloads, with the generality of the OpenMP model was a significant challenge. To achieve both high performance and full portability, we map high-level programming patterns to fine-tuned code generation schemes and customized runtimes that preserve the OpenMP semantics. In the compiler, we implement a low-overhead single-program multiple-data scheme that leverages the GPU native execution model and a fallback scheme to support the generality of OpenMP. Modular design enables the implementation to be extended with new schemes for frequently occurring patterns. Our implementation relies on key optimizations: sharing data among threads, leveraging unified memory, aggressive inlining of runtime calls, memory coalescing, and runtime simplification. We show that for commonly used patterns, performance on the Summit and Sierra GPUs matches that of hand-written native CUDA code.",
      "paperUrl": "https://doi.org/10.1147/jrd.2019.2955944",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "CUDA",
        "GPU",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexey Bataev",
        "Carlo Bertolli"
      ]
    },
    {
      "id": "openalex-w2948481461",
      "source": "openalex-discovery",
      "title": "An analysis of executable size reduction by LLVM passes",
      "authors": [
        {
          "name": "Shalini Jain",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Utpal Bora",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Prateek Kumar",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Vaibhav B. Sinha",
          "affiliation": "Indian Institute of Technology Hyderabad"
        },
        {
          "name": "Suresh Purini",
          "affiliation": "International Institute of Information Technology, Hyderabad"
        },
        {
          "name": "Ramakrishna Upadrasta",
          "affiliation": "Indian Institute of Technology Hyderabad"
        }
      ],
      "year": "2019",
      "venue": "CSI Transactions on ICT | Vol. 7 (Issue 2)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/s40012-019-00248-5",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Ramakrishna Upadrasta",
        "Shalini Jain",
        "Suresh Purini",
        "Utpal Bora"
      ]
    },
    {
      "id": "openalex-w2972314342",
      "source": "openalex-discovery",
      "title": "Alpaca: Intermittent Execution without Checkpoints",
      "authors": [
        {
          "name": "Kiwan Maeng",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Alexei Colin",
          "affiliation": "Carnegie Mellon University"
        },
        {
          "name": "Brandon Lucia",
          "affiliation": "Carnegie Mellon University"
        }
      ],
      "year": "2019",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The emergence of energy harvesting devices creates the potential for batteryless sensing and computing devices. Such devices operate only intermittently, as energy is available, presenting a number of challenges for software developers. Programmers face a complex design space requiring reasoning about energy, memory consistency, and forward progress. This paper introduces Alpaca, a low-overhead programming model for intermittent computing on energy-harvesting devices. Alpaca programs are composed of a sequence of user-defined tasks. The Alpaca runtime preserves execution progress at the granularity of a task. The key insight in Alpaca is the privatization of data shared between tasks. Updates of shared values in a task are privatized and only committed to main memory on successful execution of the task, ensuring that data remain consistent despite power failures. Alpaca provides a familiar programming interface and a highly efficient runtime model. We also present an alternate version of Alpaca, Alpaca-undo, that uses undo-logging and rollback instead of privatization and commit. We implemented a prototype of both versions of Alpaca as an extension to C with an LLVM compiler pass. We evaluated Alpaca, and directly compared to three systems from prior work. Alpacaconsistently improves performance compared to the previous systems, by up to 23.8x, while also improving memory footprint in many cases, by up to 17.6x.",
      "paperUrl": "https://arxiv.org/pdf/1909.06951",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1909.06951",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Brandon Lucia"
      ]
    },
    {
      "id": "openalex-w2915190469",
      "source": "openalex-discovery",
      "title": "A Systematic Impact Study for Fuzzer-Found Compiler Bugs",
      "authors": [
        {
          "name": "Michaël Marcozzi",
          "affiliation": ""
        },
        {
          "name": "Qiyi Tang",
          "affiliation": ""
        },
        {
          "name": "Alastair F. Donaldson",
          "affiliation": ""
        },
        {
          "name": "Cristian Cadar",
          "affiliation": ""
        }
      ],
      "year": "2019",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Despite much recent interest in compiler randomized testing (fuzzing), the practical impact of fuzzer-found compiler bugs on real-world applications has barely been assessed. We present the first quantitative and qualitative study of the tangible impact of miscompilation bugs in a mature compiler. We follow a rigorous methodology where the bug impact over the compiled application is evaluated based on (1) whether the bug appears to trigger during compilation; (2) the extent to which generated assembly code changes syntactically due to triggering of the bug; and (3) how much such changes do cause regression test suite failures and could be used to manually trigger divergences during execution. The study is conducted with respect to the compilation of more than 10 million lines of C/C++ code from 309 Debian packages, using 12% of the historical and now fixed miscompilation bugs found by four state-of-the-art fuzzers in the Clang/LLVM compiler, as well as 18 bugs found by human users compiling real code or by formal verification. The results show that almost half of the fuzzer-found bugs propagate to the generated binaries for some packages, but rarely affect their syntax and cause two failures in total when running their test suites. User-reported and formal verification bugs do not exhibit a higher impact, with less frequently triggered bugs and one test failure. Our manual analysis of a selection of bugs, either fuzzer-found or not, suggests that none can easily trigger a runtime divergence on the packages considered in the analysis, and that in general they affect only corner cases.",
      "paperUrl": "https://arxiv.org/pdf/1902.09334",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1902.09334",
      "tags": [
        "C++",
        "Clang",
        "Testing"
      ],
      "matchedAuthors": [
        "Alastair F. Donaldson",
        "Cristian Cadar"
      ]
    },
    {
      "id": "openalex-w2900545798",
      "source": "openalex-discovery",
      "title": "[Engineering Paper] Challenges of Implementing Cross Translation Unit Analysis in Clang Static Analyzer",
      "authors": [
        {
          "name": "Gábor Horváth",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Péter Szécsi",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Gera",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Dániel Krupp",
          "affiliation": "Ericsson (Hungary)"
        },
        {
          "name": "Norbert Pataki",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Static analysis is a great approach to find bugs and code smells. Some of the errors span across multiple translation units. Unfortunately, separate compilation makes cross translation unit analysis challenging for C family languages. In this paper, we describe a model and an implementation for cross translation unit symbolic execution for C family languages. We were able to extend the scope of the analysis without modifying any of the existing checkers. The analysis is implemented in the open source Clang compiler. We also measured the performance of the approach and the quality of the reports. The solution proved to be scalable to large codebases and the number of findings increased significantly for the evaluated projects. The implementation is already accepted into mainline Clang.",
      "paperUrl": "https://doi.org/10.1109/scam.2018.00027",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Dániel Krupp"
      ]
    },
    {
      "id": "openalex-w2860564492",
      "source": "openalex-discovery",
      "title": "What You Get is What You C: Controlling Side Effects in Mainstream C Compilers",
      "authors": [
        {
          "name": "Laurent Simon",
          "affiliation": "Samsung (United States)"
        },
        {
          "name": "David Chisnall",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Ross Anderson",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Security engineers have been fighting with C compilers for years. A careful programmer would test for null pointer dereferencing or division by zero; but the compiler would fail to understand, and optimize the test away. Modern compilers now have dedicated options to mitigate this. But when a programmer tries to control side effects of code, such as to make a cryptographic algorithm execute in constant time, the problem remains. Programmers devise complex tricks to obscure their intentions, but compiler writers find ever smarter ways to optimize code. A compiler upgrade can suddenly and without warning open a timing channel in previously secure code. This arms race is pointless and has to stop. We argue that we must stop fighting the compiler, and instead make it our ally. As a starting point, we analyze the ways in which compiler optimization breaks implicit properties of crypto code; and add guarantees for two of these properties in Clang/LLVM. Our work explores what is actually involved in controlling side effects on modern CPUs with a standard toolchain. Similar techniques can and should be applied to other security properties; achieving intentions by compiler commands or annotations makes them explicit, so we can reason about them. It is already understood that explicitness is essential for cryptographic protocol security and for compiler performance; it is essential for language security too. We therefore argue that this should be only the first step in a sustained engineering effort.",
      "paperUrl": "https://doi.org/10.1109/eurosp.2018.00009",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "David Chisnall"
      ]
    },
    {
      "id": "openalex-w2898969787",
      "source": "openalex-discovery",
      "title": "User-Directed Loop-Transformations in Clang",
      "authors": [
        {
          "name": "Michael Kruse",
          "affiliation": "Argonne National Laboratory"
        },
        {
          "name": "Hal Finkel",
          "affiliation": "Argonne Leadership Computing Facility"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Directives for the compiler such as pragmas can help programmers to separate an algorithm's semantics from its optimization. This keeps the code understandable and easier to optimize for different platforms. Simple transformations such as loop unrolling are already implemented in most mainstream compilers. We recently submitted a proposal to add generalized loop transformations to the OpenMP standard. We are also working on an implementation in LLVM/Clang/Polly to show its feasibility and usefulness. The current prototype allows applying patterns common to matrix-matrix multiplication optimizations.",
      "paperUrl": "https://arxiv.org/pdf/1811.00624",
      "sourceUrl": "https://doi.org/10.1109/llvm-hpc.2018.8639402",
      "tags": [
        "Clang",
        "Loop transformations",
        "Optimizations",
        "Polly"
      ],
      "matchedAuthors": [
        "Hal Finkel",
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w2889582214",
      "source": "openalex-discovery",
      "title": "Towards Verified, Constant-time Floating Point Operations",
      "authors": [
        {
          "name": "Marc Andrysco",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Andres Nötzli",
          "affiliation": "Stanford University"
        },
        {
          "name": "Fraser Brown",
          "affiliation": "Stanford University"
        },
        {
          "name": "Ranjit Jhala",
          "affiliation": "University of California, San Diego"
        },
        {
          "name": "Deian Stefan",
          "affiliation": "University of California, San Diego"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The runtimes of certain floating-point instructions can vary up to two orders of magnitude with instruction operands, allowing attackers to break security and privacy guarantees of real systems (\\eg browsers). To prevent attacks due to such floating-point timing channels, we introduce CTFP, an efficient, machine-checked, and extensible system that transforms unsafe floating-point operations into safe, constant-time computations. CTFP relies on two observations. First, that it is possible to execute floating-point computations in constant-time by emulating them in software; and second, that most security critical applications do not require full IEEE-754 floating-point precision. We use these observations to: eliminate certain classes of dangerous values from ever reaching floating-point hardware; emulate floating-point operations on dangerous values when eliminating them would severely alter application semantics; and, leverage fast floating-point hardware when it is safe to do so. We implement the constant-time transformations with our own domain-specific language that produces LLVM bitcode. Since the transformations themselves equate to bit surgery on already complicated floating-point arithmetic, we use a satisfiability modulo theories (SMT) solver to ensure that their behavior fits our specifications. Finally, we find that CTFP neither breaks real world applications nor incurs overwhelming overhead.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3243734.3243766",
      "sourceUrl": "https://doi.org/10.1145/3243734.3243766",
      "tags": [
        "Security"
      ],
      "matchedAuthors": [
        "Deian Stefan",
        "Fraser Brown",
        "Ranjit Jhala"
      ]
    },
    {
      "id": "openalex-w2883365010",
      "source": "openalex-discovery",
      "title": "The codecompass comprehension framework",
      "authors": [
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Tibor Brunner",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "CodeCompass is an open source LLVM/Clang based tool developed by Ericsson Ltd. and the Eötvös Loránd University, Budapest to help understanding large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and the virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithm are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass is not restricted to the source code. It also utilizes build information to explore the system architecture as well as version control information e.g. git commit history and blame view. Clang based static analysis results are also integrated to CodeCompass. Although the tool focuses mainly on C and C++, it also supports Java and Python languages.",
      "paperUrl": "https://doi.org/10.1145/3196321.3196352",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Tibor Brunner",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2803281868",
      "source": "openalex-discovery",
      "title": "Support OpenCL 2.0 Compiler on LLVM for PTX Simulators",
      "authors": [
        {
          "name": "Chun‐Chieh Yang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Shao-Chung Wang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Min-Yi Hsu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Ming Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Shin Hwang",
          "affiliation": "National Taiwan University of Science and Technology"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2018",
      "venue": "Journal of Signal Processing Systems | Vol. 91 (Issue 3-4)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/s11265-018-1377-4",
      "sourceUrl": "",
      "tags": [
        "OpenCL"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w2898255221",
      "source": "openalex-discovery",
      "title": "Software multiplexing: share your libraries and statically link them too",
      "authors": [
        {
          "name": "Will Dietz",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Vikram Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 2 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "We describe a compiler strategy we call “ Software Multiplexing ” that achieves many benefits of both statically linked and dynamically linked libraries, and adds some additional advantages. Specifically, it achieves the code size benefits of dynamically linked libraries while eliminating the major disadvantages: unexpected failures due to missing dependences, slow startup times, reduced execution performance due to indirect references to globals, and the potential for security vulnerabilities. We design Software Multiplexing so that it works even in the common case where application build systems support only dynamic and not static linking; we have automatically built thousands of Linux software packages in this way. Software Multiplexing combines two ideas: Automatic Multicall , i.e., where multiple independent programs are automatically merged into a single executable, and Static Linking of Shared Libraries , which works by linking an IR-level version of application code and all its libraries, even if the libraries are normally compiled as shared, before native code generation. The benefits are achieved primarily through deduplication of libraries across the multiplexed programs, while using static linking, and secondly through more effective unused code elimination for statically linked shared libraries. Compared with equivalent dynamically linked programs, &lt;span&gt;allmux-optimized programs start more quickly and even have slightly lower memory usage and total disk size. Compared with equivalent statically linked programs, &lt;span&gt;allmux-optimized programs are much smaller in both aggregate size and memory usage, and have similar startup times and execution performance. We have implemented Software Multiplexing in a tool called &lt;span&gt;allmux, part of the open-source ALLVM project. Example results show that when the LLVM Compiler Infrastructure is optimized using allmux, the resulting binaries and libraries are 18.3% smaller and 30% faster than the default production version. For 74 other packages containing 2–166 programs each, multiplexing each package into one static binary reduces the aggregate package size by 39% (geometric mean) compared with dynamic linking.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3276524",
      "sourceUrl": "https://doi.org/10.1145/3276524",
      "tags": [
        "Infrastructure",
        "IR",
        "Libraries",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Vikram Adve",
        "Will Dietz"
      ]
    },
    {
      "id": "openalex-w2801843051",
      "source": "openalex-discovery",
      "title": "Selective friends in C++",
      "authors": [
        {
          "name": "Gábor Márton",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2018",
      "venue": "Software Practice and Experience | Vol. 48 (Issue 8)",
      "type": "research-paper",
      "abstract": "Summary There is a strong prejudice against the friendship access control mechanism in C++. People claim that friendship breaks the encapsulation, reflects bad design, and creates too strong coupling. However, friends appear even in the most carefully designed systems, and if it is used judiciously (like using the attorney‐client idiom), they may be better choice than widening the public interface of the class. In this paper, we investigate how the friendship mechanism is used in C++ programs. We have made measurements on several open source projects to understand the current use of friends. Our results show various holes and errors in friend usage, like friend functions accessing only public members or not accessing members at all or the class, which declare friends has no private members at all. The results also show that friend functions actually use only a low percentage of the private members they were granted to access, which is a source of errors. These results have motivated us to propose a selective friend language construct for C++, which can restrict friendship only to well‐defined members. Such a new language element may decrease the degradation of encapsulation and significantly increase the diagnostic capacity of the compiler. We have created a proof‐of‐concept implementation based on the LLVM/Clang compiler infrastructure to show that such constructs can be established with a minimal syntactical and compilation overhead.",
      "paperUrl": "https://doi.org/10.1002/spe.2587",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure"
      ],
      "matchedAuthors": [
        "Gábor Márton",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2795286232",
      "source": "openalex-discovery",
      "title": "Racing in Hyperspace: Closing Hyper-Threading Side Channels on SGX with Contrived Data Races",
      "authors": [
        {
          "name": "Guoxing Chen",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Wenhao Wang",
          "affiliation": "Chinese Academy of Sciences"
        },
        {
          "name": "Tianyu Chen",
          "affiliation": "Indiana University Bloomington"
        },
        {
          "name": "Sanchuan Chen",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Yinqian Zhang",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "XiaoFeng Wang",
          "affiliation": "Indiana University Bloomington"
        },
        {
          "name": "Ten‐Hwang Lai",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Dongdai Lin",
          "affiliation": "Chinese Academy of Sciences"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we present HYPERRACE, an LLVM-based tool for instrumenting SGX enclave programs to eradicate all side-channel threats due to Hyper-Threading. HYPERRACE creates a shadow thread for each enclave thread and asks the underlying untrusted operating system to schedule both threads on the same physical core whenever enclave code is invoked, so that Hyper-Threading side channels are closed completely. Without placing additional trust in the operating system's CPU scheduler, HYPERRACE conducts a physical-core co-location test: it first constructs a communication channel between the threads using a shared variable inside the enclave and then measures the communication speed to verify that the communication indeed takes place in the shared L1 data cache-a strong indicator of physical-core co-location. The key novelty of the work is the measurement of communication speed without a trustworthy clock; instead, relative time measurements are taken via contrived data races on the shared variable. It is worth noting that the emphasis of HYPERRACE's defense against Hyper-Threading side channels is because they are open research problems. In fact, HYPERRACE also detects the occurrence of exception-or interrupt-based side channels, the solution.s of which have been studied by several prior works.",
      "paperUrl": "https://ieeexplore.ieee.org/ielx7/8418581/8418583/08418603.pdf",
      "sourceUrl": "https://doi.org/10.1109/sp.2018.00024",
      "tags": [
        "Rust"
      ],
      "matchedAuthors": [
        "Sanchuan Chen",
        "Yinqian Zhang"
      ]
    },
    {
      "id": "openalex-w2884076027",
      "source": "openalex-discovery",
      "title": "Practical Memory Safety with REST",
      "authors": [
        {
          "name": "Kanad Sinha",
          "affiliation": "Columbia University"
        },
        {
          "name": "Simha Sethumadhavan",
          "affiliation": "Columbia University"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we propose Random Embedded Secret Tokens (REST), a simple hardware primitive to provide content-based checks, and show how it can be used to mitigate common types of spatial and temporal memory errors at very low cost. REST is simply a very large random value that is embedded into programs. To provide memory safety, REST is used to bookend data structures during allocation. If the hardware accesses a REST value during execution, due to programming errors or adversarial actions, it reports a privileged memory safety exception. Implementing REST requires 1 bit of metadata per L1 data cache line and a comparator to check for REST tokens during a cache fill. The software infrastructure to provide memory safety with REST reuses a production-quality memory error detection tool, AddressSanitizer, by changing less than 1.5K lines of code. REST based memory safety offers several advantages com-pared to extant methods: (1) it does not require significant redesign of hardware or software, (2) the overhead of heap and stack safety is 2% compared to 40% for AddressSanitizer, (3) the security of the memory safety implementation is improved compared AddressSanitizer, and (4) REST based memory safety can mitigate heap safety errors in legacy binaries without recom-pilation or source code. These advantages provide a significant step towards continuous runtime memory safety monitoring and mitigation for legacy and new binaries.",
      "paperUrl": "https://doi.org/10.1109/isca.2018.00056",
      "sourceUrl": "",
      "tags": [
        "Embedded",
        "Infrastructure",
        "Security"
      ],
      "matchedAuthors": [
        "Simha Sethumadhavan"
      ]
    },
    {
      "id": "openalex-w2908045973",
      "source": "openalex-discovery",
      "title": "Pointers Inside Lambda Closure Objects in OpenMP Target Offload Regions",
      "authors": [
        {
          "name": "David Truby",
          "affiliation": "University of Warwick"
        },
        {
          "name": "Carlo Bertolli",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Steven A. Wright",
          "affiliation": "University of York"
        },
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Kevin O’Brien",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Stephen A. Jarvis",
          "affiliation": "University of Warwick"
        }
      ],
      "year": "2018",
      "venue": "Vol. 74 (Issue None)",
      "type": "research-paper",
      "abstract": "With the diversification of HPC architectures beyond traditional CPU-based clusters, a number of new frameworks for performance portability across architectures have arisen. One way of implementing such frameworks is to use C++ templates and lambda expressions to design loop-like functions. However, lower level programming APIs that these implementations must use are often designed with C in mind and do not specify how they interact with C++ features such as lambda expressions. This paper discusses a change to the behavior of the OpenMP specification with respect to lambda expressions such that when functions generated by lambda expressions are called inside GPU regions, any pointers used in the lambda expression correctly refer to device pointers. This change has been implemented in a branch of the Clang C++ compiler and demonstrated with two representative codes. This change has also been accepted into the draft OpenMP ® specification for inclusion in OpenMP 5. Our results show that the implicit mapping of lambda expressions always exhibits identical performance to an explicit mapping but without breaking the abstraction provided by the high level frameworks.&#13;\\n",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc.2018.8639410",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Carlo Bertolli",
        "Kevin O'Brien"
      ]
    },
    {
      "id": "openalex-w2912825911",
      "source": "openalex-discovery",
      "title": "OpenMP GPU Offload in Flang and LLVM",
      "authors": [
        {
          "name": "Güray Özen",
          "affiliation": ""
        },
        {
          "name": "Simone Atzeni",
          "affiliation": ""
        },
        {
          "name": "Michael Wolfe",
          "affiliation": ""
        },
        {
          "name": "Annemarie Southwell",
          "affiliation": ""
        },
        {
          "name": "Gary Klimowicz",
          "affiliation": ""
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Graphics Processing Units (GPUs) have been widely adopted to accelerate the execution of High Performance Computing (HPC) workloads due to their enormous computational throughput, ability to execute a large number of threads inside SIMD groups in parallel, and their use of multithreaded hardware to hide long pipelining and memory access latency. However, developing applications able to exploit the high performance of GPUs requires proper code tuning. As a consequence, computer scientists proposed different approaches to simplify GPU programming, including directive-based programming models such as OpenMP and OpenACC. Their intention is to solve the aforementioned programming challenges with a directive-based approach which allows the users to insert non-executable pragma constructs that guide the compiler to handle the low-level complexities of the system. Flang, a Fortran front end for the LLVM Compiler Infrastructure, has drawn attention from the HPC community. Although Flang supports OpenMP for multicore architectures, it has no capability of offloading parallel regions to accelerator devices. In this paper, we present OpenMP Offload support in Flang targeting NVIDIA GPUs. Our goal is to investigate possible implementation strategies of OpenMP GPU offloading into Flang. The experimental results show that our implementation achieve similar performance to those of existing compilers with OpenMP GPU offload support.",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc.2018.8639434",
      "sourceUrl": "",
      "tags": [
        "Flang",
        "GPU",
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Simone Atzeni"
      ]
    },
    {
      "id": "openalex-w2905585545",
      "source": "openalex-discovery",
      "title": "OP2-Clang: A Source-to-Source Translator Using Clang/LLVM LibTooling",
      "authors": [
        {
          "name": "Gábor Dániel Balogh",
          "affiliation": "Pázmány Péter Catholic University"
        },
        {
          "name": "Gihan R. Mudalige",
          "affiliation": "University of Warwick"
        },
        {
          "name": "István Z. Reguly",
          "affiliation": "Pázmány Péter Catholic University"
        },
        {
          "name": "Samuel Antão",
          "affiliation": ""
        },
        {
          "name": "Carlo Bertolli",
          "affiliation": ""
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Domain Specific Languages or Active Library frameworks have recently emerged as an important method for gaining performance portability, where an application can be efficiently executed on a wide range of HPC architectures without significant manual modifications. Embedded DSLs such as OP2, provides an API embedded in general purpose languages such as C/C++/Fortran. They rely on source-to-source translation and code refactorization to translate the higher-level API calls to platform specific parallel implementations. OP2 targets the solution of unstructured-mesh computations, where it can generate a variety of parallel implementations for execution on architectures such as CPUs, GPUs, distributed memory clusters and heterogeneous processors making use of a wide range of platform specific optimizations. Compiler tool-chains supporting source-to-source translation of code written in mainstream languages currently lack the capabilities to carry out such wide-ranging code transformations. Clang/LLVM’s Tooling library (LibTooling) has long been touted as having such capabilities but have only demonstrated its use in simple source refactoring tasks. &#13;\\n&#13;\\nIn this paper we introduce OP2-Clang, a source-to-source translator based on LibTooling, for OP2’s C/C++ API, capable of generating target parallel code based on SIMD, OpenMP, CUDA and their combinations with MPI. OP2-Clang is designed to significantly reduce maintenance, particularly making it easy to be extended to generate new parallelizations and optimizations for hardware platforms. In this research, we demonstrate its capabilities including (1) the use of LibTooling’s AST matchers together with a simple strategy that use parallelization templates or skeletons to significantly reduce the complexity of generating radically different and transformed target code and (2) chart the challenges and solution to generating optimized parallelizations for OpenMP, SIMD and CUDA. Results indicate that OP2-Clang produces near-identical parallel code to that of OP2’s current source-to-source translator. We believe that the lessons learnt in OP2-Clang can be readily applied to developing other similar source-to-source translators, particularly for DSLs.",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc.2018.8639205",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "Embedded",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Carlo Bertolli"
      ]
    },
    {
      "id": "openalex-w2809362563",
      "source": "openalex-discovery",
      "title": "Instrumentation of C++ Programs Using Automatic Source Code Transformations",
      "authors": [
        {
          "name": "Z. Parragi",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2018",
      "venue": "Studia Universitatis Babeș-Bolyai Informatica | Vol. 63 (Issue 2)",
      "type": "research-paper",
      "abstract": "The main tool for programmers is always the compiler, but there are also many other tools to help the development process. Some of these tools work on the source code of the program, analyzing, measuring or transforming it. Implementing a source based tool is a complex task, especially for complex languages such as C++. In recent years the C++ language received an easy-to-use library for developing such software, in the form of clang tooling. However, this library only focuses on processing a single translational unit of the program, independently to the other parts of the build process. Tools which ignore this big picture could result in failures when used on larger projects, or incorrect runtime behavior. In this paper, we describe some of these challenges encountered in real-world C++ projects and propose possible solutions for future tools to fix or mitigate the issues.",
      "paperUrl": "http://www.cs.ubbcluj.ro/~studia-i/journal/journal/article/download/28/28",
      "sourceUrl": "https://doi.org/10.24193/subbi.2018.2.04",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2793266554",
      "source": "openalex-discovery",
      "title": "Implicit mapping of pointers inside C++ Lambda closure objects in OpenMP target offload regions",
      "authors": [
        {
          "name": "David Truby",
          "affiliation": ""
        },
        {
          "name": "Carlo Bertolli",
          "affiliation": ""
        },
        {
          "name": "Steven A. Wright",
          "affiliation": ""
        },
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": ""
        },
        {
          "name": "Kevin O’Brien",
          "affiliation": ""
        },
        {
          "name": "Stephen A. Jarvis",
          "affiliation": ""
        }
      ],
      "year": "2018",
      "venue": "White Rose Research Online (University of Leeds, The University of Sheffield, University of York) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "With the diversification of HPC architectures beyond traditional CPU-based clusters, a number of new frameworks for performance portability across architectures have arisen. One way of implementing such frameworks is to use C++ templates and lambda expressions to design loop-like functions. However, lower level programming APIs that these implementations must use are often designed with C in mind and do not specify how they interact with C++ features such as lambda expressions. This paper proposes a change to the behavior of the OpenMP specification with respect to lambda expressions such that when functions generated by lambda expressions are called inside GPU regions, any pointers used in the lambda expression correctly refer to device pointers. This change has been implemented in a branch of the Clang C++ compiler and demonstrated with two representative codes. Our results show that the implicit mapping of lambda expressions always exhibits identical performance to an explicit mapping but without breaking the abstraction provided by the high level frameworks, and therefore also reduces the burden on the application developer.",
      "paperUrl": "https://eprints.whiterose.ac.uk/136574/1/implicit_mapping_pointers_inside_Lambda_closure_objects_OpenMP_Truby_2018.pdf",
      "sourceUrl": "https://orcid.org/0000-0001-7133-8533>",
      "tags": [
        "C++",
        "Clang",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Carlo Bertolli",
        "Kevin O'Brien"
      ]
    },
    {
      "id": "openalex-w4252274690",
      "source": "openalex-discovery",
      "title": "HPVM",
      "authors": [
        {
          "name": "Maria Kotsifakou",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Prakalp Srivastava",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Matthew D. Sinclair",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Rakesh Komuravelli",
          "affiliation": "Qualcomm (United Kingdom)"
        },
        {
          "name": "Vikram Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Sarita V. Adve",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2018",
      "venue": "ACM SIGPLAN Notices | Vol. 53 (Issue 1)",
      "type": "research-paper",
      "abstract": "We propose a parallel program representation for heterogeneous systems, designed to enable performance portability across a wide range of popular parallel hardware, including GPUs, vector instruction sets, multicore CPUs and potentially FPGAs. Our representation, which we call HPVM, is a hierarchical dataflow graph with shared memory and vector instructions. HPVM supports three important capabilities for programming heterogeneous systems: a compiler intermediate representation (IR), a virtual instruction set (ISA), and a basis for runtime scheduling; previous systems focus on only one of these capabilities. As a compiler IR, HPVM aims to enable effective code generation and optimization for heterogeneous systems. As a virtual ISA, it can be used to ship executable programs, in order to achieve both functional portability and performance portability across such systems. At runtime, HPVM enables flexible scheduling policies, both through the graph structure and the ability to compile individual nodes in a program to any of the target devices on a system. We have implemented a prototype HPVM system, defining the HPVM IR as an extension of the LLVM compiler IR, compiler optimizations that operate directly on HPVM graphs, and code generators that translate the virtual ISA to NVIDIA GPUs, Intel's AVX vector units, and to multicore X86-64 processors. Experimental results show that HPVM optimizations achieve significant performance improvements, HPVM translators achieve performance competitive with manually developed OpenCL code for both GPUs and vector hardware, and that runtime scheduling policies can make use of both program and runtime information to exploit the flexible compilation capabilities. Overall, we conclude that the HPVM representation is a promising basis for achieving performance portability and for implementing parallelizing compilers for heterogeneous parallel systems.",
      "paperUrl": "http://dl.acm.org/ft_gateway.cfm?id=3178493&type=pdf",
      "sourceUrl": "https://doi.org/10.1145/3200691.3178493",
      "tags": [
        "IR",
        "OpenCL",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Sarita V. Adve",
        "Vikram Adve"
      ]
    },
    {
      "id": "openalex-w2809567982",
      "source": "openalex-discovery",
      "title": "Compile-Time Function Call Interception for Testing in C/C++",
      "authors": [
        {
          "name": "G. Marton",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2018",
      "venue": "Studia Universitatis Babeș-Bolyai Informatica | Vol. 63 (Issue 1)",
      "type": "research-paper",
      "abstract": "In C/C++, during the test development process we often have to modify the public interface of a class to replace existing dependencies; supplementary setter or constructor functions or extra template parameters are added for dependency injection. These solutions may have serious detrimental effects on the code structure and sometimes on the run-time performance as well. We introduce a new technique that makes dependency replacement possible without the modification of the production code, thus it provides an alternative way to add unit tests. Our new compile-time instrumentation technique enables us to intercept function calls and replace them in runtime. Contrary to existing function call interception (FCI) methods, we instrument the call expression instead of the callee, thus we can avoid the modification and recompilation of the function in order to intercept the call. This has a clear advantage in case of system libraries and third party shared libraries, thus it provides an alternative way to automatize tests for legacy software. We created a prototype implementation based on the LLVM compiler infrastructure which is publicly available for testing.",
      "paperUrl": "http://www.cs.ubbcluj.ro/~studia-i/journal/journal/article/download/19/18",
      "sourceUrl": "https://doi.org/10.24193/subbi.2018.1.02",
      "tags": [
        "C++",
        "Infrastructure",
        "Libraries",
        "Performance",
        "Testing"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2898992563",
      "source": "openalex-discovery",
      "title": "Comparison of Clang Abstract Syntax Trees using String Kernels",
      "authors": [
        {
          "name": "Raul Torres",
          "affiliation": "Universität Hamburg"
        },
        {
          "name": "Thomas Ludwig",
          "affiliation": "Universität Hamburg"
        },
        {
          "name": "Julian Kunkel",
          "affiliation": "University of Reading"
        },
        {
          "name": "Manuel F. Dolz",
          "affiliation": "Universidad Carlos III de Madrid"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Abstract Syntax Trees (ASTs) are intermediate representations widely used by compiler frameworks. One of their strengths is that they can be used to determine the similarity among a collection of programs. In this paper we propose a novel comparison method that converts ASTs into weighted strings in order to get similarity matrices and quantify the level of correlation among codes. To evaluate the approach, we leveraged the corresponding strings derived from the Clang ASTs of a set of 100 source code examples written in C. Our kernel and two other string kernels from the literature were used to obtain similarity matrices among those examples. Next, we used Hierarchical Clustering to visualize the results. Our solution was able to identify different clusters conformed by examples that shared similar semantics. We demonstrated that the proposed strategy can be promisingly applied to similarity problems involving trees or strings.",
      "paperUrl": "https://doi.org/10.1109/hpcs.2018.00032",
      "sourceUrl": "",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Julian Kunkel"
      ]
    },
    {
      "id": "openalex-w2894900079",
      "source": "openalex-discovery",
      "title": "Codestitcher: Inter-Procedural Basic Block Layout Optimization",
      "authors": [
        {
          "name": "Rahman Lavaee",
          "affiliation": "University of Rochester"
        },
        {
          "name": "John Criswell",
          "affiliation": "University of Rochester"
        },
        {
          "name": "Chen Ding",
          "affiliation": "University of Rochester"
        }
      ],
      "year": "2018",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern software executes a large amount of code. Previous techniques of code layout optimization were developed one or two decades ago and have become inadequate to cope with the scale and complexity of new types of applications such as compilers, browsers, interpreters, language VMs and shared libraries. This paper presents Codestitcher, an inter-procedural basic block code layout optimizer which reorders basic blocks in an executable to benefit from better cache and TLB performance. Codestitcher provides a hierarchical framework which can be used to improve locality in various layers of the memory hierarchy. Our evaluation shows that Codestitcher improves the performance of the original program by 3\\% to 25\\% (on average, by 10\\%) on 5 widely used applications with large code sizes: MySQL, Clang, Firefox, Apache, and Python. It gives an additional improvement of 4\\% over LLVM's PGO and 3\\% over PGO combined with the best function reordering technique.",
      "paperUrl": "https://arxiv.org/pdf/1810.00905",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1810.00905",
      "tags": [
        "Clang",
        "Libraries",
        "Performance",
        "PGO"
      ],
      "matchedAuthors": [
        "Chen Ding",
        "John Criswell"
      ]
    },
    {
      "id": "openalex-w2883422009",
      "source": "openalex-discovery",
      "title": "Codecompass",
      "authors": [
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Tibor Brunner",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Dániel Krupp",
          "affiliation": "Ericsson (Hungary)"
        },
        {
          "name": "Márton Csordás",
          "affiliation": "Ericsson (Hungary)"
        }
      ],
      "year": "2018",
      "venue": "Vol. 3 (Issue None)",
      "type": "research-paper",
      "abstract": "CodeCompass is an open source LLVM/Clang-based tool developed by Ericsson Ltd. and Eötvös Loránd University, Budapest to help the understanding of large legacy software systems. Based on the LLVM/Clang compiler infrastructure, CodeCompass gives exact information on complex C/C++ language elements like overloading, inheritance, the usage of variables and types, possible uses of function pointers and virtual functions - features that various existing tools support only partially. Steensgaard's and Andersen's pointer analysis algorithms are used to compute and visualize the use of pointers/references. The wide range of interactive visualizations extends further than the usual class and function call diagrams; architectural, component and interface diagrams are a few of the implemented graphs. To make comprehension more extensive, CodeCompass also utilizes build information to explore the system architecture as well as version control information.",
      "paperUrl": "https://doi.org/10.1145/3196321.3197546",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure"
      ],
      "matchedAuthors": [
        "Dániel Krupp",
        "Márton Csordás",
        "Tibor Brunner",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2606228508",
      "source": "openalex-discovery",
      "title": "CUP",
      "authors": [
        {
          "name": "Nathan Burow",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Derrick McKee",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Scott A. Carr",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Mathias Payer",
          "affiliation": "Purdue University West Lafayette"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Memory corruption vulnerabilities in C/C++ applications enable attackers to\\nexecute code, change data, and leak information. Current memory sanitizers do\\nno provide comprehensive coverage of a program's data. In particular, existing\\ntools focus primarily on heap allocations with limited support for stack\\nallocations and globals. Additionally, existing tools focus on the main\\nexecutable with limited support for system libraries. Further, they suffer from\\nboth false positives and false negatives.\\n We present Comprehensive User-Space Protection for C/C++, CUP, an LLVM\\nsanitizer that provides complete spatial and probabilistic temporal memory\\nsafety for C/C++ program on 64-bit architectures (with a prototype\\nimplementation for x86_64). CUP uses a hybrid metadata scheme that supports all\\nprogram data including globals, heap, or stack and maintains the ABI. Compared\\nto existing approaches with the NIST Juliet test suite, CUP reduces false\\nnegatives by 10x (0.1%) compared to the state of the art LLVM sanitizers, and\\nproduces no false positives. CUP instruments all user-space code, including\\nlibc and other system libraries, removing them from the trusted code base.\\n",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3196494.3196540",
      "sourceUrl": "https://doi.org/10.1145/3196494.3196540",
      "tags": [
        "C++",
        "Libraries",
        "Rust"
      ],
      "matchedAuthors": [
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w2789572737",
      "source": "openalex-discovery",
      "title": "CUDAAdvisor: LLVM-based runtime profiling for modern GPUs",
      "authors": [
        {
          "name": "Du Shen",
          "affiliation": "William & Mary"
        },
        {
          "name": "Shuaiwen Leon Song",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Ang Li",
          "affiliation": "Pacific Northwest National Laboratory"
        },
        {
          "name": "Xu Liu",
          "affiliation": "William & Mary"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "General-purpose GPUs have been widely utilized to accelerate parallel applications. Given a relatively complex programming model and fast architecture evolution, producing efficient GPU code is nontrivial. A variety of simulation and profiling tools have been developed to aid GPU application optimization and architecture design. However, existing tools are either limited by insufficient insights or lacking in support across different GPU architectures, runtime and driver versions. This paper presents CUDAAdvisor, a profiling framework to guide code optimization in modern NVIDIA GPUs. CUDAAdvisor performs various fine-grained analyses based on the profiling results from GPU kernels, such as memory-level analysis (e.g., reuse distance and memory divergence), control flow analysis (e.g., branch divergence) and code-/data-centric debugging. Unlike prior tools, CUDAAdvisor supports GPU profiling across different CUDA versions and architectures, including CUDA 8.0 and Pascal architecture. We demonstrate several case studies that derive significant insights to guide GPU code optimization for performance improvement.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3168831",
      "sourceUrl": "https://doi.org/10.1145/3168831",
      "tags": [
        "CUDA",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Ang Li"
      ]
    },
    {
      "id": "openalex-w2902677827",
      "source": "openalex-discovery",
      "title": "CRIMES",
      "authors": [
        {
          "name": "Sundaresan Rajasekaran",
          "affiliation": "George Washington University"
        },
        {
          "name": "Harpreet Singh Chawla",
          "affiliation": "George Washington University"
        },
        {
          "name": "Zhen Ni",
          "affiliation": "George Washington University"
        },
        {
          "name": "Neel Shah",
          "affiliation": "George Washington University"
        },
        {
          "name": "Emery D. Berger",
          "affiliation": "University of Massachusetts Amherst"
        },
        {
          "name": "Timothy Wood",
          "affiliation": "George Washington University"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Cloud applications are appealing targets to attackers, yet current cloud infrastructures have few ways of helping defend their customers from attacks. However, the use of virtual machines, and the economy of scale found in cloud platforms, provides an opportunity to offer strong security guarantees to tenants at low cost to the cloud provider. We present CRIMES, an evidence based, modular security framework for cloud platforms that uses speculative execution coupled with memory introspection tools to detect malicious behavior in real time. By buffering VM outputs (i.e., outgoing network packets and disk writes) until a scan has been completed, CRIMES gives strong guarantees about the amount of damage an attack can do, while minimizing overheads. When an attack is detected, CRIMES rolls back to a recent checkpoint and performs automated forensic analysis to help pinpoint the source of an attack. Our evaluation demonstrates that CRIMES incurs less overhead compared to memory protection tools such as AddressSanitizer, while offering valuable forensic analysis for buffer overflow attacks and malware detection across multiple applications and the OS.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3274808.3274812",
      "sourceUrl": "https://doi.org/10.1145/3274808.3274812",
      "tags": [
        "Infrastructure",
        "Security"
      ],
      "matchedAuthors": [
        "Emery D. Berger"
      ]
    },
    {
      "id": "openalex-w2904561022",
      "source": "openalex-discovery",
      "title": "CLACC: Translating OpenACC to OpenMP in Clang",
      "authors": [
        {
          "name": "Joel Denny",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Seyong Lee",
          "affiliation": "Oak Ridge National Laboratory"
        },
        {
          "name": "Jeffrey S. Vetter",
          "affiliation": "Oak Ridge National Laboratory"
        }
      ],
      "year": "2018",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "OpenACC was launched in 2010 as a portable programming model for heterogeneous accelerators. Although various implementations already exist, no extensible, open-source, production-quality compiler support is available to the community. This deficiency poses a serious risk for HPC application developers targeting GPUs and other accelerators, and it limits experimentation and progress for the OpenACC specification. To address this deficiency, Clacc is a recent effort funded by the US Exascale Computing Project to develop production OpenACC compiler support for Clang and LLVM. A key feature of the Clacc design is to translate OpenACC to OpenMP to build on Clang's existing OpenMP compiler and runtime support. In this paper, we describe the Clacc goals and design. We also describe the challenges that we have encountered so far in our prototyping efforts, and we present some early performance results.",
      "paperUrl": "https://www.osti.gov/biblio/1489555",
      "sourceUrl": "https://doi.org/10.1109/llvm-hpc.2018.8639349",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Jeffrey S. Vetter"
      ]
    },
    {
      "id": "openalex-w4289752579",
      "source": "openalex-discovery",
      "title": "BOLT: A Practical Binary Optimizer for Data Centers and Beyond",
      "authors": [
        {
          "name": "Maksim Panchenko",
          "affiliation": ""
        },
        {
          "name": "Rafael Auler",
          "affiliation": ""
        },
        {
          "name": "Bill Nell",
          "affiliation": ""
        },
        {
          "name": "Guilherme Ottoni",
          "affiliation": ""
        }
      ],
      "year": "2018",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Performance optimization for large-scale applications has recently become more important as computation continues to move towards data centers. Data-center applications are generally very large and complex, which makes code layout an important optimization to improve their performance. This has motivated recent investigation of practical techniques to improve code layout at both compile time and link time. Although post-link optimizers had some success in the past, no recent work has explored their benefits in the context of modern data-center applications. In this paper, we present BOLT, a post-link optimizer built on top of the LLVM framework. Utilizing sample-based profiling, BOLT boosts the performance of real-world applications even for highly optimized binaries built with both feedback-driven optimizations (FDO) and link-time optimizations (LTO). We demonstrate that post-link performance improvements are complementary to conventional compiler optimizations, even when the latter are done at a whole-program level and in the presence of profile information. We evaluated BOLT on both Facebook data-center workloads and open-source compilers. For data-center applications, BOLT achieves up to 8.0% performance speedups on top of profile-guided function reordering and LTO. For the GCC and Clang compilers, our evaluation shows that BOLT speeds up their binaries by up to 20.4% on top of FDO and LTO, and up to 52.1% if the binaries are built without FDO and LTO.",
      "paperUrl": "https://arxiv.org/pdf/1807.06735",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1807.06735",
      "tags": [
        "Clang",
        "LTO",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Maksim Panchenko"
      ]
    },
    {
      "id": "openalex-w2898537641",
      "source": "openalex-discovery",
      "title": "An empirical study of the effect of source-level loop transformations on compiler stability",
      "authors": [
        {
          "name": "Zhangxiaowen Gong",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Zhi Chen",
          "affiliation": "University of California, Irvine"
        },
        {
          "name": "Justin Szaday",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "David Wong",
          "affiliation": "Intel (United States)"
        },
        {
          "name": "Zehra Sura",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Neftali Watkinson",
          "affiliation": "University of California, Irvine"
        },
        {
          "name": "Saeed Maleki",
          "affiliation": "Microsoft (United States)"
        },
        {
          "name": "David Padua",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Alexander V. Veidenbaum",
          "affiliation": "University of California, Irvine"
        },
        {
          "name": "Alexandru Nicolau",
          "affiliation": "University of California, Irvine"
        },
        {
          "name": "Josep Torrellas",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2018",
      "venue": "Proceedings of the ACM on Programming Languages | Vol. 2 (Issue OOPSLA)",
      "type": "research-paper",
      "abstract": "Modern compiler optimization is a complex process that offers no guarantees to deliver the fastest, most efficient target code. For this reason, compilers struggle to produce a stable performance from versions of code that carry out the same computation and only differ in the order of operations. This instability makes compilers much less effective program optimization tools and often forces programmers to carry out a brute force search when tuning for performance. In this paper, we analyze the stability of the compilation process and the performance headroom of three widely used general purpose compilers: GCC, ICC, and Clang. For the study, we extracted over 1,000 &lt;pre&gt;for&lt;/pre&gt; loop nests from well-known benchmarks, libraries, and real applications; then, we applied sequences of source-level loop transformations to these loop nests to create numerous semantically equivalent mutations ; finally, we analyzed the impact of transformations on code quality in terms of locality, dynamic instruction count, and vectorization. Our results show that, by applying source-to-source transformations and searching for the best vectorization setting, the percentage of loops sped up by at least 1.15x is 46.7% for GCC, 35.7% for ICC, and 46.5% for Clang, and on average the potential for performance improvement is estimated to be at least 23.7% for GCC, 18.1% for ICC, and 26.4% for Clang. Our stability analysis shows that, under our experimental setup, the average coefficient of variation of the execution time across all mutations is 18.2% for GCC, 19.5% for ICC, and 16.9% for Clang, and the highest coefficient of variation for a single loop nest reaches 118.9% for GCC, 124.3% for ICC, and 110.5% for Clang. We conclude that the evaluated compilers need further improvements to claim they have stable behavior.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3276496",
      "sourceUrl": "https://doi.org/10.1145/3276496",
      "tags": [
        "Clang",
        "Libraries",
        "Loop transformations",
        "Performance"
      ],
      "matchedAuthors": [
        "Josep Torrellas"
      ]
    },
    {
      "id": "openalex-w2908959276",
      "source": "openalex-discovery",
      "title": "An Autotuning Framework for Scalable Execution of Tiled Code via Iterative Polyhedral Compilation",
      "authors": [
        {
          "name": "Yukinori Sato",
          "affiliation": "Toyohashi University of Technology"
        },
        {
          "name": "Tomoya Yuki",
          "affiliation": "Tokyo Institute of Technology"
        },
        {
          "name": "Toshio Endo",
          "affiliation": "Tokyo Institute of Technology"
        }
      ],
      "year": "2018",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 15 (Issue 4)",
      "type": "research-paper",
      "abstract": "On modern many-core CPUs, performance tuning against complex memory subsystems and scalability for parallelism is mandatory to achieve their potential. In this article, we focus on loop tiling, which plays an important role in performance tuning, and develop a novel framework that analytically models the load balance and empirically autotunes unpredictable cache behaviors through iterative polyhedral compilation using LLVM/Polly. From an evaluation on many-core CPUs, we demonstrate that our autotuner achieves a performance superior to those that use conventional static approaches and well-known autotuning heuristics. Moreover, our autotuner achieves almost the same performance as a brute-force search-based approach.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3293449",
      "sourceUrl": "https://doi.org/10.1145/3293449",
      "tags": [
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Toshio Endo"
      ]
    },
    {
      "id": "openalex-w2583316335",
      "source": "openalex-discovery",
      "title": "rev.ng: a unified binary analysis framework to recover CFGs and function boundaries",
      "authors": [
        {
          "name": "Alessandro Di Federico",
          "affiliation": "Politecnico di Milano"
        },
        {
          "name": "Mathias Payer",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Giovanni Agosta",
          "affiliation": "Politecnico di Milano"
        }
      ],
      "year": "2017",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Static binary analysis is a key tool to assess the security of thirdparty binaries and legacy programs. Most forms of binary analysis rely on the availability of two key pieces of information: the program's control-flow graph and function boundaries. However, current tools struggle to provide accurate and precise results, in particular when dealing with hand-written assembly functions and non-trivial control-flow transfer instructions, such as tail calls. In addition, most of the existing solutions are ad-hoc, rely on handcoded heuristics, and are tied to a specific architecture. In this paper we highlight the challenges faced by an architecture agnostic static binary analysis framework to provide accurate information about a program's CFG and function boundaries without employing debugging information or symbols.We propose a set of analyses to address predicate instructions, noreturn functions, tail calls, and context-dependent CFG. REV.NG, our binary analysis framework based on QEMU and LLVM, handles all the 17 architectures supported by QEMU and produces a compilable LLVM IR. We implement our described analyses on top of LLVM IR. In an extensive evaluation, we test our tool on binaries compiled for MIPS, ARM, and x86-64 using GCC and clang and compare them to the industry's state of the art tool, IDA Pro, and two well-known academic tools, BAP/ByteWeight and angr. In all cases, the quality of the CFG and function boundaries produced by REV.NG is comparable to or improves over the alternatives.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3033019.3033028",
      "sourceUrl": "https://doi.org/10.1145/3033019.3033028",
      "tags": [
        "Clang",
        "IR",
        "Security"
      ],
      "matchedAuthors": [
        "Alessandro Di Federico",
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w3161603333",
      "source": "openalex-discovery",
      "title": "TreeFuser: A Framework for Analyzing and Fusing General Recursive Tree Traversals",
      "authors": [
        {
          "name": "Laith Sakka",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Kirshanthan Sundararajah",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Milind Kulkarni",
          "affiliation": "Purdue University West Lafayette"
        }
      ],
      "year": "2017",
      "venue": "Conference on Object-Oriented Programming Systems, Languages, and Applications | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Series of traversals of tree structures arise in numerous contexts: abstract syntax tree traversals in compiler passes, rendering traversals of the DOM in web browsers, kd-tree traversals in computational simulation codes. In each of these settings, a tree is traversed multiple times to compute various values and modify various portions of the tree. While it is relatively easy to write these traversals as separate small updates to the tree, for efficiency reasons, traversals are often manually fused to reduce the number of times that each portion of the tree is traversed: by performing multiple operations on the tree simultaneously, each node of the tree can be visited fewer times, increasing opportunities for optimization and decreasing cache pressure and other overheads. This fusion process is often done manually, requiring careful understanding of how each of traversals of the tree interact. This paper presents an automatic approach to traversal fusion: tree traversals can be written independently, and then our framework analyzes the dependences between the traversals to determine how they can be fused to reduce the number of visits to each node in the tree. A critical aspect of our framework is that it exploits two opportunities to increase the amount of fusion: i) it automatically integrates code motion, and ii) it supports partial fusion, where portions of one traversal can be fused with another, allowing for a reduction in node visits without requiring that two traversals be fully fused. We implement our framework in Clang, and show across several case studies that we can successfully fuse complex tree traversals, reducing the overall number of traversals and substantially improving locality and performance.",
      "paperUrl": "https://2017.splashcon.org/event/splash-2017-oopsla-treefusor-a-framework-for-analyzing-and-fusing-general-recursive-tree-traversals",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Laith Sakka"
      ]
    },
    {
      "id": "openalex-w2581768997",
      "source": "openalex-discovery",
      "title": "Towards Composable GPU Programming",
      "authors": [
        {
          "name": "Michael Haidl",
          "affiliation": "University of Münster"
        },
        {
          "name": "Michel Steuwer",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Hendrik Dirks",
          "affiliation": "University of Münster"
        },
        {
          "name": "Tim Humernbrum",
          "affiliation": "University of Münster"
        },
        {
          "name": "Sergei Gorlatch",
          "affiliation": "University of Münster"
        }
      ],
      "year": "2017",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "In this paper, we advocate a composable approach to programming systems with Graphics Processing Units (GPU): programs are developed as compositions of generic, reusable patterns. Current GPU programming approaches either rely on low-level, monolithic code without patterns (CUDA and OpenCL), which achieves high performance at the cost of cumbersome and error-prone programming, or they improve the programmability by using pattern-based abstractions (e.g., Thrust) but pay a performance penalty due to inefficient implementations of pattern composition.&#13;\\n&#13;\\nWe develop an API for GPUs based programming on C++ with STL-style patterns and its compiler-based implementation. Our API gives the application developers the native C++ means (views and actions) to specify precisely which pattern compositions should be automatically fused during&#13;\\ncode generation into a single efficient GPU kernel, thereby ensuring a high target performance. We implement our approach by extending the range-v3&#13;\\nlibrary which is currently being developed for the forthcoming C++ standards. The composable programming in our approach is done exclusively in the standard C++14, with STL algorithms used as patterns which we re-implemented in parallel for GPU. Our compiler implementation is based on the LLVM and Clang frameworks, and we use advanced multi-stage programming techniques for aggressive runtime optimizations.&#13;\\n&#13;\\nWe experimentally evaluate our approach using a set of benchmark applications and a real-world case study from the area of image processing. Our codes achieve performance competitive with CUDA monolithic implementations, and we outperform pattern-based codes written using Nvidia’s Thrust.",
      "paperUrl": "https://www.research.ed.ac.uk/en/publications/fff66ade-3077-40d0-ac28-5851a7d7f938",
      "sourceUrl": "https://doi.org/10.1145/3026937.3026942",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "GPU",
        "OpenCL",
        "Optimizations",
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Michael Haidl",
        "Michel Steuwer",
        "Sergei Gorlatch",
        "Tim Humernbrum"
      ]
    },
    {
      "id": "openalex-w4237194902",
      "source": "openalex-discovery",
      "title": "Tapir",
      "authors": [
        {
          "name": "Tao B. Schardl",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "William S. Moses",
          "affiliation": "Massachusetts Institute of Technology"
        },
        {
          "name": "Charles E. Leiserson",
          "affiliation": "Massachusetts Institute of Technology"
        }
      ],
      "year": "2017",
      "venue": "ACM SIGPLAN Notices | Vol. 52 (Issue 8)",
      "type": "research-paper",
      "abstract": "This paper explores how fork-join parallelism, as supported by concurrency platforms such as Cilk and OpenMP, can be embedded into a compiler's intermediate representation (IR). Mainstream compilers typically treat parallel linguistic constructs as syntactic sugar for function calls into a parallel runtime. These calls prevent the compiler from performing optimizations across parallel control constructs. Remedying this situation is generally thought to require an extensive reworking of compiler analyses and code transformations to handle parallel semantics. Tapir is a compiler IR that represents logically parallel tasks asymmetrically in the program's control flow graph. Tapir allows the compiler to optimize across parallel control constructs with only minor changes to its existing analyses and code transformations. To prototype Tapir in the LLVM compiler, for example, we added or modified about 6000 lines of LLVM's 4-million-line codebase. Tapir enables LLVM's existing compiler optimizations for serial code -- including loop-invariant-code motion, common-subexpression elimination, and tail-recursion elimination -- to work with parallel control constructs such as spawning and parallel loops. Tapir also supports parallel optimizations such as loop scheduling.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/3155284.3018758?download=true",
      "sourceUrl": "https://doi.org/10.1145/3155284.3018758",
      "tags": [
        "Embedded",
        "IR",
        "Optimizations"
      ],
      "matchedAuthors": [
        "William S. Moses"
      ]
    },
    {
      "id": "openalex-w2911179065",
      "source": "openalex-discovery",
      "title": "Symbolic execution of verification languages and floating-point code",
      "authors": [
        {
          "name": "Daniel Liew",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2017",
      "venue": "Spiral (Imperial College London) | Vol. None (Issue None)",
      "type": "thesis",
      "abstract": "The focus of this thesis is a program analysis technique named symbolic execution. We present three main contributions to this field. First, an investigation into comparing several state-of-the-art program analysis tools at the level of an intermediate verification language over a large set of benchmarks, and improvements to the state-of-the-art of symbolic execution for this language. This is explored via a new tool, Symbooglix, that operates on the Boogie intermediate verification language. Second, an investigation into performing symbolic execution of floating-point programs via a standardised theory of floating-point arithmetic that is supported by several existing constraint solvers. This is investigated via two independent extensions of the KLEE symbolic execution engine to support reasoning about floating-point operations (with one tool developed by the thesis author). Third, an investigation into the use of coverage-guided fuzzing as a means for solving constraints over finite data types, inspired by the difficulties associated with solving floating-point constraints. The associated prototype tool, JFS, which builds on the LibFuzzer project, can at present be applied to a wide range of SMT queries over bit-vector and floating-point variables, and shows promise on floating-point constraints.",
      "paperUrl": "http://hdl.handle.net/10044/1/59705",
      "sourceUrl": "https://doi.org/10.25560/59705",
      "tags": [],
      "matchedAuthors": [
        "Daniel Liew"
      ]
    },
    {
      "id": "openalex-w2770644362",
      "source": "openalex-discovery",
      "title": "Souper: A Synthesizing Superoptimizer",
      "authors": [
        {
          "name": "Raimondas Sasnauskas",
          "affiliation": ""
        },
        {
          "name": "Yang Chen",
          "affiliation": ""
        },
        {
          "name": "Peter Collingbourne",
          "affiliation": ""
        },
        {
          "name": "Jeroen Ketema",
          "affiliation": ""
        },
        {
          "name": "Jubi Taneja",
          "affiliation": ""
        },
        {
          "name": "John Regehr",
          "affiliation": ""
        },
        {
          "name": "Regehr, John",
          "affiliation": ""
        }
      ],
      "year": "2017",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "If we can automatically derive compiler optimizations, we might be able to sidestep some of the substantial engineering challenges involved in creating and maintaining a high-quality compiler. We developed Souper, a synthesizing superoptimizer, to see how far these ideas might be pushed in the context of LLVM. Along the way, we discovered that Souper's intermediate representation was sufficiently similar to the one in Microsoft Visual C++ that we applied Souper to that compiler as well. Shipping, or about-to-ship, versions of both compilers contain optimizations suggested by Souper but implemented by hand. Alternately, when Souper is used as a fully automated optimization pass it compiles a Clang compiler binary that is about 3 MB (4.4%) smaller than the one compiled by LLVM.",
      "paperUrl": "https://arxiv.org/pdf/1711.04422",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1711.04422",
      "tags": [
        "C++",
        "Clang",
        "Optimizations"
      ],
      "matchedAuthors": [
        "John Regehr",
        "Jubi Taneja",
        "Peter Collingbourne"
      ]
    },
    {
      "id": "openalex-w2607063282",
      "source": "openalex-discovery",
      "title": "SGXBOUNDS",
      "authors": [
        {
          "name": "Dmitrii Kuvaiskii",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Oleksii Oleksenko",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Sergei Arnautov",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Bohdan Trach",
          "affiliation": "TU Dresden"
        },
        {
          "name": "Pramod Bhatotia",
          "affiliation": "University of Edinburgh"
        },
        {
          "name": "Pascal Felber",
          "affiliation": "University of Neuchâtel"
        },
        {
          "name": "Christof Fetzer",
          "affiliation": "TU Dresden"
        }
      ],
      "year": "2017",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "&lt;p&gt;Shielded execution based on Intel SGX provides strong security guarantees for legacy applications running on untrusted platforms. However, memory safety attacks such as Heartbleed can render the confidentiality and integrity properties of shielded execution completely ineffective. To prevent these attacks, the state-of-the-art memory-safety approaches can be used in the context of shielded execution.&lt;/p&gt; &lt;p&gt;In this work, we first showcase that two prominent software- and hardware-based defenses, AddressSanitizer and Intel MPX respectively, are impractical for shielded execution due to high performance and memory overheads. This motivated our design of SGXBounds---an efficient memory-safety approach for shielded execution exploiting the architectural features of Intel SGX. Our design is based on a simple combination of tagged pointers and compact memory layout.&lt;/p&gt; &lt;p&gt;We implemented SGXBounds based on the LLVM compiler framework targeting unmodified multithreaded applications. Our evaluation using Phoenix, PARSEC, and RIPE benchmark suites shows that SGXBounds has performance and memory overheads of 17% and 0.1% respectively, while providing security guarantees similar to AddressSanitizer and Intel MPX. We have obtained similar results with SPEC CPU2006 and four real-world case studies: SQLite, Memcached, Apache, and Nginx.&lt;/p&gt;",
      "paperUrl": "https://zenodo.org/record/1172707",
      "sourceUrl": "https://doi.org/10.1145/3064176.3064192",
      "tags": [
        "Performance",
        "Rust",
        "Security"
      ],
      "matchedAuthors": [
        "Christof Fetzer",
        "Pascal Felber"
      ]
    },
    {
      "id": "openalex-w2769256264",
      "source": "openalex-discovery",
      "title": "Performance evaluation of OpenMPs target construct on GPUs",
      "authors": [
        {
          "name": "Vivek Sarkar",
          "affiliation": "Rice University"
        },
        {
          "name": "Robert Ho",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Ettore Tiotto",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Jun Shirako",
          "affiliation": "Rice University"
        },
        {
          "name": "Akihiro Hayashi",
          "affiliation": "Rice University"
        }
      ],
      "year": "2017",
      "venue": "International Journal of High Performance Computing and Networking | Vol. 1 (Issue 1)",
      "type": "research-paper",
      "abstract": "OpenMP is a directive-based shared memory parallel programming model and has been widely used for many years. From OpenMP 4.0 onwards, GPU platforms are supported by extending OpenMP's high-level parallel abstractions with accelerator programming. This extension allows programmers to write GPU programs in standard C/C++ or Fortran languages, without exposing too many details of GPU architectures. However, such high-level programming models generally impose additional program optimisations on compilers and runtime systems. Otherwise, OpenMP programs could be slower than fully hand-tuned and even naive implementations with low-level programming models like CUDA. To study potential performance improvements by compiling and optimising high-level programs for GPU execution, in this paper, we: 1) evaluate a set of OpenMP benchmarks on two NVIDIA Tesla GPUs (K80 and P100); 2) conduct a comparable performance analysis among hand-written CUDA and automatically-generated GPU programs by the IBM XL and clang/LLVM compilers.",
      "paperUrl": "https://doi.org/10.1504/ijhpcn.2017.10009068",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Ettore Tiotto"
      ]
    },
    {
      "id": "openalex-w2811284039",
      "source": "openalex-discovery",
      "title": "Optimisations des Compilateurs et Modèles Mémoire Relâchés",
      "authors": [
        {
          "name": "Robin Morisset",
          "affiliation": ""
        }
      ],
      "year": "2017",
      "venue": "HAL (Le Centre pour la Communication Scientifique Directe) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Modern multiprocessors architectures and programming languages exhibit weakly consistent memories. Their behaviour is formalised by the memory model of the architecture or programming language; it precisely defines which write operation can be returned by each shared memory read. This is not always the latest store to the same variable, because of optimisations in the processors such as speculative execution of instructions, the complex effects of caches, and optimisations in the compilers. In this thesis we focus on the C11 memory model that is defined by the 2011 edition of the C standard. Our contributions are threefold. First, we focused on the theory surrounding the C11 model, formally studying which compiler optimisations it enables. We show that many common compiler optimisations are allowed, but, surprisingly, some important ones are forbidden. Secondly, building on our results, we developed a random testing methodology for detecting when mainstream compilers such as GCC or Clang perform an incorrect optimisation with respect to the memory model. We found several bugs in GCC, all promptly fixed. We also implemented a novel optimisation pass in LLVM, that looks for special instructions that restrict processor optimisations - called fence instructions - and eliminates the redundant ones. Finally, we developed a user-level scheduler for lightweight threads communicating through first-in first-out single-producer single-consumer queues. This programming model is known as Kahn process networks, and we show how to efficiently implement it, using C11 synchronisation primitives. This shows that despite its flaws, C11 can be usable in practice.",
      "paperUrl": "https://theses.hal.science/tel-01823521",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Programming Languages",
        "Testing"
      ],
      "matchedAuthors": [
        "Robin Morisset"
      ]
    },
    {
      "id": "openalex-w2751520368",
      "source": "openalex-discovery",
      "title": "OpenCL 2.0 Compiler Adaptation on LLVM for PTX Simulators",
      "authors": [
        {
          "name": "Chun‐Chieh Yang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Shao-Chung Wang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Min-Yi Hsu",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Ming Chang",
          "affiliation": "National Tsing Hua University"
        },
        {
          "name": "Yuan‐Shin Hwang",
          "affiliation": "National Taiwan University of Science and Technology"
        },
        {
          "name": "Jenq‐Kuen Lee",
          "affiliation": "National Tsing Hua University"
        }
      ],
      "year": "2017",
      "venue": "Vol. 14 (Issue None)",
      "type": "research-paper",
      "abstract": "OpenCL continues to gather momentum on both desktop and mobile devices. The new features of OpenCL 2.0 provides developers better expressive power in programming heterogeneous computing environments. Currently in the experimental simulation environment, gem5-gpu only supports CUDA, but GPGPU-Sim can support OpenCL by compiling OpenCL kernel code to PTX using real GPU driver. However, this driver compilation in GPGPU-Sim only can support up to OpenCL 1.2. To support OpenCL 2.0, it is necessary to extend the compiler to enable the compilation of OpenCL 2.0 kernel code to PTX. In this paper, our experience in enabling the compiler flow is reported. In OpenCL 2.0, it provides new features such as dynamic parallelism, work-group built-in functions, extend atomic builtin functions, and so on. The proposed compiler that is modified from Low Level Virtual Machine (LLVM) extends such features for enhancing the emulator to support OpenCL 2.0. After the compiler is modified, it can support dynamic parallelism, workgroup built-in functions and extend atomic built-in functions. Using existing dynamic parallelism APIs in CUDA to implement OpenCL 2.0 enqueue kernel and revise compilation scheme in clang. Furthermore, the proposed compiler also creates local buffers for each work group to use for work-group builtin functions, and adds atomic built-in functions with memory order and memory scope for OpenCL 2.0 in NVPTX. From benchmarks, the proposed compiler can support the claim target.",
      "paperUrl": "https://doi.org/10.1109/icppw.2017.21",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "CUDA",
        "GPU",
        "OpenCL"
      ],
      "matchedAuthors": [
        "Jenq-Kuen Lee"
      ]
    },
    {
      "id": "openalex-w2746292459",
      "source": "openalex-discovery",
      "title": "Leveraging OpenMP 4.5 Support in CLANG for Fortran",
      "authors": [
        {
          "name": "Hyojin Sung",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Tong Chen",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Zehra Sura",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Tarique Islam",
          "affiliation": "IBM (United States)"
        }
      ],
      "year": "2017",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-319-65578-9_3",
      "sourceUrl": "",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Tarique Islam"
      ]
    },
    {
      "id": "openalex-w2586250227",
      "source": "openalex-discovery",
      "title": "Intel MPX Explained: An Empirical Study of Intel MPX and Software-based Bounds Checking Approaches",
      "authors": [
        {
          "name": "Oleksii Oleksenko",
          "affiliation": ""
        },
        {
          "name": "Dmitrii Kuvaiskii",
          "affiliation": ""
        },
        {
          "name": "Pramod Bhatotia",
          "affiliation": ""
        },
        {
          "name": "Pascal Felber",
          "affiliation": ""
        },
        {
          "name": "Christof Fetzer",
          "affiliation": ""
        }
      ],
      "year": "2017",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Memory-safety violations are a prevalent cause of both reliability and security vulnerabilities in systems software written in unsafe languages like C/C++. Unfortunately, all the existing software-based solutions to this problem exhibit high performance overheads preventing them from wide adoption in production runs. To address this issue, Intel recently released a new ISA extension - Memory Protection Extensions (Intel MPX), a hardware-assisted full-stack solution to protect against memory safety violations. In this work, we perform an exhaustive study of the Intel MPX architecture to understand its advantages and caveats. We base our study along three dimensions: (a) performance overheads, (b) security guarantees, and (c) usability issues. To put our results in perspective, we compare Intel MPX with three prominent software-based approaches: (1) trip-wire - AddressSanitizer, (2) object-based - SAFECode, and (3) pointer-based - SoftBound. Our main conclusion is that Intel MPX is a promising technique that is not yet practical for widespread adoption. Intel MPX's performance overheads are still high (roughly 50% on average), and the supporting infrastructure has bugs which may cause compilation or runtime errors. Moreover, we showcase the design limitations of Intel MPX: it cannot detect temporal errors, may have false positives and false negatives in multithreaded code, and its restrictions on memory layout require substantial code changes for some programs.",
      "paperUrl": "https://arxiv.org/pdf/1702.00719",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1702.00719",
      "tags": [
        "C++",
        "Infrastructure",
        "Performance",
        "Security"
      ],
      "matchedAuthors": [
        "Christof Fetzer",
        "Pascal Felber"
      ]
    },
    {
      "id": "openalex-w2760414717",
      "source": "openalex-discovery",
      "title": "Energy Consumption Measurement of C/C++ Programs Using Clang Tooling.",
      "authors": [
        {
          "name": "Mário Sérgio dos Santos",
          "affiliation": ""
        },
        {
          "name": "João Saraiva",
          "affiliation": ""
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": ""
        },
        {
          "name": "Dániel Krupp",
          "affiliation": ""
        }
      ],
      "year": "2017",
      "venue": "RepositóriUM (Universidade do Minho) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The green computing has an important role in today's software technology. Either speaking about small IoT devices or large cloud servers, there is a generic requirement of minimizing energy consumption. For this purpose, we usually first have to identify which parts of the system is responsible for the critical energy peaks. In this paper we suggest a new method to measure the energy consumption based on Low Level Virtual Machine (LLVM)/Clang tooling. The method has been tested on 2 open source systems and the output is visualized via the well-known Kcachegrind tool.",
      "paperUrl": "http://hdl.handle.net/1822/68731",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Dániel Krupp",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2786644468",
      "source": "openalex-discovery",
      "title": "Efficient Fork-Join on GPUs Through Warp Specialization",
      "authors": [
        {
          "name": "Arpith C. Jacob",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Alexandre E. Eichenberger",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Hyojin Sung",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Samuel Antão",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Carlo Bertolli",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Alexey Bataev",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Jin Tian",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Tong Chen",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Zehra Sura",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Georgios Rokos",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Kevin O’Brien",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        }
      ],
      "year": "2017",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Graphics Processing Units (GPUs) are increasingly used to accelerate portions of general-purpose applications. Higher level language extensions have been proposed to help non-experts bridge the gap between a host and the GPU's threading model. Recent updates to the OpenMP standard allow a user to parallelize code on a GPU using the well known fork-join programming model for CPUs. Mapping this model to the architecturally visible threading model of typical GPUs has been challenging. In this work we propose a novel approach using the technique of Warp Specialization. We show how to specialize one warp (a unit of 32 GPU threads) to handle sequential code on a GPU. When this master warp reaches a user-specified parallel region, it awakens unused GPU warps to collectively execute the parallel code. Based on this method, we have implemented a Clang-based, OpenMP 4.5 compliant, open source compiler for GPUs. Our work achieves a 3.6x (and up to 32x) performance improvement over a baseline that does not exploit fork-join parallelism on an NVIDIA k40m GPU across a set of 25 kernels. Compared to state-of-the-art compilers (Clang-ykt, GCC-OpenMP, GCC-OpenACC) our work is 2.1 - 7.6x faster. Our proposed technique is simpler to implement, robust, and performant.",
      "paperUrl": "https://doi.org/10.1109/hipc.2017.00048",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Alexey Bataev",
        "Carlo Bertolli",
        "Kevin O'Brien"
      ]
    },
    {
      "id": "openalex-w2625806818",
      "source": "openalex-discovery",
      "title": "Compiler-Agnostic Function Detection in Binaries",
      "authors": [
        {
          "name": "Dennis Andriesse",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Asia Slowinska",
          "affiliation": ""
        },
        {
          "name": "Herbert Bos",
          "affiliation": "Vrije Universiteit Amsterdam"
        }
      ],
      "year": "2017",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We propose Nucleus, a novel function detection algorithm for binaries. In contrast to prior work, Nucleus is compiler-agnostic, and does not require any learning phase or signature information. Instead of scanning for signatures, Nucleus detects functions at the Control Flow Graph-level, making it inherently suitable for difficult cases such as non-contiguous or multi-entry functions. We evaluate Nucleus on a diverse set of 476 C and C ++ binaries, compiled with GCC, clang and Visual Studio for x86 and x64, at optimization levels O0-O3. We achieve consistently good performance, with a mean F-score of 0.95.",
      "paperUrl": "https://hdl.handle.net/1871.1/f8e193bc-4a58-4ee8-90cf-350f242d8621",
      "sourceUrl": "https://doi.org/10.1109/eurosp.2017.11",
      "tags": [
        "Clang",
        "Performance"
      ],
      "matchedAuthors": [
        "Asia Slowinska",
        "Herbert Bos"
      ]
    },
    {
      "id": "openalex-w2292282166",
      "source": "openalex-discovery",
      "title": "gpucc: an open-source GPGPU compiler",
      "authors": [
        {
          "name": "Jingyue Wu",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Artem Belevich",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Eli Bendersky",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Mark Heffernan",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Chris Leary",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Jacques A. Pienaar",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Bjarke Roune",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Rob Springer",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Xuetian Weng",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Robert Hundt",
          "affiliation": "Google (United States)"
        }
      ],
      "year": "2016",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Graphics Processing Units have emerged as powerful accelerators for massively parallel, numerically intensive workloads. The two dominant software models for these devices are NVIDIA's CUDA and the cross-platform OpenCL standard. Until now, there has not been a fully open-source compiler targeting the CUDA environment, hampering general compiler and architecture research and making deployment difficult in datacenter or supercomputer environments. In this paper, we present gpucc, an LLVM-based, fully open-source, CUDA compatible compiler for high performance computing. It performs various general and CUDA-specific optimizations to generate high performance code. The Clang-based frontend supports modern language features such as those in C++11 and C++14. Compile time is 8% faster than NVIDIA's toolchain (nvcc) and it reduces compile time by up to 2.4x for pathological compilations (>100 secs), which tend to dominate build times in parallel build environments. Compared to nvcc, gpucc's runtime performance is on par for several open-source benchmarks, such as Rodinia (0.8% faster), SHOC (0.5% slower), or Tensor (3.7% faster). It outperforms nvcc on internal large-scale end-to-end benchmarks by up to 51.0%, with a geometric mean of 22.9%.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/2854038.2854041?download=true",
      "sourceUrl": "https://doi.org/10.1145/2854038.2854041",
      "tags": [
        "Clang",
        "CUDA",
        "Frontend",
        "OpenCL",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Bjarke Roune",
        "Chris Leary",
        "Eli Bendersky",
        "Jingyue Wu",
        "Mark Heffernan",
        "Robert Hundt"
      ]
    },
    {
      "id": "openalex-w2578771209",
      "source": "openalex-discovery",
      "title": "VTrust: Regaining Trust on Virtual Calls",
      "authors": [
        {
          "name": "Chao Zhang",
          "affiliation": "Berkeley College"
        },
        {
          "name": "Scott Carr",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Tongxin Li",
          "affiliation": "Peking University"
        },
        {
          "name": "Yu Ding",
          "affiliation": "Peking University"
        },
        {
          "name": "Chengyu Song",
          "affiliation": "Georgia Institute of Technology"
        },
        {
          "name": "Mathias Payer",
          "affiliation": "Purdue University West Lafayette"
        },
        {
          "name": "Dawn Song",
          "affiliation": "University of California, Berkeley"
        }
      ],
      "year": "2016",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Virtual function calls are one of the most popular control-flow hijack attack targets.Compilers use a virtual function pointer table, called a VTable, to dynamically dispatch virtual function calls.These VTables are read-only, but pointers to them are not.VTable pointers reside in objects that are writable, allowing attackers to overwrite them.As a result, attackers can divert the control-flow of virtual function calls and launch VTable hijacking attacks.Researchers have proposed several solutions to protect virtual calls.However, they either incur high performance overhead or fail to defeat some VTable hijacking attacks.In this paper, we propose a lightweight defense solution, VTrust, to protect all virtual function calls from VTable hijacking attacks.It consists of two independent layers of defenses: virtual function type enforcement and VTable pointer sanitization.Combined with modern compilers' default configuration, i.e., placing VTables in read-only memory, VTrust can defeat all VTable hijacking attacks and supports modularity, allowing us to harden applications module by module.We have implemented a prototype on the LLVM compiler framework.Our experiments show that this solution only introduces a low performance overhead, and it defeats real world VTable hijacking attacks.",
      "paperUrl": "https://doi.org/10.14722/ndss.2016.23164",
      "sourceUrl": "",
      "tags": [
        "Performance",
        "Rust"
      ],
      "matchedAuthors": [
        "Chao Zhang",
        "Dawn Song",
        "Mathias Payer"
      ]
    },
    {
      "id": "openalex-w2503256241",
      "source": "openalex-discovery",
      "title": "Source Transformation of C++ Codes for Compatibility with Operator Overloading",
      "authors": [
        {
          "name": "Alexander Hück",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Jean Utke",
          "affiliation": "Allstate (United States)"
        },
        {
          "name": "Christian Bischof",
          "affiliation": "Technical University of Darmstadt"
        }
      ],
      "year": "2016",
      "venue": "Procedia Computer Science | Vol. 80 (Issue None)",
      "type": "research-paper",
      "abstract": "In C++, new features and semantics can be added to an existing software package without sweeping code changes by introducing a user-defined type using operator overloading. This approach is used, for example, to add capabilities such as algorithmic differentiation. However, the introduction of operator overloading can cause a multitude of compilation errors. In a previous paper, we identified code constructs that cause a violation of the C++ language standard after a type change, and a tool called OO-Lint based on the Clang compiler that identifies these code constructs with lint-like messages. In this paper, we present an extension of this work that automatically transforms such problematic code constructs in order to make an existing code base compatible with a semantic augmentation through operator overloading. We applied our tool to the CFD software OpenFOAM and detected and transformed 23 instances of problematic code constructs in 160,000 lines of code. A significant amount of these root causes are included up to 425 times in other files causing a tremendous compiler error amplification. In addition, we show the significance of our work with a case study of the evolution of the ice flow modeling software ISSM, comparing a recent version which was manually type changed with a legacy version. The recent version shows no signs of problematic code constructs. In contrast, our tool detected and transformed a remarkable amount of issues in the legacy version that previously had to be manually located and fixed.",
      "paperUrl": "https://doi.org/10.1016/j.procs.2016.05.470",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang"
      ],
      "matchedAuthors": [
        "Alexander Hück"
      ]
    },
    {
      "id": "openalex-w2499909786",
      "source": "openalex-discovery",
      "title": "Selfrando: Securing the Tor Browser against De-anonymization Exploits",
      "authors": [
        {
          "name": "Mauro Conti",
          "affiliation": "University of Padua"
        },
        {
          "name": "Stephen Crane",
          "affiliation": ""
        },
        {
          "name": "Tommaso Frassetto",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Andrei Homescu",
          "affiliation": ""
        },
        {
          "name": "Georg Koppen",
          "affiliation": "The Tor Project"
        },
        {
          "name": "Per Larsen",
          "affiliation": ""
        },
        {
          "name": "Christopher Liebchen",
          "affiliation": "Technical University of Darmstadt"
        },
        {
          "name": "Mike Perry",
          "affiliation": "The Tor Project"
        },
        {
          "name": "Ahmad‐Reza Sadeghi",
          "affiliation": "Technical University of Darmstadt"
        }
      ],
      "year": "2016",
      "venue": "Proceedings on Privacy Enhancing Technologies | Vol. 2016 (Issue 4)",
      "type": "research-paper",
      "abstract": "Abstract Tor is a well-known anonymous communication system used by millions of users, including journalists and civil rights activists all over the world. The Tor Browser gives non-technical users an easy way to access the Tor Network. However, many government organizations are actively trying to compromise Tor not only in regions with repressive regimes but also in the free world, as the recent FBI incidents clearly demonstrate. Exploiting software vulnerabilities in general, and browser vulnerabilities in particular, constitutes a clear and present threat to the Tor software. The Tor Browser shares a large part of its attack surface with the Firefox browser. Therefore, Firefox vulnerabilities (even patched ones) are highly valuable to attackers trying to monitor users of the Tor Browser. In this paper, we present selfrando-an enhanced and practical load-time randomization technique for the Tor Browser that defends against exploits, such as the one FBI allegedly used against Tor users. Our solution significantly improves security over standard address space layout randomization (ASLR) techniques currently used by Firefox and other mainstream browsers. Moreover, we collaborated closely with the Tor Project to ensure that selfrando is fully compatible with AddressSanitizer (ASan), a compiler feature to detect memory corruption. ASan is used in a hardened version of Tor Browser for test purposes. The Tor Project decided to include our solution in the hardened releases of the Tor Browser, which is currently undergoing field testing.",
      "paperUrl": "https://doi.org/10.1515/popets-2016-0050",
      "sourceUrl": "",
      "tags": [
        "Security",
        "Testing"
      ],
      "matchedAuthors": [
        "Andrei Homescu",
        "Per Larsen"
      ]
    },
    {
      "id": "openalex-w2397875151",
      "source": "openalex-discovery",
      "title": "SMACK software verification toolchain",
      "authors": [
        {
          "name": "Montgomery Carter",
          "affiliation": "University of Utah"
        },
        {
          "name": "Shaobo He",
          "affiliation": "University of Utah"
        },
        {
          "name": "Jonathan Whitaker",
          "affiliation": "University of Utah"
        },
        {
          "name": "Zvonimir Rakamarić",
          "affiliation": "University of Utah"
        },
        {
          "name": "Michael Emmi",
          "affiliation": "IMDEA Software"
        }
      ],
      "year": "2016",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Tool prototyping is an essential step in developing novel software verification algorithms and techniques. However, implementing a verifier prototype that can handle real-world programs is a huge endeavor, which hinders researchers by forcing them to spend more time engineering tools, and less time innovating. In this paper, we present the SMACK software verification toolchain. The toolchain provides a modular and extensible software verification ecosystem that decouples the front-end source language details from backend verification algorithms. It achieves that by translating from the LLVM compiler intermediate representation into the Boogie intermediate verification language. SMACK benefits the software verification community in several ways: (i) it can be used as an off-the-shelf software verifier in an applied software verification project, (ii) it enables researchers to rapidly develop and release new verification algorithms, (iii) it allows for adding support for new languages in its front-end. We have used SMACK to verify numerous C/C++ programs, including industry examples, showing it is mature and competitive. Likewise, SMACK is already being used in several existing verification research prototypes. Our demonstration of SMACK can be found on YouTube at the following address: https://youtu.be/SPPSC1KdRzs",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/2889160.2889163?download=true",
      "sourceUrl": "https://doi.org/10.1145/2889160.2889163",
      "tags": [
        "Backend",
        "C++"
      ],
      "matchedAuthors": [
        "Michael Emmi",
        "Shaobo He"
      ]
    },
    {
      "id": "openalex-w2546764864",
      "source": "openalex-discovery",
      "title": "Performance analysis and optimization of Clang's OpenMP 4.5 GPU support",
      "authors": [
        {
          "name": "Matt Martineau",
          "affiliation": "University of Bristol"
        },
        {
          "name": "Simon McIntosh‐Smith",
          "affiliation": "University of Bristol"
        },
        {
          "name": "Carlo Bertolli",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Arpith C. Jacob",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Samuel Antão",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Alexandre E. Eichenberger",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Tong Chen",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Jin Tian",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Kevin O’Brien",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Georgios Rokos",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Hyojin Sung",
          "affiliation": "IBM (United States)"
        },
        {
          "name": "Zehra Sura",
          "affiliation": "IBM (United States)"
        }
      ],
      "year": "2016",
      "venue": "IEEE International Conference on High Performance Computing, Data, and Analytics | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The Clang implementation of OpenMP® 4.5 now provides full support for the specification, offering the only open source option for targeting NVIDIA® GPUs. While using OpenMP allows portability across different architectures, matching native CUDA® performance without major code restructuring is an open research issue.In order to analyze the current performance, we port a suite of representative benchmarks, and the mature mini-apps TeaLeaf, CloverLeaf, and SNAP to the Clang OpenMP 4.5 compiler. We then collect performance results for those ports, and their equivalent CUDA ports, on an NVIDIA Kepler GPU. Through manual analysis of the generated code, we are able to discover the root cause of the performance differences between OpenMP and CUDA.A number of improvements can be made to the existing compiler implementation to enable performance that approaches that of hand-optimized CUDA. Our first observation was that the generated code did not use fused-multiply-add instructions, which was resolved using an existing flag. Next we saw that the compiler was not passing any loads through non-coherent cache, and added a new flag to the compiler to assist with this problem.We then observed that the compiler partitioning of threads and teams could be improved upon for the majority of kernels, which guided work to ensure that the compiler can pick more optimal defaults. We uncovered a register allocation issue with the existing implementation that, when fixed alongside the other issues, enables performance that is close to CUDA.Finally, we use some different kernels to emphasize that support for managing memory hierarchies needs to be introduced into the specification, and propose a simple option for programming shared caches.",
      "paperUrl": "http://ieeexplore.ieee.org/document/7836414/",
      "sourceUrl": "https://doi.org/10.5555/3019057.3019063",
      "tags": [
        "Clang",
        "CUDA",
        "GPU",
        "Performance"
      ],
      "matchedAuthors": [
        "Carlo Bertolli",
        "Kevin O'Brien"
      ]
    },
    {
      "id": "openalex-w2332117786",
      "source": "openalex-discovery",
      "title": "LifeJacket: verifying precise floating-point optimizations in LLVM",
      "authors": [
        {
          "name": "Andres Nötzli",
          "affiliation": "Stanford University"
        },
        {
          "name": "Fraser Brown",
          "affiliation": "Stanford University"
        }
      ],
      "year": "2016",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Users depend on correct compiler optimizations but floating-point arithmetic is difficult to optimize transparently. Manually reasoning about all of floating-point arithmetic's esoteric properties is error-prone and increases the cost of adding new optimizations. We present an approach to automate reasoning about precise floating-point optimizations using satisfiability modulo theories (SMT) solvers. We implement the approach in LifeJacket, a system for automatically verifying precise floating-point optimizations for the LLVM assembly language. We have used LifeJacket to verify 43 LLVM optimizations and to discover eight incorrect ones, including three previously unreported problems. LifeJacket is an open source extension of the Alive system for optimization verification.",
      "paperUrl": "http://dl.acm.org/ft_gateway.cfm?id=2931024&type=pdf",
      "sourceUrl": "https://doi.org/10.1145/2931021.2931024",
      "tags": [
        "Optimizations"
      ],
      "matchedAuthors": [
        "Fraser Brown"
      ]
    },
    {
      "id": "openalex-w2561741525",
      "source": "openalex-discovery",
      "title": "Exploring compiler optimization opportunities for the OpenMP 4.x accelerator model on a POWER8+GPU platform",
      "authors": [
        {
          "name": "Akihiro Hayashi",
          "affiliation": "Rice University"
        },
        {
          "name": "Jun Shirako",
          "affiliation": "Rice University"
        },
        {
          "name": "Ettore Tiotto",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Robert Ho",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Vivek Sarkar",
          "affiliation": "Rice University"
        }
      ],
      "year": "2016",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "While GPUs are increasingly popular for high-performance computing, optimizing the performance of GPU programs is a time-consuming and non-trivial process in general. This complexity stems from the low abstraction level of standard GPU programming models such as CUDA and OpenCL: programmers are required to orchestrate low-level operations in order to exploit the full capability of GPUs. In terms of software productivity and portability, a more attractive approach would be to facilitate GPU programming by providing high-level abstractions for expressing parallel algorithms.OpenMP is a directive-based shared memory parallel programming model and has been widely used for many years. From OpenMP 4.0 onwards, GPU platforms are supported by extending OpenMP's high-level parallel abstractions with accelerator programming. This extension allows programmers to write GPU programs in standard C/C++ or Fortran languages, without exposing too many details of GPU architectures.However, such high-level parallel programming strategies generally impose additional program optimizations on compilers, which could result in lower performance than fully hand-tuned code with low-level programming models. To study potential performance improvements by compiling and optimizing high-level GPU programs, in this paper, we 1) evaluate a set of OpenMP 4.× benchmarks on an IBM POWER8 and NVIDIA Tesla GPU platform and 2) conduct a comparable performance analysis among hand-written CUDA and automatically-generated GPU programs by the IBM XL and clang/LLVM compilers.",
      "paperUrl": "http://ieeexplore.ieee.org/document/7836582/",
      "sourceUrl": "https://doi.org/10.5555/3019120.3019127",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "GPU",
        "OpenCL",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Ettore Tiotto"
      ]
    },
    {
      "id": "openalex-w4249705231",
      "source": "openalex-discovery",
      "title": "Exploring Compiler Optimization Opportunities for the OpenMP 4.× Accelerator Model on a POWER8+GPU Platform",
      "authors": [
        {
          "name": "Akihiro Hayashi",
          "affiliation": ""
        },
        {
          "name": "Jun Shirako",
          "affiliation": "Rice University"
        },
        {
          "name": "Ettore Tiotto",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Robert Ho",
          "affiliation": "IBM (Canada)"
        },
        {
          "name": "Vivek Sarkar",
          "affiliation": "Rice University"
        }
      ],
      "year": "2016",
      "venue": "Vol. 5 (Issue None)",
      "type": "research-paper",
      "abstract": "While GPUs are increasingly popular for high-performance computing, optimizing the performance of GPU programs is a time-consuming and non-trivial process in general. This complexity stems from the low abstraction level of standard GPU programming models such as CUDA and OpenCL: programmers are required to orchestrate low-level operations in order to exploit the full capability of GPUs. In terms of software productivity and portability, a more attractive approach would be to facilitate GPU programming by providing high-level abstractions for expressing parallel algorithms.OpenMP is a directive-based shared memory parallel programming model and has been widely used for many years. From OpenMP 4.0 onwards, GPU platforms are supported by extending OpenMP's high-level parallel abstractions with accelerator programming. This extension allows programmers to write GPU programs in standard C/C++ or Fortran languages, without exposing too many details of GPU architectures.However, such high-level parallel programming strategies generally impose additional program optimizations on compilers, which could result in lower performance than fully hand-tuned code with low-level programming models. To study potential performance improvements by compiling and optimizing high-level GPU programs, in this paper, we 1) evaluate a set of OpenMP 4.× benchmarks on an IBM POWER8 and NVIDIA Tesla GPU platform and 2) conduct a comparable performance analysis among hand-written CUDA and automatically-generated GPU programs by the IBM XL and clang/LLVM compilers.",
      "paperUrl": "https://doi.org/10.1109/waccpd.2016.011",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "GPU",
        "OpenCL",
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Ettore Tiotto"
      ]
    },
    {
      "id": "openalex-w2571316656",
      "source": "openalex-discovery",
      "title": "Dynamic compilation of SQL queries for PostgreSQL",
      "authors": [
        {
          "name": "Ruben Buchatskiy",
          "affiliation": ""
        },
        {
          "name": "E.Y. Sharygin",
          "affiliation": ""
        },
        {
          "name": "Leonid Vladlenovich Skvortsov",
          "affiliation": ""
        },
        {
          "name": "Roman Zhuykov",
          "affiliation": ""
        },
        {
          "name": "Dmitry Melnik",
          "affiliation": ""
        },
        {
          "name": "Roman Vyacheslavovich Baev",
          "affiliation": ""
        }
      ],
      "year": "2016",
      "venue": "Proceedings of the Institute for System Programming of RAS | Vol. 28 (Issue 6)",
      "type": "research-paper",
      "abstract": "In recent years, as performance and capacity of main and external memory grow, performance of database management systems (DBMSes) on certain kinds of queries is more determined by raw CPU speed. Currently, PostgreSQL uses the interpreter to execute SQL queries. This yields an overhead caused by indirect calls to handler functions and runtime checks, which could be avoided if the query were compiled into native code \"on-the-fly\", i.e. just-in-time (JIT) compiled: at run time the specific table structure is known as well as data types and built-in functions used in the query as well as the query itself. This is especially important for complex queries, performance of which is CPU-bound. We have developed a PostgreSQL extension that implements SQL query JIT compilation using LLVM compiler infrastructure. In this paper we show how to implement LLVM-analogues of the main operators of the PostgreSQL, how to replace Volcano iterator model abstraction (open(), next(), close()) by the abstraction that is more suitable to generate code for a particular query. Currently, with LLVM JIT we achieve up to 4.3x speedup on TPC-H Q1 query as compared to original PostgreSQL interpreter.",
      "paperUrl": "http://www.ispras.ru/proceedings/docs/2016/28/6/isp_28_2016_6_37.pdf",
      "sourceUrl": "https://doi.org/10.15514/ispras-2016-28(6)-3",
      "tags": [
        "Infrastructure",
        "JIT",
        "Performance"
      ],
      "matchedAuthors": [
        "Dmitry Melnik"
      ]
    },
    {
      "id": "openalex-w2562603934",
      "source": "openalex-discovery",
      "title": "A Novel Data Race Detection Approach based on Buddy Memory Allocator",
      "authors": [
        {
          "name": "Zhengyang Liu",
          "affiliation": "Beijing University of Posts and Telecommunications"
        },
        {
          "name": "Hua Zhang",
          "affiliation": "Beijing University of Posts and Telecommunications"
        }
      ],
      "year": "2016",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "As a common problem of multi-core parallel programs, the problem of data race has been paid more and more attention in recent years.In this paper, a dynamic detection approach for data race problem detection is proposed.By introducing a new metadata storage based on the buddy memory allocator, the metadata access performance is improved significantly.A specific implementation of the approach based on LLVM compiler infrastructure is made.The experimental results show that the proposed approach can reduce the time cost of dynamic race detection and achieve 2x-5x performance on the Olden benchmark.",
      "paperUrl": "https://download.atlantis-press.com/article/25866516.pdf",
      "sourceUrl": "https://doi.org/10.2991/aiea-16.2016.88",
      "tags": [
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Zhengyang Liu"
      ]
    },
    {
      "id": "openalex-w2021935721",
      "source": "openalex-discovery",
      "title": "Towards deployment-time dynamic analysis of server applications",
      "authors": [
        {
          "name": "Luís Pina",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Cristian Cadar",
          "affiliation": "Imperial College London"
        }
      ],
      "year": "2015",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Bug-finding tools based on dynamic analysis (DA), such as Valgrind or the compiler sanitizers provided by Clang and GCC, have become ubiquitous during software development. These analyses are precise but incur a large performance overhead (often several times slower than native execution), which makes them prohibitively expensive to use in production. In this work, we investigate the exciting possibility of deploying such dynamic analyses in production code, using a multi-version execution approach.",
      "paperUrl": "https://doi.org/10.1145/2823363.2823372",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Dynamic Analysis",
        "Performance"
      ],
      "matchedAuthors": [
        "Cristian Cadar"
      ]
    },
    {
      "id": "openalex-w2539700731",
      "source": "openalex-discovery",
      "title": "Tool for detecting standardwise differences in C++ legacy code",
      "authors": [
        {
          "name": "Tibor Brunner",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Norbert Pataki",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2015",
      "venue": "Vol. 2 (Issue None)",
      "type": "research-paper",
      "abstract": "Programming languages are continuously evolving as the experiences are accumulated, developers face new problems and other requirements such as increasing support for multi-threading is emerging. These changes are reflected in new language standards and compiler versions. Although these changes are carefully planned to keep reverse compatibility with previous versions to keep the syntax and the semantics of earlier written code, sometimes languages break this rule. In case of silent semantic changes, when the earlier written code is recompiled with the new version, this is especially harmful. The new C++11 standard introduced major changes in the core language. This changes are widely believed to be reverse compatible, i.e. a simple recompilation of earlier written code will keep the old semantics. Recently we found examples that the backward compatibility between language versions is broken. The previously written code compiled with a new C++ compiler may change the program behaviour without any compiler diagnostic message. In a large code base such issues are very hard to catch by manual inspection, therefore some automatic tool support is required for this purpose. In this paper we propose a tool support to detect such backward incompatibilities in C++. The basic idea is to parse the source code using different standards, and then compare the abstract syntax trees. We implemented a proof of concept prototype tool to demonstrate our idea based on the LLVM/Clang compiler infrastructure.",
      "paperUrl": "https://doi.org/10.1109/informatics.2015.7377808",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Infrastructure",
        "Programming Languages"
      ],
      "matchedAuthors": [
        "Tibor Brunner",
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2154108476",
      "source": "openalex-discovery",
      "title": "Static analysis of energy consumption for LLVM IR programs",
      "authors": [
        {
          "name": "Neville Grech",
          "affiliation": "University of Bristol"
        },
        {
          "name": "Kyriakos Georgiou",
          "affiliation": "University of Bristol"
        },
        {
          "name": "James Pallister",
          "affiliation": "University of Bristol"
        },
        {
          "name": "Steve Kerrison",
          "affiliation": "University of Bristol"
        },
        {
          "name": "Jeremy Morse",
          "affiliation": "University of Bristol"
        },
        {
          "name": "Kerstin Eder",
          "affiliation": "University of Bristol"
        }
      ],
      "year": "2015",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Energy models can be constructed by characterizing the energy consumed by executing each instruction in a processor's instruction set. This can be used to determine how much energy is required to execute a sequence of assembly instructions, without the need to instrument or measure hardware. However, statically analyzing low-level program structures is hard, and the gap between the high-level program structure and the low-level energy models needs to be bridged. We have developed techniques for performing a static analysis on the intermediate compiler representations of a program. Specifically, we target LLVM IR, a representation used by modern compilers, including Clang. Using these techniques we can automatically infer an estimate of the energy consumed when running a function under different platforms, using different compilers. One of the challenges in doing so is that of determining an energy cost of executing LLVM IR program segments, for which we have developed two different approaches. When this information is used in conjunction with our analysis, we are able to infer energy formulae that characterize the energy consumption for a particular program. This approach can be applied to any languages targeting the LLVM toolchain, including C and XC or architectures such as ARM Cortex-M or XMOS xCORE, with a focus towards embedded platforms. Our techniques are validated on these platforms by comparing the static analysis results to the physical measurements taken from the hardware. Static energy consumption estimation enables energy-aware software development, without requiring hardware knowledge.",
      "paperUrl": "https://arxiv.org/pdf/1405.4565",
      "sourceUrl": "https://doi.org/10.1145/2764967.2764974",
      "tags": [
        "Clang",
        "Embedded",
        "IR",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Jeremy Morse",
        "Neville Grech"
      ]
    },
    {
      "id": "openalex-w2075073133",
      "source": "openalex-discovery",
      "title": "Optimistic Delinearization of Parametrically Sized Arrays",
      "authors": [
        {
          "name": "Tobias Grosser",
          "affiliation": "ETH Zurich"
        },
        {
          "name": "J. Ramanujam",
          "affiliation": "Louisiana State University"
        },
        {
          "name": "Louis-Noël Pouchet",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "P. Sadayappan",
          "affiliation": "The Ohio State University"
        },
        {
          "name": "Sebastian Pop",
          "affiliation": "Samsung (United States)"
        }
      ],
      "year": "2015",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "A number of legacy codes make use of linearized array references (i.e., references to one-dimensional arrays) to encode accesses to multi-dimensional arrays. This is also true of a number of optimized libraries and the well-known LLVM intermediate representation, which linearize array accesses. In many cases, the only information available is an array base pointer and a single dimensional offset. For problems with parametric array extents, this offset is usually a multivariate polynomial. Compiler analyses such as data dependence analysis are impeded because the standard formulations with integer linear programming (ILP) solvers cannot be used. In this paper, we present an approach to delinearization, i.e., recovering the multi-dimensional nature of accesses to arrays of parametric size. In case of insufficient static information, the developed algorithm produces run-time conditions to validate the recovered multi-dimensional form. The obtained access description enhances the precision of data dependence analysis. Experimental evaluation in the context of the LLVM/Polly system using a number of benchmarks reveals significant performance benefits due to increased precision of dependence analysis and enhanced optimization opportunities that are exploited by the compiler after delinearization.",
      "paperUrl": "http://dl.acm.org/ft_gateway.cfm?id=2751248&type=pdf",
      "sourceUrl": "https://doi.org/10.1145/2751205.2751248",
      "tags": [
        "Libraries",
        "Performance",
        "Polly"
      ],
      "matchedAuthors": [
        "Sebastian Pop",
        "Tobias Grosser"
      ]
    },
    {
      "id": "openalex-w4240590466",
      "source": "openalex-discovery",
      "title": "MemorySanitizer: Fast detector of uninitialized memory use in C&#x002B;&#x002B;",
      "authors": [
        {
          "name": "Evgeniy Stepanov",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Konstantin Serebryany",
          "affiliation": "Google (United States)"
        }
      ],
      "year": "2015",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper presents MemorySanitizer, a dynamic tool that detects uses of uninitialized memory in C and C++. The tool is based on compile time instrumentation and relies on bit-precise shadow memory at run-time. Shadow propagation technique is used to avoid false positive reports on copying of uninitialized memory. MemorySanitizer finds bugs at a modest cost of 2.5× in execution time and 2× in memory usage; the tool has an optional origin tracking mode that provides better reports with moderate extra overhead. The reports with origins are more detailed compared to reports from other similar tools; such reports contain names of local variables and the entire history of the uninitialized memory including intermediate stores. In this paper we share our experience in deploying the tool at a large scale and demonstrate the benefits of compile-time instrumentation over dynamic binary instrumentation.",
      "paperUrl": "https://doi.org/10.1109/cgo.2015.7054186",
      "sourceUrl": "",
      "tags": [
        "C++"
      ],
      "matchedAuthors": [
        "Evgeniy Stepanov",
        "Konstantin Serebryany"
      ]
    },
    {
      "id": "openalex-w2180288011",
      "source": "openalex-discovery",
      "title": "Integrating GPU support for OpenMP offloading directives into Clang",
      "authors": [
        {
          "name": "Carlo Bertolli",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Samuel Antão",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Gheorghe-Teodor Bercea",
          "affiliation": "Imperial College London"
        },
        {
          "name": "Arpith C. Jacob",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Alexandre E. Eichenberger",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Tong Chen",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Zehra Sura",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Hyojin Sung",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Georgios Rokos",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "David Appelhans",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        },
        {
          "name": "Kevin O’Brien",
          "affiliation": "IBM Research - Thomas J. Watson Research Center"
        }
      ],
      "year": "2015",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "The LLVM community is currently developing OpenMP 4.1 support, consisting of software improvements for Clang and new runtime libraries. OpenMP 4.1 includes offloading constructs that permit execution of user selected regions on generic devices, external to the main host processor. This paper describes our ongoing work towards delivering support for OpenMP offloading constructs for the OpenPower system into the LLVM compiler infrastructure. We previously introduced a design for a control loop scheme necessary to implement the OpenMP generic offloading model on NVIDIA GPUs. In this paper we show how we integrated the complexity of the control loop into Clang by limiting its support to OpenMP-related functionality. We also synthetically report the results of performance analysis on benchmarks and a complex application kernel. We show an optimization in the Clang code generation scheme for specific code patterns, alternative to the control loop, which delivers improved performance.",
      "paperUrl": "https://doi.org/10.1145/2833157.2833161",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "GPU",
        "Infrastructure",
        "Libraries",
        "Performance"
      ],
      "matchedAuthors": [
        "Carlo Bertolli",
        "Kevin O'Brien"
      ]
    },
    {
      "id": "openalex-w2610904413",
      "source": "openalex-discovery",
      "title": "Implementation of memory scalability approach for llvm-based link-time optimization and static analyzing systems",
      "authors": [
        {
          "name": "Ksenia Dolgorukova",
          "affiliation": "Institute for System Programming"
        }
      ],
      "year": "2015",
      "venue": "Proceedings of the Institute for System Programming of RAS | Vol. 27 (Issue 6)",
      "type": "research-paper",
      "abstract": "Link-time optimization and static analyzing systems scalability problem is of current importance: in spite of growth of performance and memory volume of modern computers programs grow in size and complexity as much. In particular, this is actual for such complex and large programs as browsers, operation systems, etc. This paper introduces memory scalability approach for LLVM-based link-time optimization system and proposes technique for applying this approach to static analyzing systems. Proposed approach was implemented and tested on SPEC CPU2000 benchmark suite [2].",
      "paperUrl": "https://ispranproceedings.elpub.ru/jour/article/download/916/641",
      "sourceUrl": "https://doi.org/10.15514/ispras-2015-27(6)-7",
      "tags": [
        "Performance"
      ],
      "matchedAuthors": [
        "Ksenia Dolgorukova"
      ]
    },
    {
      "id": "openalex-w2298801862",
      "source": "openalex-discovery",
      "title": "ITensor: Switch to C++11, Storage Redesign, Fixed Index Comparisons, AutoMPO",
      "authors": [
        {
          "name": "Miles",
          "affiliation": "UC Irvine Health"
        },
        {
          "name": "Lucas O. Wagner",
          "affiliation": ""
        },
        {
          "name": "jgukelberger",
          "affiliation": ""
        },
        {
          "name": "SDepenbrock",
          "affiliation": "University of Michigan–Ann Arbor"
        },
        {
          "name": "Guy Cohen",
          "affiliation": "Columbia University"
        },
        {
          "name": "srwhite",
          "affiliation": "GTx (United States)"
        },
        {
          "name": "Erik Schnetter",
          "affiliation": ""
        },
        {
          "name": "Jessica Alfonsi",
          "affiliation": ""
        },
        {
          "name": "Andrey E. Antipov",
          "affiliation": "Perimeter Institute"
        }
      ],
      "year": "2015",
      "venue": "Zenodo (CERN European Organization for Nuclear Research) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This major new release of ITensor is the first official release to require C++11 compiler support (approximately the following compiler versions: gcc &gt; v4.6; icpc &gt; v15; clang &gt; v2.9). Requiring C++11 allows us to start making many key improvements to ITensor such as allowing arbitrary numbers of function arguments; improving memory safety through features such as unique_ptr and shared_ptr; and removing dependence on boost. In this release many internals have been redesigned and many bugs fixed. The uniqueReal system for comparing indices is gone as it occasionally led to incorrect comparisons in codes using many indices. ITensors and IQTensor now have simpler data storage layouts. One major new feature is the AutoMPO class. AutoMPO provides a \"pencil and paper\" interface for creating MPOs, both for Hamiltonians and for other sums of local operators such as orbital creation/annihilation operators.",
      "paperUrl": "https://doi.org/10.5281/zenodo.17664",
      "sourceUrl": "",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Erik Schnetter"
      ]
    },
    {
      "id": "openalex-w2019119315",
      "source": "openalex-discovery",
      "title": "Generalized Task Parallelism",
      "authors": [
        {
          "name": "Kevin Streit",
          "affiliation": "Saarland University"
        },
        {
          "name": "Johannes Doerfert",
          "affiliation": "Saarland University"
        },
        {
          "name": "Clemens Hammacher",
          "affiliation": "Saarland University"
        },
        {
          "name": "Andreas Zeller",
          "affiliation": "Saarland University"
        },
        {
          "name": "Sebastian Hack",
          "affiliation": "Saarland University"
        }
      ],
      "year": "2015",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 12 (Issue 1)",
      "type": "research-paper",
      "abstract": "Existing approaches to automatic parallelization produce good results in specific domains. Yet, it is unclear how to integrate their individual strengths to match the demands and opportunities of complex software. This lack of integration has both practical reasons, as integrating those largely differing approaches into one compiler would impose an engineering hell, as well as theoretical reasons, as no joint cost model exists that would drive the choice between parallelization methods. By reducing the problem of generating parallel code from a program dependence graph to integer linear programming, &lt;i&gt;generalized task parallelization&lt;/i&gt; integrates central aspects of existing parallelization approaches into a single unified framework. Implemented on top of LLVM, the framework seamlessly integrates enabling technologies such as speculation, privatization, and the realization of reductions. Evaluating our implementation on various C programs from different domains, we demonstrate the effectiveness and generality of generalized task parallelization. On a quad-core machine with hyperthreading we achieve speedups of up to 4.6 ×.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/2723164",
      "sourceUrl": "https://doi.org/10.1145/2723164",
      "tags": [],
      "matchedAuthors": [
        "Clemens Hammacher",
        "Johannes Doerfert",
        "Sebastian Hack"
      ]
    },
    {
      "id": "openalex-w1904738922",
      "source": "openalex-discovery",
      "title": "CHERI: A Hybrid Capability-System Architecture for Scalable Software Compartmentalization",
      "authors": [
        {
          "name": "Robert N. M. Watson",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Jonathan Woodruff",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Peter G. Neumann",
          "affiliation": "SRI International"
        },
        {
          "name": "Simon W. Moore",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Jonathan Anderson",
          "affiliation": "Memorial"
        },
        {
          "name": "David Chisnall",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Nirav Dave",
          "affiliation": "SRI International"
        },
        {
          "name": "Brooks Davis",
          "affiliation": "SRI International"
        },
        {
          "name": "Khilan Gudka",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Ben Laurie",
          "affiliation": "Google (United Kingdom)"
        },
        {
          "name": "Steven J. Murdoch",
          "affiliation": "University College London"
        },
        {
          "name": "Robert M. Norton",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Michael Roe",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Stacey Son",
          "affiliation": "University of Cambridge"
        },
        {
          "name": "Munraj Vadera",
          "affiliation": "University of Cambridge"
        }
      ],
      "year": "2015",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "CHERI extends a conventional RISC Instruction-Set Architecture, compiler, and operating system to support fine-grained, capability-based memory protection to mitigate memory-related vulnerabilities in C-language TCBs. We describe how CHERI capabilities can also underpin a hardware-software object-capability model for application compartmentalization that can mitigate broader classes of attack. Prototyped as an extension to the open-source 64-bit BERI RISC FPGA soft-core processor, Free BSD operating system, and LLVM compiler, we demonstrate multiple orders-of-magnitude improvement in scalability, simplified programmability, and resulting tangible security benefits as compared to compartmentalization based on pure Memory-Management Unit (MMU) designs. We evaluate incrementally deployable CHERI-based compartmentalization using several real-world UNIX libraries and applications.",
      "paperUrl": "https://ieeexplore.ieee.org/ielx7/7160813/7163005/07163016.pdf",
      "sourceUrl": "https://doi.org/10.1109/sp.2015.9",
      "tags": [
        "Libraries",
        "Security"
      ],
      "matchedAuthors": [
        "Brooks Davis",
        "David Chisnall",
        "Jonathan Anderson",
        "Jonathan Woodruff",
        "Khilan Gudka",
        "Michael Roe",
        "Munraj Vadera",
        "Peter G. Neumann",
        "Robert N. M. Watson",
        "Simon W. Moore"
      ]
    },
    {
      "id": "openalex-w2405775600",
      "source": "openalex-discovery",
      "title": "Automatic Checking of the Usage of the C++11 Move Semantics",
      "authors": [
        {
          "name": "Áron Baráth",
          "affiliation": "Eötvös Loránd University"
        },
        {
          "name": "Zoltán Porkoláb",
          "affiliation": "Eötvös Loránd University"
        }
      ],
      "year": "2015",
      "venue": "Acta Cybernetica | Vol. 22 (Issue 1)",
      "type": "research-paper",
      "abstract": "The C++ programming language is a favorable choice when implementing high performance applications, like real-time and embedded programming, large telecommunication systems, financial simulations, as well as a wide range of other speed sensitive programs. While C++ has all the facilities to handle the computer hardware without compromises, the copy based value semantics of assignment is a common source of performance degradation. New language features, like the move semantics were introduced recently to serve an instrument to avoid unnecessary copies. Unfortunately, correct usage of move semantics is not trivial, and unintentional expensive copies of C++ objects - like copying containers instead of using move semantics - may determine the main (worst-case) time characteristics of the programs. In this paper we introduce a new approach of investigating performance bottlenecks for C++ programs, which operates at language source level and targets the move semantics of the C++ programming language. We detect copies occurring in operations marked as move operations, i.e. intended not containing expensive copy actions. Move operations are marked with generalized attributes - a new language feature introduced to C++11 standard. We implemented a tool prototype to detect such copy/move semantic errors in C++ programs. Our prototype is using the open source LLVM/Clang parser infrastructure, therefore highly portable.",
      "paperUrl": "https://cyber.bibl.u-szeged.hu/index.php/actcybern/article/download/3866/3850",
      "sourceUrl": "https://doi.org/10.14232/actacyb.22.1.2015.2",
      "tags": [
        "C++",
        "Clang",
        "Embedded",
        "Infrastructure",
        "Performance"
      ],
      "matchedAuthors": [
        "Zoltán Porkoláb"
      ]
    },
    {
      "id": "openalex-w2223901015",
      "source": "openalex-discovery",
      "title": "Perfrewrite -- Program Complexity Analysis via Source Code Instrumentation",
      "authors": [
        {
          "name": "Michael Kruse",
          "affiliation": "Université Paris Cité"
        }
      ],
      "year": "2014",
      "venue": "arXiv (Cornell University) | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Most program profiling methods output the execution time of one specific program execution, but not its computational complexity class in terms of the big-O notation. Perfrewrite is a tool based on LLVM's Clang compiler to rewrite a program such that it tracks semantic information while the program executes and uses it to guess memory usage, communication and computational complexity. While source code instrumentation is a standard technique for profiling, using it for deriving formulas is an uncommon approach.",
      "paperUrl": "https://arxiv.org/pdf/1409.2089",
      "sourceUrl": "https://doi.org/10.48550/arxiv.1409.2089",
      "tags": [
        "Clang"
      ],
      "matchedAuthors": [
        "Michael Kruse"
      ]
    },
    {
      "id": "openalex-w2076812085",
      "source": "openalex-discovery",
      "title": "PACXX: Towards a Unified Programming Model for Programming Accelerators Using C++14",
      "authors": [
        {
          "name": "Michael Haidl",
          "affiliation": "University of Münster"
        },
        {
          "name": "Sergei Gorlatch",
          "affiliation": "University of Münster"
        }
      ],
      "year": "2014",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "We present PACXX -- a unified programming model for programming many-core systems that comprise accelerators like Graphics Processing Units (GPUs). One of the main difficulties of the current GPU programming is that two distinct programming models are required: the host code for the CPU is written in C/C++ with the restricted, C-like API for memory management, while the device code for the GPU has to be written using a device-dependent, explicitly parallel programming model, e.g., OpenCL or CUDA. This leads to long, poorly structured and error-prone codes. In PACXX, both host and device programs are written in the same programming language -- the newest C++14 standard, with all modern features including type inference (auto), variadic templates, generic lambda expressions, as well as STL containers and algorithms. We implement PACXX by a custom compiler (based on the Clang front-end and LLVM IR) and a runtime system, that together perform major tasks of memory management and data synchronization automatically and transparently for the programmer. We evaluate our approach by comparing it to CUDA and OpenCL regarding program size and target performance.",
      "paperUrl": "https://doi.org/10.1109/llvm-hpc.2014.9",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "CUDA",
        "GPU",
        "IR",
        "OpenCL",
        "Performance"
      ],
      "matchedAuthors": [
        "Michael Haidl",
        "Sergei Gorlatch"
      ]
    },
    {
      "id": "openalex-w255480450",
      "source": "openalex-discovery",
      "title": "FlipIt: An LLVM Based Fault Injector for HPC",
      "authors": [
        {
          "name": "Jon C. Calhoun",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Luke N. Olson",
          "affiliation": "University of Illinois Urbana-Champaign"
        },
        {
          "name": "Marc Snir",
          "affiliation": "University of Illinois Urbana-Champaign"
        }
      ],
      "year": "2014",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://link.springer.com/content/pdf/10.1007/978-3-319-14325-5_47.pdf",
      "sourceUrl": "https://doi.org/10.1007/978-3-319-14325-5_47",
      "tags": [],
      "matchedAuthors": [
        "Marc Snir"
      ]
    },
    {
      "id": "openalex-w2000629861",
      "source": "openalex-discovery",
      "title": "C/C++ Thread Safety Analysis",
      "authors": [
        {
          "name": "DeLesley Hutchins",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Aaron Ballman",
          "affiliation": "Research and Studies Telecommunications Centre"
        },
        {
          "name": "Dean F. Sutherland",
          "affiliation": "Carnegie Mellon University"
        }
      ],
      "year": "2014",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Writing multithreaded programs is hard. Static analysis tools can help developers by allowing threading policies to be formally specified and mechanically checked. They essentially provide a static type system for threads, and can detect potential race conditions and deadlocks. This paper describes Clang Thread Safety Analysis, a tool which uses annotations to declare and enforce thread safety policies in C and C++ programs. Clang is a production-quality C++ compiler which is available on most platforms, and the analysis can be enabled for any build with a simple warning flag: -Wthread-safety. The analysis is deployed on a large scale at Google, where it has provided sufficient value in practice to drive widespread voluntary adoption. Contrary to popular belief, the need for annotations has not been a liability, and even confers some benefits with respect to software evolution and maintenance.",
      "paperUrl": "https://doi.org/10.1109/scam.2014.34",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Aaron Ballman",
        "DeLesley Hutchins"
      ]
    },
    {
      "id": "openalex-w32896423",
      "source": "openalex-discovery",
      "title": "Bounds Check Hoisting for AddressSanitizer",
      "authors": [
        {
          "name": "Simon Moll",
          "affiliation": "Saarland University"
        },
        {
          "name": "Henrique Nazaré",
          "affiliation": "Universidade Federal de Minas Gerais"
        },
        {
          "name": "Gustavo V. Machado",
          "affiliation": "Universidade Federal de Minas Gerais"
        },
        {
          "name": "Raphael Ernani Rodrigues",
          "affiliation": "Universidade Federal de Minas Gerais"
        }
      ],
      "year": "2014",
      "venue": "Lecture notes in computer science | Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "No abstract available in discovery metadata.",
      "paperUrl": "https://doi.org/10.1007/978-3-319-11863-5_4",
      "sourceUrl": "",
      "tags": [],
      "matchedAuthors": [
        "Simon Moll"
      ]
    },
    {
      "id": "openalex-w2067596360",
      "source": "openalex-discovery",
      "title": "BEAMJIT",
      "authors": [
        {
          "name": "Frej Drejhammar",
          "affiliation": "Swedish Institute"
        },
        {
          "name": "Lars Rasmusson",
          "affiliation": "Swedish Institute"
        }
      ],
      "year": "2014",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "BEAMJIT is a tracing just-in-time compiling runtime for the Erlang programming language. The core parts of BEAMJIT are synthesized from the C source code of BEAM, the reference Erlang abstract machine. The source code for BEAM's instructions is extracted automatically from BEAM's emulator loop. A tracing version of the abstract machine, as well as a code generator are synthesized. BEAMJIT uses the LLVM toolkit for optimization and native code emission. The automatic synthesis process greatly reduces the amount of manual work required to maintain a just-in-time compiler as it automatically tracks the BEAM system. The performance is evaluated with HiPE's, the Erlang ahead-of-time native compiler, benchmark suite. For most benchmarks BEAMJIT delivers a performance improvement compared to BEAM, although in some cases, with known causes, it fails to deliver a performance boost. BEAMJIT does not yet match the performance of HiPE mainly because it does not yet implement Erlang specific optimizations such as boxing/unboxing elimination and a deep understanding of BIFs. Despite this BEAMJIT, for some benchmarks, reduces the runtime with up to 40\\%.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/2633448.2633450",
      "sourceUrl": "https://doi.org/10.1145/2633448.2633450",
      "tags": [
        "Optimizations",
        "Performance"
      ],
      "matchedAuthors": [
        "Frej Drejhammar",
        "Lars Rasmusson"
      ]
    },
    {
      "id": "openalex-w2064391085",
      "source": "openalex-discovery",
      "title": "A Retargetable Static Binary Translator for the ARM Architecture",
      "authors": [
        {
          "name": "Bor-Yeh Shen",
          "affiliation": "National Yang Ming Chiao Tung University"
        },
        {
          "name": "Wei‐Chung Hsu",
          "affiliation": "National Yang Ming Chiao Tung University"
        },
        {
          "name": "Wuu Yang",
          "affiliation": "National Yang Ming Chiao Tung University"
        }
      ],
      "year": "2014",
      "venue": "ACM Transactions on Architecture and Code Optimization | Vol. 11 (Issue 2)",
      "type": "research-paper",
      "abstract": "Machines designed with new but incompatible Instruction Set Architecture (ISA) may lack proper applications. Binary translation can address this incompatibility by migrating applications from one legacy ISA to a new one, although binary translation has problems such as code discovery for variable-length ISA and code location issues for handling indirect branches. Dynamic Binary Translation (DBT) has been widely adopted for migrating applications since it avoids those problems. Static Binary Translation (SBT) is a less general solution and has not been actively researched. However, SBT performs more aggressive optimizations, which could yield more compact code and better code quality. Applications translated by SBT can consume less memory, processor cycles, and power than DBT and can be started more quickly. These advantages are even more critical for embedded systems than for general systems. In this article, we designed and implemented a new SBT tool, called LLBT, which translates ARM instructions into LLVM IRs and then retargets the LLVM IRs to various ISAs, including ×86, ×86--64, ARM, and MIPS. LLBT leverages two important functionalities from LLVM: comprehensive optimizations and retargetability. More importantly, LLBT solves the code discovery problem for ARM/Thumb binaries without resorting to interpretation. LLBT also effectively reduced the size of the address mapping table, making SBT a viable solution for embedded systems. Our experiments based on the EEMBC benchmark suite show that the LLBT-generated code can run more than 6× and 2.3× faster on average than emulation with QEMU and HQEMU, respectively.",
      "paperUrl": "https://dl.acm.org/doi/pdf/10.1145/2629335",
      "sourceUrl": "https://doi.org/10.1145/2629335",
      "tags": [
        "Embedded",
        "Optimizations"
      ],
      "matchedAuthors": [
        "Wei-Chung Hsu"
      ]
    },
    {
      "id": "openalex-w1981015239",
      "source": "openalex-discovery",
      "title": "Who allocated my memory? Detecting custom memory allocators in C binaries",
      "authors": [
        {
          "name": "Xi Chen",
          "affiliation": "Vrije Universiteit Amsterdam"
        },
        {
          "name": "Asia Slowinska",
          "affiliation": "University of Amsterdam"
        },
        {
          "name": "Herbert Bos",
          "affiliation": "Vrije Universiteit Amsterdam"
        }
      ],
      "year": "2013",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Many reversing techniques for data structures rely on the knowledge of memory allocation routines. Typically, they interpose on the system's malloc and free functions, and track each chunk of memory thus allocated as a data structure. However, many performance-critical applications implement their own custom memory allocators. Examples include webservers, database management systems, and compilers like gcc and clang. As a result, current binary analysis techniques for tracking data structures fail on such binaries. We present MemBrush, a new tool to detect memory allocation and deallocation functions in stripped binaries with high accuracy. We evaluated the technique on a large number of real world applications that use custom memory allocators. As we show, we can furnish existing reversing tools with detailed information about the memory management API, and as a result perform an analysis of the actual application specific data structures designed by the programmer. Our system uses dynamic analysis and detects memory allocation and deallocation routines by searching for functions that comply with a set of generic characteristics of allocators and deallocators.",
      "paperUrl": "https://doi.org/10.1109/wcre.2013.6671277",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Dynamic Analysis",
        "Performance"
      ],
      "matchedAuthors": [
        "Asia Slowinska",
        "Herbert Bos"
      ]
    },
    {
      "id": "openalex-w2029643891",
      "source": "openalex-discovery",
      "title": "Large-Scale Automated Refactoring Using ClangMR",
      "authors": [
        {
          "name": "Hyrum K. Wright",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Daniel Jasper",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Manuel Klimek",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Chandler Carruth",
          "affiliation": "Google (United States)"
        },
        {
          "name": "Zhanyong Wan",
          "affiliation": "Google (United States)"
        }
      ],
      "year": "2013",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "Maintaining large code bases can be a challenging endeavour. As new libraries, APIs and standards are introduced, old code is migrated to use them. To provide as clean and succinct an interface as possible for developers, old APIs are ideally removed as new ones are introduced. In practice, this becomes difficult as automatically finding and transforming code in a semantically correct way can be challenging, particularly as the size of a code base increases. In this paper, we present a real-world implementation of a system to refactor large C++ code bases efficiently. A combination of the Clang compiler framework and the MapReduce parallel processor, ClangMR enables code maintainers to easily and correctly transform large collections of code. We describe the motivation behind such a tool, its implementation and then present our experiences using it in a recent API update with Google's C++ code base.",
      "paperUrl": "https://doi.org/10.1109/icsm.2013.93",
      "sourceUrl": "",
      "tags": [
        "C++",
        "Clang",
        "Libraries"
      ],
      "matchedAuthors": [
        "Chandler Carruth",
        "Daniel Jasper",
        "Manuel Klimek"
      ]
    },
    {
      "id": "openalex-w1954113551",
      "source": "openalex-discovery",
      "title": "Removing and restoring control flow with the Value State Dependence Graph",
      "authors": [
        {
          "name": "James Stanier",
          "affiliation": ""
        }
      ],
      "year": "2012",
      "venue": "UPT. Syiah Kuala University Library (Syiah Kuala University) | Vol. None (Issue None)",
      "type": "thesis",
      "abstract": "This thesis studies the practicality of compiling with only data flow information.&#13;\\nSpecifically, we focus on the challenges that arise when using the Value&#13;\\nState Dependence Graph (VSDG) as an intermediate representation (IR).&#13;\\nWe perform a detailed survey of IRs in the literature in order to discover&#13;\\ntrends over time, and we classify them by their features in a taxonomy. We&#13;\\nsee how the VSDG fits into the IR landscape, and look at the divide between&#13;\\nacademia and the 'real world' in terms of compiler technology. Since most&#13;\\ndata flow IRs cannot be constructed for irreducible programs, we perform an&#13;\\nempirical study of irreducibility in current versions of open source software,&#13;\\nand then compare them with older versions of the same software. We also&#13;\\nstudy machine-generated C code from a variety of different software tools.&#13;\\nWe show that irreducibility is no longer a problem, and is becoming less so&#13;\\nwith time. We then address the problem of constructing the VSDG. Since&#13;\\nprevious approaches in the literature have been poorly documented or ignored&#13;\\naltogether, we give our approach to constructing the VSDG from a common&#13;\\nIR: the Control Flow Graph. We show how our approach is independent of&#13;\\nthe source and target language, how it is able to handle unstructured control&#13;\\nflow, and how it is able to transform irreducible programs on the fly. Once the&#13;\\nVSDG is constructed, we implement Lawrence's proceduralisation algorithm&#13;\\nin order to encode an evaluation strategy whilst translating the program into&#13;\\na parallel representation: the Program Dependence Graph. From here, we&#13;\\nimplement scheduling and then code generation using the LLVM compiler.&#13;\\nWe compare our compiler framework against several existing compilers, and&#13;\\nshow how removing control flow with the VSDG and then restoring it later&#13;\\ncan produce high quality code. We also examine specific situations where the&#13;\\nVSDG can put pressure on existing code generators. Our results show that the&#13;\\nVSDG represents a radically different, yet practical, approach to compilation.",
      "paperUrl": "http://sro.sussex.ac.uk/7576/1/Stanier%2C_James.pdf",
      "sourceUrl": "http://uilis.unsyiah.ac.id/opentheses/items/show/28030",
      "tags": [
        "IR"
      ],
      "matchedAuthors": [
        "James Stanier"
      ]
    },
    {
      "id": "openalex-w2188953358",
      "source": "openalex-discovery",
      "title": "Enabling Clang to statically check MPI type safety",
      "authors": [
        {
          "name": "Dmitri Gribenko",
          "affiliation": ""
        },
        {
          "name": "Alexander Zinenko",
          "affiliation": ""
        }
      ],
      "year": "2012",
      "venue": "Vol. None (Issue None)",
      "type": "research-paper",
      "abstract": "This paper introduces a static analysis method to check that the actual type of the data buffer and the type specified by MPI library constants match. The method is based on annotations in MPI library header files. Type checking is performed at compile time and has no run-time overhead. The paper also covers implementation of the method in Clang compiler and MPICH2 implementation of MPI.",
      "paperUrl": "http://hpc-ua.org/hpc-ua-12/files/proceedings/3.pdf",
      "sourceUrl": "",
      "tags": [
        "Clang",
        "Static Analysis"
      ],
      "matchedAuthors": [
        "Dmitri Gribenko"
      ]
    },
    {
      "id": "openalex-w2012902881",
      "source": "openalex-discovery",
      "title": "The Optimal Transmission Power Per Round for Hybrid-ARQ Rayleigh Fading Links",
      "authors": [
        {
          "name": "Weifeng Su",
          "affiliation": "University at Buffalo, State University of New York"
        },
        {
          "name": "S. Lee",
          "affiliation": "University at Buffalo, State University of New York"
        },
        {
          "name": "Dimitris A. Pados",
          "affiliation": "University at Buffalo, State University of New York"
        },
        {
          "name": "John D. Matyjas",
          "affiliation": "United States Air Force Research Laboratory"
        }
      ],
      "year": "2010",
      "venue": "Vol. 30 (Issue None)",
      "type": "research-paper",
      "abstract": "We address the fundamental problem of identifying the optimal power allocation sequence for hybrid automatic-repeat-request (H-ARQ) communications over quasistatic Rayleigh fading channels. For any targeted H-ARQ link outage probability, we find the sequence of power values that minimizes the average total expended transmission power. The newly founded power allocation solution reveals that conventional equal-power H-ARQ assignment is far from optimal. For example, for targeted outage probability of 10 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-3</sup> with a maximum of two transmissions, the average total transmission power with optimal assignment is 9dB lower than the equal-power protocol. The difference in average total power cost grows further when the number of allowable retransmissions increases (for example, lldB gain with a cap of 5 transmissions) or the targeted outage probability decreases (27dB gain with outage probability 10 <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">-5</sup> and transmissions capped at 5).",
      "paperUrl": "https://doi.org/10.1109/icc.2010.5502465",
      "sourceUrl": "",
      "tags": [
        "LLDB"
      ],
      "matchedAuthors": [
        "S. Lee"
      ]
    }
  ]
}
